Variable OMP_NUM_THREADS has been set to 24
--------------------
Hostname: rdg9
Fri Aug 16 01:35:54 BST 2024
Free GPU: 0 of 2
--------------------
GPU0: [92mNot in use.[39m
GPU1: [92mNot in use.[39m

User: [91macw572[39m JobID: [91m3796991[39m GPU Allocation: [91m2[39m Queue: [91mshort.q[39m
User: [91macw782[39m JobID: [91m3795335[39m GPU Allocation: [91m1[39m Queue: [91mshort.q[39m
User: [91macw782[39m JobID: [91m3795335[39m GPU Allocation: [91m1[39m Queue: [91mshort.q[39m
[91mWarning! GPUs requested but not used![39m
In main
[[36m2024-08-16 01:36:42,239[0m][[34mutils.utils[0m][[32mINFO[0m] - [rank: 0] Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2024-08-16 01:36:42,246[0m][[34mutils.utils[0m][[32mINFO[0m] - [rank: 0] Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.fsd_datamodule.FSDDataModule                         
â”‚       json_path: /data/scratch/acw572/LHGNN/datafiles/                        
â”‚       data_dir: /data/EECS-MachineListeningLab/datasets/AudioSet              
â”‚       meta_path: /data/EECS-MachineListeningLab/datasets/AudioSet/ground_truth
â”‚       label_csv_pth: /data/scratch/acw572/LHGNN/datafiles/class_labels_indices
â”‚       samplr_csv_pth: /data/scratch/acw572/LHGNN/datafiles/fsd50k_tr_full_weig
â”‚       balance_samplr: true                                                    
â”‚       batch_size: 50                                                          
â”‚       num_workers: 8                                                          
â”‚       pin_memory: true                                                        
â”‚       persistent_workers: true                                                
â”‚       sr: 16000                                                               
â”‚       fmin: 20                                                                
â”‚       fmax: 8000                                                              
â”‚       num_mels: 128                                                           
â”‚       window_type: hanning                                                    
â”‚       target_len: 1024                                                        
â”‚       freqm: 48                                                               
â”‚       timem: 192                                                              
â”‚       mixup: 0.5                                                              
â”‚       norm_mean: -4.6476                                                      
â”‚       norm_std: 4.5699                                                        
â”‚       num_devices: 2                                                          
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.tagging_module_test.TaggingModule                  
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0005                                                            
â”‚         weight_decay: 5.0e-07                                                 
â”‚         eps: 1.0e-08                                                          
â”‚         betas:                                                                
â”‚         - 0.95                                                                
â”‚         - 0.999                                                               
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.MultiStepLR                        
â”‚         _partial_: true                                                       
â”‚         milestones:                                                           
â”‚         - 10                                                                  
â”‚         - 15                                                                  
â”‚         - 20                                                                  
â”‚         - 25                                                                  
â”‚         - 30                                                                  
â”‚         - 35                                                                  
â”‚         - 40                                                                  
â”‚         gamma: 0.5                                                            
â”‚       net:                                                                    
â”‚         _target_: src.models.components.Hypergraph.HGCN                       
â”‚         k: 25                                                                 
â”‚         act: gelu                                                             
â”‚         norm: batch                                                           
â”‚         bias: true                                                            
â”‚         dropout: 0.0                                                          
â”‚         dilation: true                                                        
â”‚         epsilon: 0.2                                                          
â”‚         drop_path: 0.1                                                        
â”‚         size: s                                                               
â”‚         num_class: 200                                                        
â”‚         emb_dims: 1024                                                        
â”‚         freq_num: 128                                                         
â”‚         time_num: 1024                                                        
â”‚       compile: false                                                          
â”‚       loss: bce                                                               
â”‚       opt_warmup: true                                                        
â”‚       learning_rate: 0.0005                                                   
â”‚       lr_rate:                                                                
â”‚       - 0.05                                                                  
â”‚       - 0.02                                                                  
â”‚       - 0.01                                                                  
â”‚       - 0.005                                                                 
â”‚       - 0.002                                                                 
â”‚       - 0.001                                                                 
â”‚       - 0.0005                                                                
â”‚       - 0.0002                                                                
â”‚       lr_scheduler_epoch:                                                     
â”‚       - 10                                                                    
â”‚       - 15                                                                    
â”‚       - 20                                                                    
â”‚       - 25                                                                    
â”‚       - 30                                                                    
â”‚       - 35                                                                    
â”‚       - 50                                                                    
â”‚       - 45                                                                    
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-16_01-36-4
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mAP                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 20                                                        
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/loss                                                     
â”‚         min_delta: 0.0                                                        
â”‚         patience: 5                                                           
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       tqdm_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.TQDMProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.WandbLogger                       
â”‚         save_dir: /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-16_01-36-
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: audioset-bal                                                 
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: Tagging                                                        
â”‚         tags:                                                                 
â”‚         - fsd                                                                 
â”‚         - hgcn                                                                
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.trainer.Trainer                             
â”‚       default_root_dir: /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-16_
â”‚       num_sanity_val_steps: 0                                                 
â”‚       min_epochs: 3                                                           
â”‚       max_epochs: 5                                                           
â”‚       accelerator: gpu                                                        
â”‚       devices: 2                                                              
â”‚       gradient_clip_val: 0.5                                                  
â”‚       precision: 32                                                           
â”‚       detect_anomaly: false                                                   
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       strategy: ddp                                                           
â”‚       num_nodes: 1                                                            
â”‚       sync_batchnorm: true                                                    
â”‚       use_distributed_sampler: true                                           
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /data/home/acw572/hgann/HGANN                                 
â”‚       exp_dir: /data/scratch/acw572                                           
â”‚       data_dir: /data/EECS-MachineListeningLab/datasets/AudioSet              
â”‚       meta_dir: /data/EECS-MachineListeningLab/shubhr/hgann                   
â”‚       log_dir: /data/scratch/acw572/LHGNN/logs/                               
â”‚       output_dir: /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-16_01-36-
â”‚       work_dir: /data/home/acw572/hgann/HGANN                                 
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ pretrained
â”‚   â””â”€â”€ img                                                                     
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['dev']                                                                 
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ eval
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ wa
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ /data/EECS-MachineListeningLab/shubhr/imagenet_weights/model_best.pth.ta
â””â”€â”€ seed
    â””â”€â”€ None                                                                    
[[36m2024-08-16 01:36:42,394[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] In train[0m
[[36m2024-08-16 01:36:42,394[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating datamodule <src.data.fsd_datamodule.FSDDataModule>[0m
[[36m2024-08-16 01:36:44,456[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating model <src.models.tagging_module_test.TaggingModule>[0m
/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
norm is batch
bias is True
drop_path is 0.1
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
[[36m2024-08-16 01:37:13,038[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Loading img pretrained weights[0m
[[36m2024-08-16 01:37:13,038[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating callbacks...[0m
[[36m2024-08-16 01:37:13,039[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2024-08-16 01:37:13,044[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2024-08-16 01:37:13,045[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2024-08-16 01:37:13,046[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <pytorch_lightning.callbacks.TQDMProgressBar>[0m
[[36m2024-08-16 01:37:13,046[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating loggers...[0m
[[36m2024-08-16 01:37:13,047[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating logger <pytorch_lightning.loggers.WandbLogger>[0m
[[36m2024-08-16 01:37:13,358[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating trainer <pytorch_lightning.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-08-16 01:37:13,642[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Logging hyperparameters![0m
wandb: Currently logged in as: shubhr. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-16_01-36-41/wandb/run-20240816_013717-bb1db5lg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-waterfall-54
wandb: â­ï¸ View project at https://wandb.ai/shubhr/audioset-bal
wandb: ğŸš€ View run at https://wandb.ai/shubhr/audioset-bal/runs/bb1db5lg
[[36m2024-08-16 01:37:36,413[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting training![0m
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”³â”â”â”“
â”ƒ   â”ƒ Name                                                              â”ƒ â€¦ â”ƒ  â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â•‡â”â”â”©
â”‚ 0 â”‚ net                                                               â”‚ â€¦ â”‚  â”‚
â”‚ 1 â”‚ net.stem                                                          â”‚ â€¦ â”‚  â”‚
â”‚ 2 â”‚ net.stem.convs                                                    â”‚ â€¦ â”‚  â”‚
â”‚ 3 â”‚ net.stem.convs.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 4 â”‚ net.stem.convs.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 5 â”‚ net.stem.convs.2                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 6 â”‚ net.stem.convs.3                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 7 â”‚ net.stem.convs.4                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 8 â”‚ net.stem.convs.5                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 9 â”‚ net.stem.convs.6                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.stem.convs.7                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone                                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.2                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.2.conv                                               â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.2.conv.0                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.2.conv.1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.5                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.5.conv                                               â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.5.conv.0                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.5.conv.1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv                                â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.nn                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.nn.0                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.nn.1                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.nn.2                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.get_centroids                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.get_centroids.centers_proposal â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.dilated_knn_graph                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.dilated_knn_graph._dilated           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.act                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.conv                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.conv.conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv                                â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.nn                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.nn.0                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.nn.1                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.nn.2                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.get_centroids                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.get_centroids.centers_proposal â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.dilated_knn_graph                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.dilated_knn_graph._dilated           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.act                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.conv                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.conv.conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.12                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.12.conv                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.12.conv.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.12.conv.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv                                â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.nn                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.nn.0                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.nn.1                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.nn.2                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.get_centroids                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.get_centroids.centers_proposal â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.dilated_knn_graph                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.dilated_knn_graph._dilated           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.act                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.conv                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.conv.conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv                                â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.nn                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.nn.0                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.nn.1                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.nn.2                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.get_centroids                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.get_centroids.centers_proposal â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.dilated_knn_graph                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.dilated_knn_graph._dilated           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.act                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.conv                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.conv.conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.2                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.3                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.4                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ criterion                                                         â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ train_loss                                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ val_loss                                                          â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ test_loss                                                         â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ val_mAP                                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ test_mAP                                                          â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ val_mAP_best                                                      â”‚ â€¦ â”‚  â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”˜
Trainable params: 31.1 M                                                        
Non-trainable params: 12.1 M                                                    
Total params: 43.2 M                                                            
Total estimated model params size (MB): 172                                     
In main
norm is batch
bias is True
drop_path is 0.1
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/367 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/367 [00:00<?, ?it/s] hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
Epoch 0:   0%|          | 1/367 [00:15<1:35:28,  0.06it/s]Epoch 0:   0%|          | 1/367 [00:15<1:35:35,  0.06it/s, v_num=b5lg, train/loss_step=0.801]Epoch 0:   1%|          | 2/367 [00:16<51:36,  0.12it/s, v_num=b5lg, train/loss_step=0.801]  Epoch 0:   1%|          | 2/367 [00:16<51:36,  0.12it/s, v_num=b5lg, train/loss_step=0.742]Epoch 0:   1%|          | 3/367 [00:18<36:54,  0.16it/s, v_num=b5lg, train/loss_step=0.742]Epoch 0:   1%|          | 3/367 [00:18<36:54,  0.16it/s, v_num=b5lg, train/loss_step=0.791]Epoch 0:   1%|          | 4/367 [00:19<29:32,  0.20it/s, v_num=b5lg, train/loss_step=0.791]Epoch 0:   1%|          | 4/367 [00:19<29:32,  0.20it/s, v_num=b5lg, train/loss_step=0.777]Epoch 0:   1%|â–         | 5/367 [00:20<25:07,  0.24it/s, v_num=b5lg, train/loss_step=0.777]Epoch 0:   1%|â–         | 5/367 [00:20<25:10,  0.24it/s, v_num=b5lg, train/loss_step=0.786]Epoch 0:   2%|â–         | 6/367 [00:22<22:12,  0.27it/s, v_num=b5lg, train/loss_step=0.786]Epoch 0:   2%|â–         | 6/367 [00:22<22:12,  0.27it/s, v_num=b5lg, train/loss_step=0.754]Epoch 0:   2%|â–         | 7/367 [00:23<20:04,  0.30it/s, v_num=b5lg, train/loss_step=0.754]Epoch 0:   2%|â–         | 7/367 [00:23<20:04,  0.30it/s, v_num=b5lg, train/loss_step=0.746]Epoch 0:   2%|â–         | 8/367 [00:24<18:26,  0.32it/s, v_num=b5lg, train/loss_step=0.746]Epoch 0:   2%|â–         | 8/367 [00:24<18:26,  0.32it/s, v_num=b5lg, train/loss_step=0.747]Epoch 0:   2%|â–         | 9/367 [00:25<17:10,  0.35it/s, v_num=b5lg, train/loss_step=0.747]Epoch 0:   2%|â–         | 9/367 [00:25<17:10,  0.35it/s, v_num=b5lg, train/loss_step=0.718]Epoch 0:   3%|â–         | 10/367 [00:27<16:08,  0.37it/s, v_num=b5lg, train/loss_step=0.718]Epoch 0:   3%|â–         | 10/367 [00:27<16:08,  0.37it/s, v_num=b5lg, train/loss_step=0.770]Epoch 0:   3%|â–         | 11/367 [00:28<15:18,  0.39it/s, v_num=b5lg, train/loss_step=0.770]Epoch 0:   3%|â–         | 11/367 [00:28<15:18,  0.39it/s, v_num=b5lg, train/loss_step=0.733]Epoch 0:   3%|â–         | 12/367 [00:29<14:35,  0.41it/s, v_num=b5lg, train/loss_step=0.733]Epoch 0:   3%|â–         | 12/367 [00:29<14:35,  0.41it/s, v_num=b5lg, train/loss_step=0.729]Epoch 0:   4%|â–         | 13/367 [00:30<13:59,  0.42it/s, v_num=b5lg, train/loss_step=0.729]Epoch 0:   4%|â–         | 13/367 [00:30<13:59,  0.42it/s, v_num=b5lg, train/loss_step=0.779]Epoch 0:   4%|â–         | 14/367 [00:32<13:28,  0.44it/s, v_num=b5lg, train/loss_step=0.779]Epoch 0:   4%|â–         | 14/367 [00:32<13:28,  0.44it/s, v_num=b5lg, train/loss_step=0.725]Epoch 0:   4%|â–         | 15/367 [00:33<13:01,  0.45it/s, v_num=b5lg, train/loss_step=0.725]Epoch 0:   4%|â–         | 15/367 [00:33<13:01,  0.45it/s, v_num=b5lg, train/loss_step=0.798]Epoch 0:   4%|â–         | 16/367 [00:34<12:37,  0.46it/s, v_num=b5lg, train/loss_step=0.798]Epoch 0:   4%|â–         | 16/367 [00:34<12:37,  0.46it/s, v_num=b5lg, train/loss_step=0.764]Epoch 0:   5%|â–         | 17/367 [00:35<12:16,  0.48it/s, v_num=b5lg, train/loss_step=0.764]Epoch 0:   5%|â–         | 17/367 [00:35<12:16,  0.48it/s, v_num=b5lg, train/loss_step=0.726]Epoch 0:   5%|â–         | 18/367 [00:37<11:57,  0.49it/s, v_num=b5lg, train/loss_step=0.726]Epoch 0:   5%|â–         | 18/367 [00:37<11:57,  0.49it/s, v_num=b5lg, train/loss_step=0.776]Epoch 0:   5%|â–Œ         | 19/367 [00:38<11:40,  0.50it/s, v_num=b5lg, train/loss_step=0.776]Epoch 0:   5%|â–Œ         | 19/367 [00:38<11:40,  0.50it/s, v_num=b5lg, train/loss_step=0.768]Epoch 0:   5%|â–Œ         | 20/367 [00:39<11:24,  0.51it/s, v_num=b5lg, train/loss_step=0.768]Epoch 0:   5%|â–Œ         | 20/367 [00:39<11:24,  0.51it/s, v_num=b5lg, train/loss_step=0.732]Epoch 0:   6%|â–Œ         | 21/367 [00:40<11:11,  0.52it/s, v_num=b5lg, train/loss_step=0.732]Epoch 0:   6%|â–Œ         | 21/367 [00:40<11:11,  0.52it/s, v_num=b5lg, train/loss_step=0.752]Epoch 0:   6%|â–Œ         | 22/367 [00:42<10:58,  0.52it/s, v_num=b5lg, train/loss_step=0.752]Epoch 0:   6%|â–Œ         | 22/367 [00:42<10:58,  0.52it/s, v_num=b5lg, train/loss_step=0.769]Epoch 0:   6%|â–‹         | 23/367 [00:43<10:47,  0.53it/s, v_num=b5lg, train/loss_step=0.769]Epoch 0:   6%|â–‹         | 23/367 [00:43<10:47,  0.53it/s, v_num=b5lg, train/loss_step=0.752]Epoch 0:   7%|â–‹         | 24/367 [00:44<10:36,  0.54it/s, v_num=b5lg, train/loss_step=0.752]Epoch 0:   7%|â–‹         | 24/367 [00:44<10:36,  0.54it/s, v_num=b5lg, train/loss_step=0.744]Epoch 0:   7%|â–‹         | 25/367 [00:45<10:27,  0.55it/s, v_num=b5lg, train/loss_step=0.744]Epoch 0:   7%|â–‹         | 25/367 [00:45<10:27,  0.55it/s, v_num=b5lg, train/loss_step=0.689]Epoch 0:   7%|â–‹         | 26/367 [00:47<10:17,  0.55it/s, v_num=b5lg, train/loss_step=0.689]Epoch 0:   7%|â–‹         | 26/367 [00:47<10:17,  0.55it/s, v_num=b5lg, train/loss_step=0.705]Epoch 0:   7%|â–‹         | 27/367 [00:48<10:08,  0.56it/s, v_num=b5lg, train/loss_step=0.705]Epoch 0:   7%|â–‹         | 27/367 [00:48<10:08,  0.56it/s, v_num=b5lg, train/loss_step=0.707]Epoch 0:   8%|â–Š         | 28/367 [00:49<09:59,  0.57it/s, v_num=b5lg, train/loss_step=0.707]Epoch 0:   8%|â–Š         | 28/367 [00:49<09:59,  0.57it/s, v_num=b5lg, train/loss_step=0.703]Epoch 0:   8%|â–Š         | 29/367 [00:50<09:51,  0.57it/s, v_num=b5lg, train/loss_step=0.703]Epoch 0:   8%|â–Š         | 29/367 [00:50<09:51,  0.57it/s, v_num=b5lg, train/loss_step=0.715]Epoch 0:   8%|â–Š         | 30/367 [00:52<09:44,  0.58it/s, v_num=b5lg, train/loss_step=0.715]Epoch 0:   8%|â–Š         | 30/367 [00:52<09:44,  0.58it/s, v_num=b5lg, train/loss_step=0.734]Epoch 0:   8%|â–Š         | 31/367 [00:53<09:37,  0.58it/s, v_num=b5lg, train/loss_step=0.734]Epoch 0:   8%|â–Š         | 31/367 [00:53<09:37,  0.58it/s, v_num=b5lg, train/loss_step=0.718]Epoch 0:   9%|â–Š         | 32/367 [00:54<09:30,  0.59it/s, v_num=b5lg, train/loss_step=0.718]Epoch 0:   9%|â–Š         | 32/367 [00:54<09:30,  0.59it/s, v_num=b5lg, train/loss_step=0.739]Epoch 0:   9%|â–‰         | 33/367 [00:55<09:23,  0.59it/s, v_num=b5lg, train/loss_step=0.739]Epoch 0:   9%|â–‰         | 33/367 [00:55<09:23,  0.59it/s, v_num=b5lg, train/loss_step=0.731]Epoch 0:   9%|â–‰         | 34/367 [00:56<09:17,  0.60it/s, v_num=b5lg, train/loss_step=0.731]Epoch 0:   9%|â–‰         | 34/367 [00:56<09:17,  0.60it/s, v_num=b5lg, train/loss_step=0.733]Epoch 0:  10%|â–‰         | 35/367 [00:58<09:11,  0.60it/s, v_num=b5lg, train/loss_step=0.733]Epoch 0:  10%|â–‰         | 35/367 [00:58<09:11,  0.60it/s, v_num=b5lg, train/loss_step=0.681]Epoch 0:  10%|â–‰         | 36/367 [00:59<09:05,  0.61it/s, v_num=b5lg, train/loss_step=0.681]Epoch 0:  10%|â–‰         | 36/367 [00:59<09:05,  0.61it/s, v_num=b5lg, train/loss_step=0.671]Epoch 0:  10%|â–ˆ         | 37/367 [01:00<09:00,  0.61it/s, v_num=b5lg, train/loss_step=0.671]Epoch 0:  10%|â–ˆ         | 37/367 [01:00<09:00,  0.61it/s, v_num=b5lg, train/loss_step=0.666]Epoch 0:  10%|â–ˆ         | 38/367 [01:01<08:55,  0.61it/s, v_num=b5lg, train/loss_step=0.666]Epoch 0:  10%|â–ˆ         | 38/367 [01:01<08:55,  0.61it/s, v_num=b5lg, train/loss_step=0.712]Epoch 0:  11%|â–ˆ         | 39/367 [01:03<08:50,  0.62it/s, v_num=b5lg, train/loss_step=0.712]Epoch 0:  11%|â–ˆ         | 39/367 [01:03<08:50,  0.62it/s, v_num=b5lg, train/loss_step=0.732]Epoch 0:  11%|â–ˆ         | 40/367 [01:04<08:45,  0.62it/s, v_num=b5lg, train/loss_step=0.732]Epoch 0:  11%|â–ˆ         | 40/367 [01:04<08:45,  0.62it/s, v_num=b5lg, train/loss_step=0.664]Epoch 0:  11%|â–ˆ         | 41/367 [01:05<08:40,  0.63it/s, v_num=b5lg, train/loss_step=0.664]Epoch 0:  11%|â–ˆ         | 41/367 [01:05<08:40,  0.63it/s, v_num=b5lg, train/loss_step=0.681]Epoch 0:  11%|â–ˆâ–        | 42/367 [01:06<08:36,  0.63it/s, v_num=b5lg, train/loss_step=0.681]Epoch 0:  11%|â–ˆâ–        | 42/367 [01:06<08:36,  0.63it/s, v_num=b5lg, train/loss_step=0.670]Epoch 0:  12%|â–ˆâ–        | 43/367 [01:07<08:32,  0.63it/s, v_num=b5lg, train/loss_step=0.670]Epoch 0:  12%|â–ˆâ–        | 43/367 [01:07<08:32,  0.63it/s, v_num=b5lg, train/loss_step=0.664]Epoch 0:  12%|â–ˆâ–        | 44/367 [01:09<08:27,  0.64it/s, v_num=b5lg, train/loss_step=0.664]Epoch 0:  12%|â–ˆâ–        | 44/367 [01:09<08:27,  0.64it/s, v_num=b5lg, train/loss_step=0.704]Epoch 0:  12%|â–ˆâ–        | 45/367 [01:10<08:23,  0.64it/s, v_num=b5lg, train/loss_step=0.704]Epoch 0:  12%|â–ˆâ–        | 45/367 [01:10<08:24,  0.64it/s, v_num=b5lg, train/loss_step=0.695]Epoch 0:  13%|â–ˆâ–        | 46/367 [01:11<08:20,  0.64it/s, v_num=b5lg, train/loss_step=0.695]Epoch 0:  13%|â–ˆâ–        | 46/367 [01:11<08:20,  0.64it/s, v_num=b5lg, train/loss_step=0.649]Epoch 0:  13%|â–ˆâ–        | 47/367 [01:12<08:16,  0.64it/s, v_num=b5lg, train/loss_step=0.649]Epoch 0:  13%|â–ˆâ–        | 47/367 [01:12<08:16,  0.64it/s, v_num=b5lg, train/loss_step=0.636]Epoch 0:  13%|â–ˆâ–        | 48/367 [01:14<08:12,  0.65it/s, v_num=b5lg, train/loss_step=0.636]Epoch 0:  13%|â–ˆâ–        | 48/367 [01:14<08:12,  0.65it/s, v_num=b5lg, train/loss_step=0.654]Epoch 0:  13%|â–ˆâ–        | 49/367 [01:15<08:09,  0.65it/s, v_num=b5lg, train/loss_step=0.654]Epoch 0:  13%|â–ˆâ–        | 49/367 [01:15<08:09,  0.65it/s, v_num=b5lg, train/loss_step=0.704]Epoch 0:  14%|â–ˆâ–        | 50/367 [01:16<08:06,  0.65it/s, v_num=b5lg, train/loss_step=0.704]Epoch 0:  14%|â–ˆâ–        | 50/367 [01:16<08:06,  0.65it/s, v_num=b5lg, train/loss_step=0.651]Epoch 0:  14%|â–ˆâ–        | 51/367 [01:17<08:02,  0.65it/s, v_num=b5lg, train/loss_step=0.651]Epoch 0:  14%|â–ˆâ–        | 51/367 [01:17<08:02,  0.65it/s, v_num=b5lg, train/loss_step=0.660]Epoch 0:  14%|â–ˆâ–        | 52/367 [01:19<07:59,  0.66it/s, v_num=b5lg, train/loss_step=0.660]Epoch 0:  14%|â–ˆâ–        | 52/367 [01:19<07:59,  0.66it/s, v_num=b5lg, train/loss_step=0.659]Epoch 0:  14%|â–ˆâ–        | 53/367 [01:20<07:56,  0.66it/s, v_num=b5lg, train/loss_step=0.659]Epoch 0:  14%|â–ˆâ–        | 53/367 [01:20<07:56,  0.66it/s, v_num=b5lg, train/loss_step=0.672]Epoch 0:  15%|â–ˆâ–        | 54/367 [01:21<07:53,  0.66it/s, v_num=b5lg, train/loss_step=0.672]Epoch 0:  15%|â–ˆâ–        | 54/367 [01:21<07:53,  0.66it/s, v_num=b5lg, train/loss_step=0.616]Epoch 0:  15%|â–ˆâ–        | 55/367 [01:22<07:49,  0.66it/s, v_num=b5lg, train/loss_step=0.616]Epoch 0:  15%|â–ˆâ–        | 55/367 [01:22<07:49,  0.66it/s, v_num=b5lg, train/loss_step=0.609]Epoch 0:  15%|â–ˆâ–Œ        | 56/367 [01:24<07:46,  0.67it/s, v_num=b5lg, train/loss_step=0.609]Epoch 0:  15%|â–ˆâ–Œ        | 56/367 [01:24<07:46,  0.67it/s, v_num=b5lg, train/loss_step=0.643]Epoch 0:  16%|â–ˆâ–Œ        | 57/367 [01:25<07:43,  0.67it/s, v_num=b5lg, train/loss_step=0.643]Epoch 0:  16%|â–ˆâ–Œ        | 57/367 [01:25<07:43,  0.67it/s, v_num=b5lg, train/loss_step=0.609]Epoch 0:  16%|â–ˆâ–Œ        | 58/367 [01:26<07:40,  0.67it/s, v_num=b5lg, train/loss_step=0.609]Epoch 0:  16%|â–ˆâ–Œ        | 58/367 [01:26<07:40,  0.67it/s, v_num=b5lg, train/loss_step=0.648]Epoch 0:  16%|â–ˆâ–Œ        | 59/367 [01:27<07:38,  0.67it/s, v_num=b5lg, train/loss_step=0.648]Epoch 0:  16%|â–ˆâ–Œ        | 59/367 [01:27<07:38,  0.67it/s, v_num=b5lg, train/loss_step=0.623]Epoch 0:  16%|â–ˆâ–‹        | 60/367 [01:28<07:35,  0.67it/s, v_num=b5lg, train/loss_step=0.623]Epoch 0:  16%|â–ˆâ–‹        | 60/367 [01:28<07:35,  0.67it/s, v_num=b5lg, train/loss_step=0.639]Epoch 0:  17%|â–ˆâ–‹        | 61/367 [01:30<07:32,  0.68it/s, v_num=b5lg, train/loss_step=0.639]Epoch 0:  17%|â–ˆâ–‹        | 61/367 [01:30<07:32,  0.68it/s, v_num=b5lg, train/loss_step=0.615]Epoch 0:  17%|â–ˆâ–‹        | 62/367 [01:31<07:29,  0.68it/s, v_num=b5lg, train/loss_step=0.615]Epoch 0:  17%|â–ˆâ–‹        | 62/367 [01:31<07:29,  0.68it/s, v_num=b5lg, train/loss_step=0.631]Epoch 0:  17%|â–ˆâ–‹        | 63/367 [01:32<07:27,  0.68it/s, v_num=b5lg, train/loss_step=0.631]Epoch 0:  17%|â–ˆâ–‹        | 63/367 [01:32<07:27,  0.68it/s, v_num=b5lg, train/loss_step=0.588]Epoch 0:  17%|â–ˆâ–‹        | 64/367 [01:33<07:24,  0.68it/s, v_num=b5lg, train/loss_step=0.588]Epoch 0:  17%|â–ˆâ–‹        | 64/367 [01:33<07:24,  0.68it/s, v_num=b5lg, train/loss_step=0.613]Epoch 0:  18%|â–ˆâ–Š        | 65/367 [01:35<07:21,  0.68it/s, v_num=b5lg, train/loss_step=0.613]Epoch 0:  18%|â–ˆâ–Š        | 65/367 [01:35<07:21,  0.68it/s, v_num=b5lg, train/loss_step=0.582]Epoch 0:  18%|â–ˆâ–Š        | 66/367 [01:36<07:19,  0.69it/s, v_num=b5lg, train/loss_step=0.582]Epoch 0:  18%|â–ˆâ–Š        | 66/367 [01:36<07:19,  0.68it/s, v_num=b5lg, train/loss_step=0.620]Epoch 0:  18%|â–ˆâ–Š        | 67/367 [01:37<07:16,  0.69it/s, v_num=b5lg, train/loss_step=0.620]Epoch 0:  18%|â–ˆâ–Š        | 67/367 [01:37<07:16,  0.69it/s, v_num=b5lg, train/loss_step=0.624]Epoch 0:  19%|â–ˆâ–Š        | 68/367 [01:38<07:14,  0.69it/s, v_num=b5lg, train/loss_step=0.624]Epoch 0:  19%|â–ˆâ–Š        | 68/367 [01:38<07:14,  0.69it/s, v_num=b5lg, train/loss_step=0.608]Epoch 0:  19%|â–ˆâ–‰        | 69/367 [01:40<07:12,  0.69it/s, v_num=b5lg, train/loss_step=0.608]Epoch 0:  19%|â–ˆâ–‰        | 69/367 [01:40<07:12,  0.69it/s, v_num=b5lg, train/loss_step=0.564]Epoch 0:  19%|â–ˆâ–‰        | 70/367 [01:41<07:09,  0.69it/s, v_num=b5lg, train/loss_step=0.564]Epoch 0:  19%|â–ˆâ–‰        | 70/367 [01:41<07:09,  0.69it/s, v_num=b5lg, train/loss_step=0.560]Epoch 0:  19%|â–ˆâ–‰        | 71/367 [01:42<07:07,  0.69it/s, v_num=b5lg, train/loss_step=0.560]Epoch 0:  19%|â–ˆâ–‰        | 71/367 [01:42<07:07,  0.69it/s, v_num=b5lg, train/loss_step=0.536]Epoch 0:  20%|â–ˆâ–‰        | 72/367 [01:43<07:04,  0.69it/s, v_num=b5lg, train/loss_step=0.536]Epoch 0:  20%|â–ˆâ–‰        | 72/367 [01:43<07:04,  0.69it/s, v_num=b5lg, train/loss_step=0.584]Epoch 0:  20%|â–ˆâ–‰        | 73/367 [01:44<07:02,  0.70it/s, v_num=b5lg, train/loss_step=0.584]Epoch 0:  20%|â–ˆâ–‰        | 73/367 [01:44<07:02,  0.70it/s, v_num=b5lg, train/loss_step=0.574]Epoch 0:  20%|â–ˆâ–ˆ        | 74/367 [01:46<07:00,  0.70it/s, v_num=b5lg, train/loss_step=0.574]Epoch 0:  20%|â–ˆâ–ˆ        | 74/367 [01:46<07:00,  0.70it/s, v_num=b5lg, train/loss_step=0.554]Epoch 0:  20%|â–ˆâ–ˆ        | 75/367 [01:47<06:58,  0.70it/s, v_num=b5lg, train/loss_step=0.554]Epoch 0:  20%|â–ˆâ–ˆ        | 75/367 [01:47<06:58,  0.70it/s, v_num=b5lg, train/loss_step=0.532]Epoch 0:  21%|â–ˆâ–ˆ        | 76/367 [01:48<06:55,  0.70it/s, v_num=b5lg, train/loss_step=0.532]Epoch 0:  21%|â–ˆâ–ˆ        | 76/367 [01:48<06:55,  0.70it/s, v_num=b5lg, train/loss_step=0.557]Epoch 0:  21%|â–ˆâ–ˆ        | 77/367 [01:49<06:53,  0.70it/s, v_num=b5lg, train/loss_step=0.557]Epoch 0:  21%|â–ˆâ–ˆ        | 77/367 [01:49<06:53,  0.70it/s, v_num=b5lg, train/loss_step=0.547]Epoch 0:  21%|â–ˆâ–ˆâ–       | 78/367 [01:51<06:51,  0.70it/s, v_num=b5lg, train/loss_step=0.547]Epoch 0:  21%|â–ˆâ–ˆâ–       | 78/367 [01:51<06:51,  0.70it/s, v_num=b5lg, train/loss_step=0.549]Epoch 0:  22%|â–ˆâ–ˆâ–       | 79/367 [01:52<06:49,  0.70it/s, v_num=b5lg, train/loss_step=0.549]Epoch 0:  22%|â–ˆâ–ˆâ–       | 79/367 [01:52<06:49,  0.70it/s, v_num=b5lg, train/loss_step=0.548]Epoch 0:  22%|â–ˆâ–ˆâ–       | 80/367 [01:53<06:47,  0.70it/s, v_num=b5lg, train/loss_step=0.548]Epoch 0:  22%|â–ˆâ–ˆâ–       | 80/367 [01:53<06:47,  0.70it/s, v_num=b5lg, train/loss_step=0.530]Epoch 0:  22%|â–ˆâ–ˆâ–       | 81/367 [01:54<06:45,  0.71it/s, v_num=b5lg, train/loss_step=0.530]Epoch 0:  22%|â–ˆâ–ˆâ–       | 81/367 [01:54<06:45,  0.71it/s, v_num=b5lg, train/loss_step=0.501]Epoch 0:  22%|â–ˆâ–ˆâ–       | 82/367 [01:55<06:43,  0.71it/s, v_num=b5lg, train/loss_step=0.501]Epoch 0:  22%|â–ˆâ–ˆâ–       | 82/367 [01:55<06:43,  0.71it/s, v_num=b5lg, train/loss_step=0.529]Epoch 0:  23%|â–ˆâ–ˆâ–       | 83/367 [01:57<06:41,  0.71it/s, v_num=b5lg, train/loss_step=0.529]Epoch 0:  23%|â–ˆâ–ˆâ–       | 83/367 [01:57<06:41,  0.71it/s, v_num=b5lg, train/loss_step=0.486]Epoch 0:  23%|â–ˆâ–ˆâ–       | 84/367 [01:58<06:39,  0.71it/s, v_num=b5lg, train/loss_step=0.486]Epoch 0:  23%|â–ˆâ–ˆâ–       | 84/367 [01:58<06:39,  0.71it/s, v_num=b5lg, train/loss_step=0.504]Epoch 0:  23%|â–ˆâ–ˆâ–       | 85/367 [01:59<06:37,  0.71it/s, v_num=b5lg, train/loss_step=0.504]Epoch 0:  23%|â–ˆâ–ˆâ–       | 85/367 [01:59<06:37,  0.71it/s, v_num=b5lg, train/loss_step=0.484]Epoch 0:  23%|â–ˆâ–ˆâ–       | 86/367 [02:00<06:35,  0.71it/s, v_num=b5lg, train/loss_step=0.484]Epoch 0:  23%|â–ˆâ–ˆâ–       | 86/367 [02:00<06:35,  0.71it/s, v_num=b5lg, train/loss_step=0.501]Epoch 0:  24%|â–ˆâ–ˆâ–       | 87/367 [02:02<06:33,  0.71it/s, v_num=b5lg, train/loss_step=0.501]Epoch 0:  24%|â–ˆâ–ˆâ–       | 87/367 [02:02<06:33,  0.71it/s, v_num=b5lg, train/loss_step=0.486]Epoch 0:  24%|â–ˆâ–ˆâ–       | 88/367 [02:03<06:31,  0.71it/s, v_num=b5lg, train/loss_step=0.486]Epoch 0:  24%|â–ˆâ–ˆâ–       | 88/367 [02:03<06:32,  0.71it/s, v_num=b5lg, train/loss_step=0.466]Epoch 0:  24%|â–ˆâ–ˆâ–       | 89/367 [02:04<06:30,  0.71it/s, v_num=b5lg, train/loss_step=0.466]Epoch 0:  24%|â–ˆâ–ˆâ–       | 89/367 [02:04<06:30,  0.71it/s, v_num=b5lg, train/loss_step=0.456]Epoch 0:  25%|â–ˆâ–ˆâ–       | 90/367 [02:06<06:28,  0.71it/s, v_num=b5lg, train/loss_step=0.456]Epoch 0:  25%|â–ˆâ–ˆâ–       | 90/367 [02:06<06:28,  0.71it/s, v_num=b5lg, train/loss_step=0.458]Epoch 0:  25%|â–ˆâ–ˆâ–       | 91/367 [02:07<06:26,  0.71it/s, v_num=b5lg, train/loss_step=0.458]Epoch 0:  25%|â–ˆâ–ˆâ–       | 91/367 [02:07<06:26,  0.71it/s, v_num=b5lg, train/loss_step=0.466]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [02:08<06:24,  0.72it/s, v_num=b5lg, train/loss_step=0.466]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [02:08<06:24,  0.72it/s, v_num=b5lg, train/loss_step=0.466]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [02:09<06:22,  0.72it/s, v_num=b5lg, train/loss_step=0.466]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [02:09<06:22,  0.72it/s, v_num=b5lg, train/loss_step=0.442]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [02:10<06:20,  0.72it/s, v_num=b5lg, train/loss_step=0.442]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [02:10<06:20,  0.72it/s, v_num=b5lg, train/loss_step=0.468]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [02:12<06:18,  0.72it/s, v_num=b5lg, train/loss_step=0.468]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [02:12<06:18,  0.72it/s, v_num=b5lg, train/loss_step=0.418]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [02:13<06:16,  0.72it/s, v_num=b5lg, train/loss_step=0.418]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [02:13<06:16,  0.72it/s, v_num=b5lg, train/loss_step=0.420]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:14<06:14,  0.72it/s, v_num=b5lg, train/loss_step=0.420]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:14<06:14,  0.72it/s, v_num=b5lg, train/loss_step=0.472]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:15<06:13,  0.72it/s, v_num=b5lg, train/loss_step=0.472]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:15<06:13,  0.72it/s, v_num=b5lg, train/loss_step=0.455]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:17<06:11,  0.72it/s, v_num=b5lg, train/loss_step=0.455]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:17<06:11,  0.72it/s, v_num=b5lg, train/loss_step=0.426]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:18<06:09,  0.72it/s, v_num=b5lg, train/loss_step=0.426]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:18<06:09,  0.72it/s, v_num=b5lg, train/loss_step=0.401]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:19<06:07,  0.72it/s, v_num=b5lg, train/loss_step=0.401]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:19<06:07,  0.72it/s, v_num=b5lg, train/loss_step=0.409]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:20<06:05,  0.72it/s, v_num=b5lg, train/loss_step=0.409]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:20<06:05,  0.72it/s, v_num=b5lg, train/loss_step=0.424]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:22<06:04,  0.73it/s, v_num=b5lg, train/loss_step=0.424]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:22<06:04,  0.73it/s, v_num=b5lg, train/loss_step=0.432]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:23<06:02,  0.73it/s, v_num=b5lg, train/loss_step=0.432]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:23<06:02,  0.73it/s, v_num=b5lg, train/loss_step=0.394]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:24<06:00,  0.73it/s, v_num=b5lg, train/loss_step=0.394]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:24<06:00,  0.73it/s, v_num=b5lg, train/loss_step=0.383]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:25<05:58,  0.73it/s, v_num=b5lg, train/loss_step=0.383]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:25<05:58,  0.73it/s, v_num=b5lg, train/loss_step=0.380]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:26<05:57,  0.73it/s, v_num=b5lg, train/loss_step=0.380]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:26<05:57,  0.73it/s, v_num=b5lg, train/loss_step=0.393]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:28<05:55,  0.73it/s, v_num=b5lg, train/loss_step=0.393]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:28<05:55,  0.73it/s, v_num=b5lg, train/loss_step=0.366]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:29<05:53,  0.73it/s, v_num=b5lg, train/loss_step=0.366]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:29<05:53,  0.73it/s, v_num=b5lg, train/loss_step=0.375]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:30<05:51,  0.73it/s, v_num=b5lg, train/loss_step=0.375]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:30<05:51,  0.73it/s, v_num=b5lg, train/loss_step=0.366]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:31<05:50,  0.73it/s, v_num=b5lg, train/loss_step=0.366]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:31<05:50,  0.73it/s, v_num=b5lg, train/loss_step=0.351]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:33<05:48,  0.73it/s, v_num=b5lg, train/loss_step=0.351]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:33<05:48,  0.73it/s, v_num=b5lg, train/loss_step=0.345]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:34<05:46,  0.73it/s, v_num=b5lg, train/loss_step=0.345]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:34<05:46,  0.73it/s, v_num=b5lg, train/loss_step=0.352]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:35<05:45,  0.73it/s, v_num=b5lg, train/loss_step=0.352]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:35<05:45,  0.73it/s, v_num=b5lg, train/loss_step=0.355]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:36<05:43,  0.73it/s, v_num=b5lg, train/loss_step=0.355]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:36<05:43,  0.73it/s, v_num=b5lg, train/loss_step=0.365]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:38<05:41,  0.73it/s, v_num=b5lg, train/loss_step=0.365]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:38<05:41,  0.73it/s, v_num=b5lg, train/loss_step=0.331]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:39<05:40,  0.73it/s, v_num=b5lg, train/loss_step=0.331]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:39<05:40,  0.73it/s, v_num=b5lg, train/loss_step=0.333]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:40<05:38,  0.74it/s, v_num=b5lg, train/loss_step=0.333]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:40<05:38,  0.74it/s, v_num=b5lg, train/loss_step=0.321]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:41<05:36,  0.74it/s, v_num=b5lg, train/loss_step=0.321]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:41<05:36,  0.74it/s, v_num=b5lg, train/loss_step=0.307]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:42<05:35,  0.74it/s, v_num=b5lg, train/loss_step=0.307]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:42<05:35,  0.74it/s, v_num=b5lg, train/loss_step=0.304]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:44<05:33,  0.74it/s, v_num=b5lg, train/loss_step=0.304]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:44<05:33,  0.74it/s, v_num=b5lg, train/loss_step=0.315]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:45<05:32,  0.74it/s, v_num=b5lg, train/loss_step=0.315]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:45<05:32,  0.74it/s, v_num=b5lg, train/loss_step=0.333]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:46<05:30,  0.74it/s, v_num=b5lg, train/loss_step=0.333]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:46<05:30,  0.74it/s, v_num=b5lg, train/loss_step=0.296]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:47<05:28,  0.74it/s, v_num=b5lg, train/loss_step=0.296]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:47<05:28,  0.74it/s, v_num=b5lg, train/loss_step=0.317]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:48<05:27,  0.74it/s, v_num=b5lg, train/loss_step=0.317]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:48<05:27,  0.74it/s, v_num=b5lg, train/loss_step=0.308]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:50<05:25,  0.74it/s, v_num=b5lg, train/loss_step=0.308]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:50<05:25,  0.74it/s, v_num=b5lg, train/loss_step=0.282]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:51<05:23,  0.74it/s, v_num=b5lg, train/loss_step=0.282]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:51<05:23,  0.74it/s, v_num=b5lg, train/loss_step=0.284]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:52<05:22,  0.74it/s, v_num=b5lg, train/loss_step=0.284]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:52<05:22,  0.74it/s, v_num=b5lg, train/loss_step=0.278]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:53<05:20,  0.74it/s, v_num=b5lg, train/loss_step=0.278]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:53<05:20,  0.74it/s, v_num=b5lg, train/loss_step=0.267]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:55<05:19,  0.74it/s, v_num=b5lg, train/loss_step=0.267]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:55<05:19,  0.74it/s, v_num=b5lg, train/loss_step=0.258]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:56<05:17,  0.74it/s, v_num=b5lg, train/loss_step=0.258]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:56<05:17,  0.74it/s, v_num=b5lg, train/loss_step=0.263]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:57<05:15,  0.74it/s, v_num=b5lg, train/loss_step=0.263]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:57<05:15,  0.74it/s, v_num=b5lg, train/loss_step=0.239]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:58<05:14,  0.74it/s, v_num=b5lg, train/loss_step=0.239]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:58<05:14,  0.74it/s, v_num=b5lg, train/loss_step=0.263]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:59<05:12,  0.74it/s, v_num=b5lg, train/loss_step=0.263]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:59<05:12,  0.74it/s, v_num=b5lg, train/loss_step=0.235]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [03:01<05:11,  0.75it/s, v_num=b5lg, train/loss_step=0.235]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [03:01<05:11,  0.75it/s, v_num=b5lg, train/loss_step=0.227]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [03:02<05:09,  0.75it/s, v_num=b5lg, train/loss_step=0.227]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [03:02<05:09,  0.75it/s, v_num=b5lg, train/loss_step=0.231]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [03:03<05:08,  0.75it/s, v_num=b5lg, train/loss_step=0.231]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [03:03<05:08,  0.75it/s, v_num=b5lg, train/loss_step=0.219]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [03:04<05:06,  0.75it/s, v_num=b5lg, train/loss_step=0.219]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [03:04<05:06,  0.75it/s, v_num=b5lg, train/loss_step=0.215]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [03:05<05:05,  0.75it/s, v_num=b5lg, train/loss_step=0.215]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [03:05<05:05,  0.75it/s, v_num=b5lg, train/loss_step=0.223]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [03:07<05:03,  0.75it/s, v_num=b5lg, train/loss_step=0.223]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [03:07<05:03,  0.75it/s, v_num=b5lg, train/loss_step=0.206]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [03:08<05:02,  0.75it/s, v_num=b5lg, train/loss_step=0.206]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [03:08<05:02,  0.75it/s, v_num=b5lg, train/loss_step=0.209]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [03:09<05:00,  0.75it/s, v_num=b5lg, train/loss_step=0.209]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [03:09<05:00,  0.75it/s, v_num=b5lg, train/loss_step=0.200]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [03:10<04:58,  0.75it/s, v_num=b5lg, train/loss_step=0.200]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [03:10<04:58,  0.75it/s, v_num=b5lg, train/loss_step=0.211]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [03:12<04:57,  0.75it/s, v_num=b5lg, train/loss_step=0.211]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [03:12<04:57,  0.75it/s, v_num=b5lg, train/loss_step=0.187]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [03:13<04:55,  0.75it/s, v_num=b5lg, train/loss_step=0.187]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [03:13<04:55,  0.75it/s, v_num=b5lg, train/loss_step=0.178]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:14<04:54,  0.75it/s, v_num=b5lg, train/loss_step=0.178]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:14<04:54,  0.75it/s, v_num=b5lg, train/loss_step=0.178]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:15<04:52,  0.75it/s, v_num=b5lg, train/loss_step=0.178]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:15<04:52,  0.75it/s, v_num=b5lg, train/loss_step=0.176]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:16<04:51,  0.75it/s, v_num=b5lg, train/loss_step=0.176]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:16<04:51,  0.75it/s, v_num=b5lg, train/loss_step=0.161]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:18<04:49,  0.75it/s, v_num=b5lg, train/loss_step=0.161]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:18<04:49,  0.75it/s, v_num=b5lg, train/loss_step=0.162]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:19<04:48,  0.75it/s, v_num=b5lg, train/loss_step=0.162]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:19<04:48,  0.75it/s, v_num=b5lg, train/loss_step=0.167]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:20<04:46,  0.75it/s, v_num=b5lg, train/loss_step=0.167]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:20<04:46,  0.75it/s, v_num=b5lg, train/loss_step=0.156]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:21<04:45,  0.75it/s, v_num=b5lg, train/loss_step=0.156]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:21<04:45,  0.75it/s, v_num=b5lg, train/loss_step=0.155]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:23<04:44,  0.75it/s, v_num=b5lg, train/loss_step=0.155]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:23<04:44,  0.75it/s, v_num=b5lg, train/loss_step=0.151]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:24<04:42,  0.75it/s, v_num=b5lg, train/loss_step=0.151]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:24<04:42,  0.75it/s, v_num=b5lg, train/loss_step=0.145]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:25<04:41,  0.75it/s, v_num=b5lg, train/loss_step=0.145]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:25<04:41,  0.75it/s, v_num=b5lg, train/loss_step=0.141]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:26<04:39,  0.75it/s, v_num=b5lg, train/loss_step=0.141]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:26<04:39,  0.75it/s, v_num=b5lg, train/loss_step=0.138]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:27<04:38,  0.76it/s, v_num=b5lg, train/loss_step=0.138]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:27<04:38,  0.76it/s, v_num=b5lg, train/loss_step=0.144]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:29<04:36,  0.76it/s, v_num=b5lg, train/loss_step=0.144]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:29<04:36,  0.76it/s, v_num=b5lg, train/loss_step=0.137]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:30<04:35,  0.76it/s, v_num=b5lg, train/loss_step=0.137]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:30<04:35,  0.76it/s, v_num=b5lg, train/loss_step=0.127]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:31<04:33,  0.76it/s, v_num=b5lg, train/loss_step=0.127]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:31<04:33,  0.76it/s, v_num=b5lg, train/loss_step=0.132]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:32<04:32,  0.76it/s, v_num=b5lg, train/loss_step=0.132]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:32<04:32,  0.76it/s, v_num=b5lg, train/loss_step=0.122]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:34<04:30,  0.76it/s, v_num=b5lg, train/loss_step=0.122]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:34<04:30,  0.76it/s, v_num=b5lg, train/loss_step=0.117]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:35<04:29,  0.76it/s, v_num=b5lg, train/loss_step=0.117]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:35<04:29,  0.76it/s, v_num=b5lg, train/loss_step=0.121]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:36<04:28,  0.76it/s, v_num=b5lg, train/loss_step=0.121]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:36<04:28,  0.76it/s, v_num=b5lg, train/loss_step=0.119]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:37<04:26,  0.76it/s, v_num=b5lg, train/loss_step=0.119]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:37<04:26,  0.76it/s, v_num=b5lg, train/loss_step=0.108]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:38<04:25,  0.76it/s, v_num=b5lg, train/loss_step=0.108]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:38<04:25,  0.76it/s, v_num=b5lg, train/loss_step=0.113]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:40<04:23,  0.76it/s, v_num=b5lg, train/loss_step=0.113]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:40<04:23,  0.76it/s, v_num=b5lg, train/loss_step=0.112]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:41<04:22,  0.76it/s, v_num=b5lg, train/loss_step=0.112]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:41<04:22,  0.76it/s, v_num=b5lg, train/loss_step=0.112]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:42<04:20,  0.76it/s, v_num=b5lg, train/loss_step=0.112]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:42<04:20,  0.76it/s, v_num=b5lg, train/loss_step=0.105]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:43<04:19,  0.76it/s, v_num=b5lg, train/loss_step=0.105]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:43<04:19,  0.76it/s, v_num=b5lg, train/loss_step=0.0953]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:45<04:17,  0.76it/s, v_num=b5lg, train/loss_step=0.0953]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:45<04:17,  0.76it/s, v_num=b5lg, train/loss_step=0.0967]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:46<04:16,  0.76it/s, v_num=b5lg, train/loss_step=0.0967]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:46<04:16,  0.76it/s, v_num=b5lg, train/loss_step=0.115] Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:47<04:15,  0.76it/s, v_num=b5lg, train/loss_step=0.115]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:47<04:15,  0.76it/s, v_num=b5lg, train/loss_step=0.094]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:48<04:13,  0.76it/s, v_num=b5lg, train/loss_step=0.094]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:48<04:13,  0.76it/s, v_num=b5lg, train/loss_step=0.0949]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:49<04:12,  0.76it/s, v_num=b5lg, train/loss_step=0.0949]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:49<04:12,  0.76it/s, v_num=b5lg, train/loss_step=0.0927]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:51<04:10,  0.76it/s, v_num=b5lg, train/loss_step=0.0927]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:51<04:10,  0.76it/s, v_num=b5lg, train/loss_step=0.100] Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:52<04:09,  0.76it/s, v_num=b5lg, train/loss_step=0.100]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:52<04:09,  0.76it/s, v_num=b5lg, train/loss_step=0.0888]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:53<04:08,  0.76it/s, v_num=b5lg, train/loss_step=0.0888]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:53<04:08,  0.76it/s, v_num=b5lg, train/loss_step=0.0872]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:54<04:06,  0.76it/s, v_num=b5lg, train/loss_step=0.0872]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:54<04:06,  0.76it/s, v_num=b5lg, train/loss_step=0.0829]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:56<04:05,  0.76it/s, v_num=b5lg, train/loss_step=0.0829]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:56<04:05,  0.76it/s, v_num=b5lg, train/loss_step=0.0892]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:57<04:03,  0.76it/s, v_num=b5lg, train/loss_step=0.0892]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:57<04:03,  0.76it/s, v_num=b5lg, train/loss_step=0.0814]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:58<04:02,  0.76it/s, v_num=b5lg, train/loss_step=0.0814]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:58<04:02,  0.76it/s, v_num=b5lg, train/loss_step=0.0782]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:59<04:01,  0.76it/s, v_num=b5lg, train/loss_step=0.0782]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:59<04:01,  0.76it/s, v_num=b5lg, train/loss_step=0.0796]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [04:00<03:59,  0.76it/s, v_num=b5lg, train/loss_step=0.0796]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [04:00<03:59,  0.76it/s, v_num=b5lg, train/loss_step=0.0779]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [04:02<03:58,  0.76it/s, v_num=b5lg, train/loss_step=0.0779]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [04:02<03:58,  0.76it/s, v_num=b5lg, train/loss_step=0.0789]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [04:03<03:56,  0.76it/s, v_num=b5lg, train/loss_step=0.0789]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [04:03<03:56,  0.76it/s, v_num=b5lg, train/loss_step=0.0766]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [04:04<03:55,  0.76it/s, v_num=b5lg, train/loss_step=0.0766]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [04:04<03:55,  0.76it/s, v_num=b5lg, train/loss_step=0.075] Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [04:05<03:54,  0.76it/s, v_num=b5lg, train/loss_step=0.075]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [04:05<03:54,  0.76it/s, v_num=b5lg, train/loss_step=0.0719]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [04:07<03:52,  0.77it/s, v_num=b5lg, train/loss_step=0.0719]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [04:07<03:52,  0.77it/s, v_num=b5lg, train/loss_step=0.0757]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [04:08<03:51,  0.77it/s, v_num=b5lg, train/loss_step=0.0757]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [04:08<03:51,  0.77it/s, v_num=b5lg, train/loss_step=0.0744]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [04:09<03:49,  0.77it/s, v_num=b5lg, train/loss_step=0.0744]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [04:09<03:49,  0.77it/s, v_num=b5lg, train/loss_step=0.0772]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [04:10<03:48,  0.77it/s, v_num=b5lg, train/loss_step=0.0772]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [04:10<03:48,  0.77it/s, v_num=b5lg, train/loss_step=0.0677]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [04:11<03:47,  0.77it/s, v_num=b5lg, train/loss_step=0.0677]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [04:11<03:47,  0.77it/s, v_num=b5lg, train/loss_step=0.0732]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:13<03:45,  0.77it/s, v_num=b5lg, train/loss_step=0.0732]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:13<03:45,  0.77it/s, v_num=b5lg, train/loss_step=0.0684]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:14<03:44,  0.77it/s, v_num=b5lg, train/loss_step=0.0684]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:14<03:44,  0.77it/s, v_num=b5lg, train/loss_step=0.0691]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:15<03:43,  0.77it/s, v_num=b5lg, train/loss_step=0.0691]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:15<03:43,  0.77it/s, v_num=b5lg, train/loss_step=0.0639]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:16<03:41,  0.77it/s, v_num=b5lg, train/loss_step=0.0639]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:16<03:41,  0.77it/s, v_num=b5lg, train/loss_step=0.0679]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:18<03:40,  0.77it/s, v_num=b5lg, train/loss_step=0.0679]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:18<03:40,  0.77it/s, v_num=b5lg, train/loss_step=0.064] Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:19<03:38,  0.77it/s, v_num=b5lg, train/loss_step=0.064]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:19<03:38,  0.77it/s, v_num=b5lg, train/loss_step=0.0626]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:20<03:37,  0.77it/s, v_num=b5lg, train/loss_step=0.0626]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:20<03:37,  0.77it/s, v_num=b5lg, train/loss_step=0.0662]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:21<03:36,  0.77it/s, v_num=b5lg, train/loss_step=0.0662]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:21<03:36,  0.77it/s, v_num=b5lg, train/loss_step=0.0642]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:23<03:34,  0.77it/s, v_num=b5lg, train/loss_step=0.0642]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:23<03:34,  0.77it/s, v_num=b5lg, train/loss_step=0.0595]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:24<03:33,  0.77it/s, v_num=b5lg, train/loss_step=0.0595]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:24<03:33,  0.77it/s, v_num=b5lg, train/loss_step=0.0654]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:25<03:32,  0.77it/s, v_num=b5lg, train/loss_step=0.0654]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:25<03:32,  0.77it/s, v_num=b5lg, train/loss_step=0.0681]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:26<03:30,  0.77it/s, v_num=b5lg, train/loss_step=0.0681]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:26<03:30,  0.77it/s, v_num=b5lg, train/loss_step=0.0627]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:27<03:29,  0.77it/s, v_num=b5lg, train/loss_step=0.0627]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:27<03:29,  0.77it/s, v_num=b5lg, train/loss_step=0.0629]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:29<03:28,  0.77it/s, v_num=b5lg, train/loss_step=0.0629]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:29<03:28,  0.77it/s, v_num=b5lg, train/loss_step=0.0556]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:30<03:26,  0.77it/s, v_num=b5lg, train/loss_step=0.0556]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:30<03:26,  0.77it/s, v_num=b5lg, train/loss_step=0.0595]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:31<03:25,  0.77it/s, v_num=b5lg, train/loss_step=0.0595]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:31<03:25,  0.77it/s, v_num=b5lg, train/loss_step=0.0545]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:32<03:24,  0.77it/s, v_num=b5lg, train/loss_step=0.0545]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:32<03:24,  0.77it/s, v_num=b5lg, train/loss_step=0.0583]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:34<03:22,  0.77it/s, v_num=b5lg, train/loss_step=0.0583]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:34<03:22,  0.77it/s, v_num=b5lg, train/loss_step=0.0603]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:35<03:21,  0.77it/s, v_num=b5lg, train/loss_step=0.0603]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:35<03:21,  0.77it/s, v_num=b5lg, train/loss_step=0.0571]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:36<03:19,  0.77it/s, v_num=b5lg, train/loss_step=0.0571]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:36<03:19,  0.77it/s, v_num=b5lg, train/loss_step=0.0579]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:37<03:18,  0.77it/s, v_num=b5lg, train/loss_step=0.0579]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:37<03:18,  0.77it/s, v_num=b5lg, train/loss_step=0.0569]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:39<03:17,  0.77it/s, v_num=b5lg, train/loss_step=0.0569]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:39<03:17,  0.77it/s, v_num=b5lg, train/loss_step=0.0577]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:40<03:15,  0.77it/s, v_num=b5lg, train/loss_step=0.0577]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:40<03:15,  0.77it/s, v_num=b5lg, train/loss_step=0.0624]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:41<03:14,  0.77it/s, v_num=b5lg, train/loss_step=0.0624]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:41<03:14,  0.77it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:42<03:13,  0.77it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:42<03:13,  0.77it/s, v_num=b5lg, train/loss_step=0.0604]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:44<03:11,  0.77it/s, v_num=b5lg, train/loss_step=0.0604]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:44<03:11,  0.77it/s, v_num=b5lg, train/loss_step=0.0603]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:45<03:10,  0.77it/s, v_num=b5lg, train/loss_step=0.0603]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:45<03:10,  0.77it/s, v_num=b5lg, train/loss_step=0.0557]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:46<03:09,  0.77it/s, v_num=b5lg, train/loss_step=0.0557]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:46<03:09,  0.77it/s, v_num=b5lg, train/loss_step=0.059] Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:47<03:07,  0.77it/s, v_num=b5lg, train/loss_step=0.059]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:47<03:07,  0.77it/s, v_num=b5lg, train/loss_step=0.0582]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:49<03:06,  0.77it/s, v_num=b5lg, train/loss_step=0.0582]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:49<03:06,  0.77it/s, v_num=b5lg, train/loss_step=0.0572]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:50<03:05,  0.77it/s, v_num=b5lg, train/loss_step=0.0572]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:50<03:05,  0.77it/s, v_num=b5lg, train/loss_step=0.054] Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:51<03:04,  0.77it/s, v_num=b5lg, train/loss_step=0.054]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:51<03:04,  0.77it/s, v_num=b5lg, train/loss_step=0.0544]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:52<03:02,  0.77it/s, v_num=b5lg, train/loss_step=0.0544]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:52<03:02,  0.77it/s, v_num=b5lg, train/loss_step=0.0558]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:54<03:01,  0.77it/s, v_num=b5lg, train/loss_step=0.0558]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:54<03:01,  0.77it/s, v_num=b5lg, train/loss_step=0.0699]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:55<03:00,  0.77it/s, v_num=b5lg, train/loss_step=0.0699]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:55<03:00,  0.77it/s, v_num=b5lg, train/loss_step=0.0565]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:56<02:58,  0.77it/s, v_num=b5lg, train/loss_step=0.0565]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:56<02:58,  0.77it/s, v_num=b5lg, train/loss_step=0.053] Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:57<02:57,  0.77it/s, v_num=b5lg, train/loss_step=0.053]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:57<02:57,  0.77it/s, v_num=b5lg, train/loss_step=0.0515]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:59<02:56,  0.77it/s, v_num=b5lg, train/loss_step=0.0515]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:59<02:56,  0.77it/s, v_num=b5lg, train/loss_step=0.0595]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [05:00<02:54,  0.77it/s, v_num=b5lg, train/loss_step=0.0595]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [05:00<02:54,  0.77it/s, v_num=b5lg, train/loss_step=0.0507]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [05:01<02:53,  0.77it/s, v_num=b5lg, train/loss_step=0.0507]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [05:01<02:53,  0.77it/s, v_num=b5lg, train/loss_step=0.0524]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [05:02<02:52,  0.77it/s, v_num=b5lg, train/loss_step=0.0524]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [05:02<02:52,  0.77it/s, v_num=b5lg, train/loss_step=0.054] Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [05:04<02:50,  0.77it/s, v_num=b5lg, train/loss_step=0.054]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [05:04<02:50,  0.77it/s, v_num=b5lg, train/loss_step=0.0558]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [05:05<02:49,  0.77it/s, v_num=b5lg, train/loss_step=0.0558]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [05:05<02:49,  0.77it/s, v_num=b5lg, train/loss_step=0.0614]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [05:06<02:48,  0.77it/s, v_num=b5lg, train/loss_step=0.0614]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [05:06<02:48,  0.77it/s, v_num=b5lg, train/loss_step=0.0582]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [05:07<02:46,  0.77it/s, v_num=b5lg, train/loss_step=0.0582]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [05:07<02:46,  0.77it/s, v_num=b5lg, train/loss_step=0.0531]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [05:09<02:45,  0.77it/s, v_num=b5lg, train/loss_step=0.0531]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [05:09<02:45,  0.77it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [05:10<02:44,  0.77it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [05:10<02:44,  0.77it/s, v_num=b5lg, train/loss_step=0.0551]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [05:11<02:42,  0.77it/s, v_num=b5lg, train/loss_step=0.0551]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [05:11<02:42,  0.77it/s, v_num=b5lg, train/loss_step=0.0571]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [05:12<02:41,  0.77it/s, v_num=b5lg, train/loss_step=0.0571]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [05:12<02:41,  0.77it/s, v_num=b5lg, train/loss_step=0.0513]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:14<02:40,  0.77it/s, v_num=b5lg, train/loss_step=0.0513]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:14<02:40,  0.77it/s, v_num=b5lg, train/loss_step=0.0583]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:15<02:39,  0.77it/s, v_num=b5lg, train/loss_step=0.0583]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:15<02:39,  0.77it/s, v_num=b5lg, train/loss_step=0.053] Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:16<02:37,  0.77it/s, v_num=b5lg, train/loss_step=0.053]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:16<02:37,  0.77it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:18<02:36,  0.77it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:18<02:36,  0.77it/s, v_num=b5lg, train/loss_step=0.0504]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:19<02:35,  0.77it/s, v_num=b5lg, train/loss_step=0.0504]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:19<02:35,  0.77it/s, v_num=b5lg, train/loss_step=0.0543]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:20<02:33,  0.77it/s, v_num=b5lg, train/loss_step=0.0543]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:20<02:33,  0.77it/s, v_num=b5lg, train/loss_step=0.0537]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:21<02:32,  0.77it/s, v_num=b5lg, train/loss_step=0.0537]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:21<02:32,  0.77it/s, v_num=b5lg, train/loss_step=0.0567]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:23<02:31,  0.77it/s, v_num=b5lg, train/loss_step=0.0567]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:23<02:31,  0.77it/s, v_num=b5lg, train/loss_step=0.0542]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:24<02:29,  0.77it/s, v_num=b5lg, train/loss_step=0.0542]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:24<02:29,  0.77it/s, v_num=b5lg, train/loss_step=0.0485]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:25<02:28,  0.77it/s, v_num=b5lg, train/loss_step=0.0485]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:25<02:28,  0.77it/s, v_num=b5lg, train/loss_step=0.0539]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:26<02:27,  0.77it/s, v_num=b5lg, train/loss_step=0.0539]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:26<02:27,  0.77it/s, v_num=b5lg, train/loss_step=0.0578]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:28<02:25,  0.77it/s, v_num=b5lg, train/loss_step=0.0578]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:28<02:25,  0.77it/s, v_num=b5lg, train/loss_step=0.0519]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:29<02:24,  0.77it/s, v_num=b5lg, train/loss_step=0.0519]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:29<02:24,  0.77it/s, v_num=b5lg, train/loss_step=0.0532]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:30<02:23,  0.77it/s, v_num=b5lg, train/loss_step=0.0532]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:30<02:23,  0.77it/s, v_num=b5lg, train/loss_step=0.0526]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:31<02:22,  0.77it/s, v_num=b5lg, train/loss_step=0.0526]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:31<02:22,  0.77it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:33<02:20,  0.77it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:33<02:20,  0.77it/s, v_num=b5lg, train/loss_step=0.0545]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:34<02:19,  0.77it/s, v_num=b5lg, train/loss_step=0.0545]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:34<02:19,  0.77it/s, v_num=b5lg, train/loss_step=0.0492]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:35<02:18,  0.77it/s, v_num=b5lg, train/loss_step=0.0492]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:35<02:18,  0.77it/s, v_num=b5lg, train/loss_step=0.0526]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:36<02:16,  0.77it/s, v_num=b5lg, train/loss_step=0.0526]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:36<02:16,  0.77it/s, v_num=b5lg, train/loss_step=0.0539]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:38<02:15,  0.78it/s, v_num=b5lg, train/loss_step=0.0539]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:38<02:15,  0.78it/s, v_num=b5lg, train/loss_step=0.0572]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:39<02:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0572]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:39<02:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0511]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:40<02:12,  0.78it/s, v_num=b5lg, train/loss_step=0.0511]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:40<02:12,  0.78it/s, v_num=b5lg, train/loss_step=0.0513]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:41<02:11,  0.78it/s, v_num=b5lg, train/loss_step=0.0513]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:41<02:11,  0.78it/s, v_num=b5lg, train/loss_step=0.0479]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:42<02:10,  0.78it/s, v_num=b5lg, train/loss_step=0.0479]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:42<02:10,  0.78it/s, v_num=b5lg, train/loss_step=0.0532]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:44<02:08,  0.78it/s, v_num=b5lg, train/loss_step=0.0532]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:44<02:08,  0.78it/s, v_num=b5lg, train/loss_step=0.0544]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:45<02:07,  0.78it/s, v_num=b5lg, train/loss_step=0.0544]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:45<02:07,  0.78it/s, v_num=b5lg, train/loss_step=0.0525]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:46<02:06,  0.78it/s, v_num=b5lg, train/loss_step=0.0525]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:46<02:06,  0.78it/s, v_num=b5lg, train/loss_step=0.0499]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:47<02:04,  0.78it/s, v_num=b5lg, train/loss_step=0.0499]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:47<02:04,  0.78it/s, v_num=b5lg, train/loss_step=0.0588]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:49<02:03,  0.78it/s, v_num=b5lg, train/loss_step=0.0588]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:49<02:03,  0.78it/s, v_num=b5lg, train/loss_step=0.0553]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:50<02:02,  0.78it/s, v_num=b5lg, train/loss_step=0.0553]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:50<02:02,  0.78it/s, v_num=b5lg, train/loss_step=0.0566]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:51<02:01,  0.78it/s, v_num=b5lg, train/loss_step=0.0566]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:51<02:01,  0.78it/s, v_num=b5lg, train/loss_step=0.0468]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:52<01:59,  0.78it/s, v_num=b5lg, train/loss_step=0.0468]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:52<01:59,  0.78it/s, v_num=b5lg, train/loss_step=0.0503]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:53<01:58,  0.78it/s, v_num=b5lg, train/loss_step=0.0503]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:53<01:58,  0.78it/s, v_num=b5lg, train/loss_step=0.0507]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:55<01:57,  0.78it/s, v_num=b5lg, train/loss_step=0.0507]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:55<01:57,  0.78it/s, v_num=b5lg, train/loss_step=0.051] Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:56<01:55,  0.78it/s, v_num=b5lg, train/loss_step=0.051]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:56<01:55,  0.78it/s, v_num=b5lg, train/loss_step=0.0533]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:57<01:54,  0.78it/s, v_num=b5lg, train/loss_step=0.0533]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:57<01:54,  0.78it/s, v_num=b5lg, train/loss_step=0.0509]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:58<01:53,  0.78it/s, v_num=b5lg, train/loss_step=0.0509]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:58<01:53,  0.78it/s, v_num=b5lg, train/loss_step=0.0495]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [06:00<01:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0495]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [06:00<01:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0532]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [06:01<01:50,  0.78it/s, v_num=b5lg, train/loss_step=0.0532]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [06:01<01:50,  0.78it/s, v_num=b5lg, train/loss_step=0.0519]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [06:02<01:49,  0.78it/s, v_num=b5lg, train/loss_step=0.0519]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [06:02<01:49,  0.78it/s, v_num=b5lg, train/loss_step=0.0527]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [06:03<01:47,  0.78it/s, v_num=b5lg, train/loss_step=0.0527]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [06:03<01:47,  0.78it/s, v_num=b5lg, train/loss_step=0.0556]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [06:05<01:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0556]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [06:05<01:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [06:06<01:45,  0.78it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [06:06<01:45,  0.78it/s, v_num=b5lg, train/loss_step=0.0514]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [06:07<01:44,  0.78it/s, v_num=b5lg, train/loss_step=0.0514]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [06:07<01:44,  0.78it/s, v_num=b5lg, train/loss_step=0.054] Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [06:08<01:42,  0.78it/s, v_num=b5lg, train/loss_step=0.054]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [06:08<01:42,  0.78it/s, v_num=b5lg, train/loss_step=0.0479]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [06:09<01:41,  0.78it/s, v_num=b5lg, train/loss_step=0.0479]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [06:09<01:41,  0.78it/s, v_num=b5lg, train/loss_step=0.0493]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [06:11<01:40,  0.78it/s, v_num=b5lg, train/loss_step=0.0493]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [06:11<01:40,  0.78it/s, v_num=b5lg, train/loss_step=0.0488]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [06:12<01:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0488]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [06:12<01:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [06:13<01:37,  0.78it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [06:13<01:37,  0.78it/s, v_num=b5lg, train/loss_step=0.0505]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:14<01:36,  0.78it/s, v_num=b5lg, train/loss_step=0.0505]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:14<01:36,  0.78it/s, v_num=b5lg, train/loss_step=0.0514]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:15<01:34,  0.78it/s, v_num=b5lg, train/loss_step=0.0514]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:15<01:34,  0.78it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:17<01:33,  0.78it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:17<01:33,  0.78it/s, v_num=b5lg, train/loss_step=0.0544]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:18<01:32,  0.78it/s, v_num=b5lg, train/loss_step=0.0544]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:18<01:32,  0.78it/s, v_num=b5lg, train/loss_step=0.0498]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:19<01:31,  0.78it/s, v_num=b5lg, train/loss_step=0.0498]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:19<01:31,  0.78it/s, v_num=b5lg, train/loss_step=0.0516]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:20<01:29,  0.78it/s, v_num=b5lg, train/loss_step=0.0516]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:20<01:29,  0.78it/s, v_num=b5lg, train/loss_step=0.052] Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:22<01:28,  0.78it/s, v_num=b5lg, train/loss_step=0.052]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:22<01:28,  0.78it/s, v_num=b5lg, train/loss_step=0.0512]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:23<01:27,  0.78it/s, v_num=b5lg, train/loss_step=0.0512]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:23<01:27,  0.78it/s, v_num=b5lg, train/loss_step=0.0502]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:24<01:25,  0.78it/s, v_num=b5lg, train/loss_step=0.0502]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:24<01:25,  0.78it/s, v_num=b5lg, train/loss_step=0.0482]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:25<01:24,  0.78it/s, v_num=b5lg, train/loss_step=0.0482]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:25<01:24,  0.78it/s, v_num=b5lg, train/loss_step=0.0493]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:27<01:23,  0.78it/s, v_num=b5lg, train/loss_step=0.0493]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:27<01:23,  0.78it/s, v_num=b5lg, train/loss_step=0.0526]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:28<01:22,  0.78it/s, v_num=b5lg, train/loss_step=0.0526]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:28<01:22,  0.78it/s, v_num=b5lg, train/loss_step=0.0486]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:29<01:20,  0.78it/s, v_num=b5lg, train/loss_step=0.0486]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:29<01:20,  0.78it/s, v_num=b5lg, train/loss_step=0.0474]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:30<01:19,  0.78it/s, v_num=b5lg, train/loss_step=0.0474]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:30<01:19,  0.78it/s, v_num=b5lg, train/loss_step=0.0529]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:32<01:18,  0.78it/s, v_num=b5lg, train/loss_step=0.0529]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:32<01:18,  0.78it/s, v_num=b5lg, train/loss_step=0.0549]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:33<01:16,  0.78it/s, v_num=b5lg, train/loss_step=0.0549]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:33<01:16,  0.78it/s, v_num=b5lg, train/loss_step=0.0458]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:34<01:15,  0.78it/s, v_num=b5lg, train/loss_step=0.0458]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:34<01:15,  0.78it/s, v_num=b5lg, train/loss_step=0.0519]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:35<01:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0519]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:35<01:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0449]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:37<01:13,  0.78it/s, v_num=b5lg, train/loss_step=0.0449]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:37<01:13,  0.78it/s, v_num=b5lg, train/loss_step=0.0518]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:38<01:11,  0.78it/s, v_num=b5lg, train/loss_step=0.0518]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:38<01:11,  0.78it/s, v_num=b5lg, train/loss_step=0.0455]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:39<01:10,  0.78it/s, v_num=b5lg, train/loss_step=0.0455]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:39<01:10,  0.78it/s, v_num=b5lg, train/loss_step=0.048] Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:40<01:09,  0.78it/s, v_num=b5lg, train/loss_step=0.048]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:40<01:09,  0.78it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:42<01:07,  0.78it/s, v_num=b5lg, train/loss_step=0.0546]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:42<01:07,  0.78it/s, v_num=b5lg, train/loss_step=0.0513]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:43<01:06,  0.78it/s, v_num=b5lg, train/loss_step=0.0513]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:43<01:06,  0.78it/s, v_num=b5lg, train/loss_step=0.0507]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:44<01:05,  0.78it/s, v_num=b5lg, train/loss_step=0.0507]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:44<01:05,  0.78it/s, v_num=b5lg, train/loss_step=0.0453]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:45<01:04,  0.78it/s, v_num=b5lg, train/loss_step=0.0453]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:45<01:04,  0.78it/s, v_num=b5lg, train/loss_step=0.0469]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:47<01:02,  0.78it/s, v_num=b5lg, train/loss_step=0.0469]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:47<01:02,  0.78it/s, v_num=b5lg, train/loss_step=0.050] Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:48<01:01,  0.78it/s, v_num=b5lg, train/loss_step=0.050]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:48<01:01,  0.78it/s, v_num=b5lg, train/loss_step=0.0484]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:49<01:00,  0.78it/s, v_num=b5lg, train/loss_step=0.0484]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:49<01:00,  0.78it/s, v_num=b5lg, train/loss_step=0.0542]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:51<00:58,  0.78it/s, v_num=b5lg, train/loss_step=0.0542]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:51<00:58,  0.78it/s, v_num=b5lg, train/loss_step=0.0516]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:52<00:57,  0.78it/s, v_num=b5lg, train/loss_step=0.0516]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:52<00:57,  0.78it/s, v_num=b5lg, train/loss_step=0.0502]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:53<00:56,  0.78it/s, v_num=b5lg, train/loss_step=0.0502]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:53<00:56,  0.78it/s, v_num=b5lg, train/loss_step=0.0457]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:54<00:55,  0.78it/s, v_num=b5lg, train/loss_step=0.0457]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:54<00:55,  0.78it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:56<00:53,  0.78it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:56<00:53,  0.78it/s, v_num=b5lg, train/loss_step=0.0462]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:57<00:52,  0.78it/s, v_num=b5lg, train/loss_step=0.0462]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:57<00:52,  0.78it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:58<00:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0508]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:58<00:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0472]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:59<00:49,  0.78it/s, v_num=b5lg, train/loss_step=0.0472]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:59<00:49,  0.78it/s, v_num=b5lg, train/loss_step=0.0535]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [07:01<00:48,  0.78it/s, v_num=b5lg, train/loss_step=0.0535]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [07:01<00:48,  0.78it/s, v_num=b5lg, train/loss_step=0.0474]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [07:02<00:47,  0.78it/s, v_num=b5lg, train/loss_step=0.0474]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [07:02<00:47,  0.78it/s, v_num=b5lg, train/loss_step=0.0478]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [07:03<00:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0478]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [07:03<00:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0473]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [07:05<00:44,  0.78it/s, v_num=b5lg, train/loss_step=0.0473]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [07:05<00:44,  0.78it/s, v_num=b5lg, train/loss_step=0.0495]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [07:06<00:43,  0.78it/s, v_num=b5lg, train/loss_step=0.0495]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [07:06<00:43,  0.78it/s, v_num=b5lg, train/loss_step=0.0484]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [07:07<00:42,  0.78it/s, v_num=b5lg, train/loss_step=0.0484]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [07:07<00:42,  0.78it/s, v_num=b5lg, train/loss_step=0.0481]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [07:08<00:40,  0.78it/s, v_num=b5lg, train/loss_step=0.0481]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [07:08<00:40,  0.78it/s, v_num=b5lg, train/loss_step=0.0482]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [07:10<00:39,  0.78it/s, v_num=b5lg, train/loss_step=0.0482]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [07:10<00:39,  0.78it/s, v_num=b5lg, train/loss_step=0.0509]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [07:11<00:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0509]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [07:11<00:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0492]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [07:12<00:37,  0.78it/s, v_num=b5lg, train/loss_step=0.0492]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [07:12<00:37,  0.78it/s, v_num=b5lg, train/loss_step=0.0492]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [07:13<00:35,  0.78it/s, v_num=b5lg, train/loss_step=0.0492]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [07:13<00:35,  0.78it/s, v_num=b5lg, train/loss_step=0.0504]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [07:15<00:34,  0.78it/s, v_num=b5lg, train/loss_step=0.0504]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [07:15<00:34,  0.78it/s, v_num=b5lg, train/loss_step=0.0565]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:16<00:33,  0.78it/s, v_num=b5lg, train/loss_step=0.0565]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:16<00:33,  0.78it/s, v_num=b5lg, train/loss_step=0.0487]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:17<00:31,  0.78it/s, v_num=b5lg, train/loss_step=0.0487]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:17<00:31,  0.78it/s, v_num=b5lg, train/loss_step=0.0394]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:18<00:30,  0.78it/s, v_num=b5lg, train/loss_step=0.0394]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:18<00:30,  0.78it/s, v_num=b5lg, train/loss_step=0.0458]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:20<00:29,  0.78it/s, v_num=b5lg, train/loss_step=0.0458]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:20<00:29,  0.78it/s, v_num=b5lg, train/loss_step=0.0473]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:21<00:28,  0.78it/s, v_num=b5lg, train/loss_step=0.0473]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:21<00:28,  0.78it/s, v_num=b5lg, train/loss_step=0.045] Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:22<00:26,  0.78it/s, v_num=b5lg, train/loss_step=0.045]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:22<00:26,  0.78it/s, v_num=b5lg, train/loss_step=0.0487]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:23<00:25,  0.78it/s, v_num=b5lg, train/loss_step=0.0487]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:23<00:25,  0.78it/s, v_num=b5lg, train/loss_step=0.0435]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:24<00:24,  0.78it/s, v_num=b5lg, train/loss_step=0.0435]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:24<00:24,  0.78it/s, v_num=b5lg, train/loss_step=0.044] Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:26<00:23,  0.78it/s, v_num=b5lg, train/loss_step=0.044]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:26<00:23,  0.78it/s, v_num=b5lg, train/loss_step=0.049]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:27<00:21,  0.78it/s, v_num=b5lg, train/loss_step=0.049]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:27<00:21,  0.78it/s, v_num=b5lg, train/loss_step=0.0474]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:28<00:20,  0.78it/s, v_num=b5lg, train/loss_step=0.0474]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:28<00:20,  0.78it/s, v_num=b5lg, train/loss_step=0.0447]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:29<00:19,  0.78it/s, v_num=b5lg, train/loss_step=0.0447]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:29<00:19,  0.78it/s, v_num=b5lg, train/loss_step=0.0413]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:31<00:17,  0.78it/s, v_num=b5lg, train/loss_step=0.0413]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:31<00:17,  0.78it/s, v_num=b5lg, train/loss_step=0.0455]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:32<00:16,  0.78it/s, v_num=b5lg, train/loss_step=0.0455]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:32<00:16,  0.78it/s, v_num=b5lg, train/loss_step=0.0462]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:33<00:15,  0.78it/s, v_num=b5lg, train/loss_step=0.0462]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:33<00:15,  0.78it/s, v_num=b5lg, train/loss_step=0.0403]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:34<00:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0403]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:34<00:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0446]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:35<00:12,  0.78it/s, v_num=b5lg, train/loss_step=0.0446]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:35<00:12,  0.78it/s, v_num=b5lg, train/loss_step=0.0464]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:37<00:11,  0.78it/s, v_num=b5lg, train/loss_step=0.0464]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:37<00:11,  0.78it/s, v_num=b5lg, train/loss_step=0.0494]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:38<00:10,  0.78it/s, v_num=b5lg, train/loss_step=0.0494]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:38<00:10,  0.78it/s, v_num=b5lg, train/loss_step=0.0463]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:39<00:08,  0.78it/s, v_num=b5lg, train/loss_step=0.0463]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:39<00:08,  0.78it/s, v_num=b5lg, train/loss_step=0.0483]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:40<00:07,  0.78it/s, v_num=b5lg, train/loss_step=0.0483]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:40<00:07,  0.78it/s, v_num=b5lg, train/loss_step=0.0491]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:41<00:06,  0.78it/s, v_num=b5lg, train/loss_step=0.0491]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:41<00:06,  0.78it/s, v_num=b5lg, train/loss_step=0.0497]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:43<00:05,  0.78it/s, v_num=b5lg, train/loss_step=0.0497]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:43<00:05,  0.78it/s, v_num=b5lg, train/loss_step=0.0459]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:44<00:03,  0.78it/s, v_num=b5lg, train/loss_step=0.0459]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:44<00:03,  0.78it/s, v_num=b5lg, train/loss_step=0.0443]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:45<00:02,  0.78it/s, v_num=b5lg, train/loss_step=0.0443]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:45<00:02,  0.78it/s, v_num=b5lg, train/loss_step=0.0434]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:46<00:01,  0.78it/s, v_num=b5lg, train/loss_step=0.0434]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:46<00:01,  0.78it/s, v_num=b5lg, train/loss_step=0.0436]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:47<00:00,  0.78it/s, v_num=b5lg, train/loss_step=0.0436]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:47<00:00,  0.78it/s, v_num=b5lg, train/loss_step=0.0487]hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   2%|â–         | 1/41 [00:01<00:45,  0.88it/s][A
Validation DataLoader 0:   5%|â–         | 2/41 [00:01<00:34,  1.13it/s][A
Validation DataLoader 0:   7%|â–‹         | 3/41 [00:02<00:30,  1.24it/s][A
Validation DataLoader 0:  10%|â–‰         | 4/41 [00:03<00:28,  1.31it/s][A
Validation DataLoader 0:  12%|â–ˆâ–        | 5/41 [00:03<00:26,  1.36it/s][A
Validation DataLoader 0:  15%|â–ˆâ–        | 6/41 [00:04<00:25,  1.39it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 7/41 [00:04<00:24,  1.42it/s][A
Validation DataLoader 0:  20%|â–ˆâ–‰        | 8/41 [00:05<00:22,  1.44it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 9/41 [00:06<00:21,  1.46it/s][A
Validation DataLoader 0:  24%|â–ˆâ–ˆâ–       | 10/41 [00:06<00:21,  1.47it/s][A
Validation DataLoader 0:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:07<00:20,  1.48it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:08<00:19,  1.49it/s][A
Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:08<00:18,  1.50it/s][A
Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:09<00:17,  1.51it/s][A
Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:09<00:17,  1.51it/s][A
Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:10<00:16,  1.52it/s][A
Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:11<00:15,  1.53it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:11<00:15,  1.53it/s][A
Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:12<00:14,  1.53it/s][A
Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:13<00:13,  1.54it/s][A
Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:13<00:12,  1.54it/s][A
Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [00:14<00:12,  1.54it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:14<00:11,  1.55it/s][A
Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:15<00:10,  1.55it/s][A
Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:16<00:10,  1.55it/s][A
Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/41 [00:16<00:09,  1.55it/s][A
Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [00:17<00:08,  1.56it/s][A
Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [00:17<00:08,  1.56it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [00:18<00:07,  1.56it/s][A
Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/41 [00:19<00:07,  1.56it/s][A
Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [00:19<00:06,  1.56it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [00:20<00:05,  1.57it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:21<00:05,  1.57it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/41 [00:21<00:04,  1.57it/s][A
Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [00:22<00:03,  1.57it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [00:22<00:03,  1.57it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [00:23<00:02,  1.57it/s][A
Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 38/41 [00:24<00:01,  1.57it/s][A
Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [00:24<00:01,  1.57it/s][A
Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [00:25<00:00,  1.58it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.58it/s][A/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: It is recommended to use `self.log('val/mAP', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.

                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:20<00:00,  0.73it/s, v_num=b5lg, train/loss_step=0.0487, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:20<00:00,  0.73it/s, v_num=b5lg, train/loss_step=0.0487, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 0:   0%|          | 0/367 [00:00<?, ?it/s, v_num=b5lg, train/loss_step=0.0487, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]          Epoch 1:   0%|          | 0/367 [00:00<?, ?it/s, v_num=b5lg, train/loss_step=0.0487, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
Epoch 1:   0%|          | 1/367 [00:06<36:53,  0.17it/s, v_num=b5lg, train/loss_step=0.0487, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   0%|          | 1/367 [00:06<36:58,  0.16it/s, v_num=b5lg, train/loss_step=0.0479, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   1%|          | 2/367 [00:07<22:21,  0.27it/s, v_num=b5lg, train/loss_step=0.0479, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   1%|          | 2/367 [00:07<22:21,  0.27it/s, v_num=b5lg, train/loss_step=0.0446, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   1%|          | 3/367 [00:08<17:30,  0.35it/s, v_num=b5lg, train/loss_step=0.0446, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   1%|          | 3/367 [00:08<17:30,  0.35it/s, v_num=b5lg, train/loss_step=0.0423, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   1%|          | 4/367 [00:09<15:02,  0.40it/s, v_num=b5lg, train/loss_step=0.0423, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   1%|          | 4/367 [00:09<15:02,  0.40it/s, v_num=b5lg, train/loss_step=0.0492, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   1%|â–         | 5/367 [00:11<13:33,  0.44it/s, v_num=b5lg, train/loss_step=0.0492, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   1%|â–         | 5/367 [00:11<13:33,  0.44it/s, v_num=b5lg, train/loss_step=0.0491, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   2%|â–         | 6/367 [00:12<12:33,  0.48it/s, v_num=b5lg, train/loss_step=0.0491, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   2%|â–         | 6/367 [00:12<12:33,  0.48it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   2%|â–         | 7/367 [00:13<11:49,  0.51it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   2%|â–         | 7/367 [00:13<11:49,  0.51it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   2%|â–         | 8/367 [00:15<11:15,  0.53it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   2%|â–         | 8/367 [00:15<11:16,  0.53it/s, v_num=b5lg, train/loss_step=0.0467, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   2%|â–         | 9/367 [00:16<10:50,  0.55it/s, v_num=b5lg, train/loss_step=0.0467, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   2%|â–         | 9/367 [00:16<10:50,  0.55it/s, v_num=b5lg, train/loss_step=0.0443, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   3%|â–         | 10/367 [00:17<10:29,  0.57it/s, v_num=b5lg, train/loss_step=0.0443, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   3%|â–         | 10/367 [00:17<10:29,  0.57it/s, v_num=b5lg, train/loss_step=0.0436, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   3%|â–         | 11/367 [00:18<10:11,  0.58it/s, v_num=b5lg, train/loss_step=0.0436, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   3%|â–         | 11/367 [00:18<10:11,  0.58it/s, v_num=b5lg, train/loss_step=0.0433, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   3%|â–         | 12/367 [00:20<09:56,  0.60it/s, v_num=b5lg, train/loss_step=0.0433, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   3%|â–         | 12/367 [00:20<09:56,  0.60it/s, v_num=b5lg, train/loss_step=0.0448, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   4%|â–         | 13/367 [00:21<09:42,  0.61it/s, v_num=b5lg, train/loss_step=0.0448, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   4%|â–         | 13/367 [00:21<09:42,  0.61it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   4%|â–         | 14/367 [00:22<09:30,  0.62it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   4%|â–         | 14/367 [00:22<09:30,  0.62it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   4%|â–         | 15/367 [00:23<09:19,  0.63it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   4%|â–         | 15/367 [00:23<09:19,  0.63it/s, v_num=b5lg, train/loss_step=0.0487, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   4%|â–         | 16/367 [00:25<09:10,  0.64it/s, v_num=b5lg, train/loss_step=0.0487, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   4%|â–         | 16/367 [00:25<09:10,  0.64it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   5%|â–         | 17/367 [00:26<09:02,  0.65it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   5%|â–         | 17/367 [00:26<09:02,  0.65it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   5%|â–         | 18/367 [00:27<08:55,  0.65it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   5%|â–         | 18/367 [00:27<08:55,  0.65it/s, v_num=b5lg, train/loss_step=0.0469, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   5%|â–Œ         | 19/367 [00:28<08:49,  0.66it/s, v_num=b5lg, train/loss_step=0.0469, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   5%|â–Œ         | 19/367 [00:28<08:49,  0.66it/s, v_num=b5lg, train/loss_step=0.038, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:   5%|â–Œ         | 20/367 [00:30<08:43,  0.66it/s, v_num=b5lg, train/loss_step=0.038, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   5%|â–Œ         | 20/367 [00:30<08:43,  0.66it/s, v_num=b5lg, train/loss_step=0.0445, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   6%|â–Œ         | 21/367 [00:31<08:38,  0.67it/s, v_num=b5lg, train/loss_step=0.0445, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   6%|â–Œ         | 21/367 [00:31<08:38,  0.67it/s, v_num=b5lg, train/loss_step=0.0468, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   6%|â–Œ         | 22/367 [00:32<08:33,  0.67it/s, v_num=b5lg, train/loss_step=0.0468, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   6%|â–Œ         | 22/367 [00:32<08:33,  0.67it/s, v_num=b5lg, train/loss_step=0.044, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:   6%|â–‹         | 23/367 [00:34<08:28,  0.68it/s, v_num=b5lg, train/loss_step=0.044, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   6%|â–‹         | 23/367 [00:34<08:28,  0.68it/s, v_num=b5lg, train/loss_step=0.0475, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   7%|â–‹         | 24/367 [00:35<08:24,  0.68it/s, v_num=b5lg, train/loss_step=0.0475, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   7%|â–‹         | 24/367 [00:35<08:24,  0.68it/s, v_num=b5lg, train/loss_step=0.040, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:   7%|â–‹         | 25/367 [00:36<08:20,  0.68it/s, v_num=b5lg, train/loss_step=0.040, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   7%|â–‹         | 25/367 [00:36<08:20,  0.68it/s, v_num=b5lg, train/loss_step=0.044, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   7%|â–‹         | 26/367 [00:37<08:16,  0.69it/s, v_num=b5lg, train/loss_step=0.044, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   7%|â–‹         | 26/367 [00:37<08:16,  0.69it/s, v_num=b5lg, train/loss_step=0.0439, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   7%|â–‹         | 27/367 [00:39<08:12,  0.69it/s, v_num=b5lg, train/loss_step=0.0439, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   7%|â–‹         | 27/367 [00:39<08:12,  0.69it/s, v_num=b5lg, train/loss_step=0.0474, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   8%|â–Š         | 28/367 [00:40<08:09,  0.69it/s, v_num=b5lg, train/loss_step=0.0474, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   8%|â–Š         | 28/367 [00:40<08:09,  0.69it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   8%|â–Š         | 29/367 [00:41<08:05,  0.70it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   8%|â–Š         | 29/367 [00:41<08:05,  0.70it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   8%|â–Š         | 30/367 [00:42<08:01,  0.70it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   8%|â–Š         | 30/367 [00:42<08:01,  0.70it/s, v_num=b5lg, train/loss_step=0.0436, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   8%|â–Š         | 31/367 [00:44<07:57,  0.70it/s, v_num=b5lg, train/loss_step=0.0436, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   8%|â–Š         | 31/367 [00:44<07:57,  0.70it/s, v_num=b5lg, train/loss_step=0.0434, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   9%|â–Š         | 32/367 [00:45<07:54,  0.71it/s, v_num=b5lg, train/loss_step=0.0434, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   9%|â–Š         | 32/367 [00:45<07:54,  0.71it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   9%|â–‰         | 33/367 [00:46<07:51,  0.71it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   9%|â–‰         | 33/367 [00:46<07:51,  0.71it/s, v_num=b5lg, train/loss_step=0.0425, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   9%|â–‰         | 34/367 [00:47<07:48,  0.71it/s, v_num=b5lg, train/loss_step=0.0425, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:   9%|â–‰         | 34/367 [00:47<07:48,  0.71it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  10%|â–‰         | 35/367 [00:49<07:45,  0.71it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  10%|â–‰         | 35/367 [00:49<07:45,  0.71it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  10%|â–‰         | 36/367 [00:50<07:42,  0.72it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  10%|â–‰         | 36/367 [00:50<07:42,  0.72it/s, v_num=b5lg, train/loss_step=0.0433, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  10%|â–ˆ         | 37/367 [00:51<07:39,  0.72it/s, v_num=b5lg, train/loss_step=0.0433, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  10%|â–ˆ         | 37/367 [00:51<07:39,  0.72it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  10%|â–ˆ         | 38/367 [00:52<07:36,  0.72it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  10%|â–ˆ         | 38/367 [00:52<07:36,  0.72it/s, v_num=b5lg, train/loss_step=0.0428, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  11%|â–ˆ         | 39/367 [00:53<07:33,  0.72it/s, v_num=b5lg, train/loss_step=0.0428, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  11%|â–ˆ         | 39/367 [00:53<07:33,  0.72it/s, v_num=b5lg, train/loss_step=0.0429, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  11%|â–ˆ         | 40/367 [00:55<07:31,  0.72it/s, v_num=b5lg, train/loss_step=0.0429, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  11%|â–ˆ         | 40/367 [00:55<07:31,  0.72it/s, v_num=b5lg, train/loss_step=0.0463, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  11%|â–ˆ         | 41/367 [00:56<07:28,  0.73it/s, v_num=b5lg, train/loss_step=0.0463, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  11%|â–ˆ         | 41/367 [00:56<07:28,  0.73it/s, v_num=b5lg, train/loss_step=0.0435, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  11%|â–ˆâ–        | 42/367 [00:57<07:26,  0.73it/s, v_num=b5lg, train/loss_step=0.0435, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  11%|â–ˆâ–        | 42/367 [00:57<07:26,  0.73it/s, v_num=b5lg, train/loss_step=0.0458, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  12%|â–ˆâ–        | 43/367 [00:58<07:24,  0.73it/s, v_num=b5lg, train/loss_step=0.0458, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  12%|â–ˆâ–        | 43/367 [00:58<07:24,  0.73it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  12%|â–ˆâ–        | 44/367 [01:00<07:22,  0.73it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  12%|â–ˆâ–        | 44/367 [01:00<07:22,  0.73it/s, v_num=b5lg, train/loss_step=0.040, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  12%|â–ˆâ–        | 45/367 [01:01<07:20,  0.73it/s, v_num=b5lg, train/loss_step=0.040, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  12%|â–ˆâ–        | 45/367 [01:01<07:20,  0.73it/s, v_num=b5lg, train/loss_step=0.0431, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  13%|â–ˆâ–        | 46/367 [01:02<07:18,  0.73it/s, v_num=b5lg, train/loss_step=0.0431, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  13%|â–ˆâ–        | 46/367 [01:02<07:18,  0.73it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  13%|â–ˆâ–        | 47/367 [01:04<07:16,  0.73it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  13%|â–ˆâ–        | 47/367 [01:04<07:16,  0.73it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  13%|â–ˆâ–        | 48/367 [01:05<07:14,  0.73it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  13%|â–ˆâ–        | 48/367 [01:05<07:14,  0.73it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  13%|â–ˆâ–        | 49/367 [01:06<07:12,  0.74it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  13%|â–ˆâ–        | 49/367 [01:06<07:12,  0.74it/s, v_num=b5lg, train/loss_step=0.0516, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  14%|â–ˆâ–        | 50/367 [01:07<07:10,  0.74it/s, v_num=b5lg, train/loss_step=0.0516, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  14%|â–ˆâ–        | 50/367 [01:07<07:10,  0.74it/s, v_num=b5lg, train/loss_step=0.0475, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  14%|â–ˆâ–        | 51/367 [01:09<07:08,  0.74it/s, v_num=b5lg, train/loss_step=0.0475, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  14%|â–ˆâ–        | 51/367 [01:09<07:08,  0.74it/s, v_num=b5lg, train/loss_step=0.0421, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  14%|â–ˆâ–        | 52/367 [01:10<07:06,  0.74it/s, v_num=b5lg, train/loss_step=0.0421, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  14%|â–ˆâ–        | 52/367 [01:10<07:06,  0.74it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  14%|â–ˆâ–        | 53/367 [01:11<07:04,  0.74it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  14%|â–ˆâ–        | 53/367 [01:11<07:04,  0.74it/s, v_num=b5lg, train/loss_step=0.042, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  15%|â–ˆâ–        | 54/367 [01:12<07:02,  0.74it/s, v_num=b5lg, train/loss_step=0.042, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  15%|â–ˆâ–        | 54/367 [01:12<07:02,  0.74it/s, v_num=b5lg, train/loss_step=0.0466, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  15%|â–ˆâ–        | 55/367 [01:14<07:00,  0.74it/s, v_num=b5lg, train/loss_step=0.0466, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  15%|â–ˆâ–        | 55/367 [01:14<07:00,  0.74it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  15%|â–ˆâ–Œ        | 56/367 [01:15<06:58,  0.74it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  15%|â–ˆâ–Œ        | 56/367 [01:15<06:58,  0.74it/s, v_num=b5lg, train/loss_step=0.0472, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  16%|â–ˆâ–Œ        | 57/367 [01:16<06:56,  0.74it/s, v_num=b5lg, train/loss_step=0.0472, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  16%|â–ˆâ–Œ        | 57/367 [01:16<06:56,  0.74it/s, v_num=b5lg, train/loss_step=0.0426, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  16%|â–ˆâ–Œ        | 58/367 [01:17<06:54,  0.75it/s, v_num=b5lg, train/loss_step=0.0426, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  16%|â–ˆâ–Œ        | 58/367 [01:17<06:54,  0.75it/s, v_num=b5lg, train/loss_step=0.0435, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  16%|â–ˆâ–Œ        | 59/367 [01:19<06:52,  0.75it/s, v_num=b5lg, train/loss_step=0.0435, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  16%|â–ˆâ–Œ        | 59/367 [01:19<06:52,  0.75it/s, v_num=b5lg, train/loss_step=0.0436, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  16%|â–ˆâ–‹        | 60/367 [01:20<06:50,  0.75it/s, v_num=b5lg, train/loss_step=0.0436, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  16%|â–ˆâ–‹        | 60/367 [01:20<06:50,  0.75it/s, v_num=b5lg, train/loss_step=0.0419, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  17%|â–ˆâ–‹        | 61/367 [01:21<06:48,  0.75it/s, v_num=b5lg, train/loss_step=0.0419, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  17%|â–ˆâ–‹        | 61/367 [01:21<06:48,  0.75it/s, v_num=b5lg, train/loss_step=0.044, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  17%|â–ˆâ–‹        | 62/367 [01:22<06:47,  0.75it/s, v_num=b5lg, train/loss_step=0.044, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  17%|â–ˆâ–‹        | 62/367 [01:22<06:47,  0.75it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  17%|â–ˆâ–‹        | 63/367 [01:23<06:45,  0.75it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  17%|â–ˆâ–‹        | 63/367 [01:23<06:45,  0.75it/s, v_num=b5lg, train/loss_step=0.0451, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  17%|â–ˆâ–‹        | 64/367 [01:25<06:43,  0.75it/s, v_num=b5lg, train/loss_step=0.0451, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  17%|â–ˆâ–‹        | 64/367 [01:25<06:43,  0.75it/s, v_num=b5lg, train/loss_step=0.0455, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  18%|â–ˆâ–Š        | 65/367 [01:26<06:41,  0.75it/s, v_num=b5lg, train/loss_step=0.0455, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  18%|â–ˆâ–Š        | 65/367 [01:26<06:41,  0.75it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  18%|â–ˆâ–Š        | 66/367 [01:27<06:39,  0.75it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  18%|â–ˆâ–Š        | 66/367 [01:27<06:39,  0.75it/s, v_num=b5lg, train/loss_step=0.0427, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  18%|â–ˆâ–Š        | 67/367 [01:28<06:38,  0.75it/s, v_num=b5lg, train/loss_step=0.0427, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  18%|â–ˆâ–Š        | 67/367 [01:28<06:38,  0.75it/s, v_num=b5lg, train/loss_step=0.0444, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  19%|â–ˆâ–Š        | 68/367 [01:30<06:36,  0.75it/s, v_num=b5lg, train/loss_step=0.0444, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  19%|â–ˆâ–Š        | 68/367 [01:30<06:36,  0.75it/s, v_num=b5lg, train/loss_step=0.0425, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  19%|â–ˆâ–‰        | 69/367 [01:31<06:34,  0.76it/s, v_num=b5lg, train/loss_step=0.0425, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  19%|â–ˆâ–‰        | 69/367 [01:31<06:34,  0.76it/s, v_num=b5lg, train/loss_step=0.0464, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  19%|â–ˆâ–‰        | 70/367 [01:32<06:32,  0.76it/s, v_num=b5lg, train/loss_step=0.0464, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  19%|â–ˆâ–‰        | 70/367 [01:32<06:32,  0.76it/s, v_num=b5lg, train/loss_step=0.0501, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  19%|â–ˆâ–‰        | 71/367 [01:33<06:31,  0.76it/s, v_num=b5lg, train/loss_step=0.0501, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  19%|â–ˆâ–‰        | 71/367 [01:33<06:31,  0.76it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  20%|â–ˆâ–‰        | 72/367 [01:35<06:29,  0.76it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  20%|â–ˆâ–‰        | 72/367 [01:35<06:29,  0.76it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  20%|â–ˆâ–‰        | 73/367 [01:36<06:27,  0.76it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  20%|â–ˆâ–‰        | 73/367 [01:36<06:27,  0.76it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  20%|â–ˆâ–ˆ        | 74/367 [01:37<06:26,  0.76it/s, v_num=b5lg, train/loss_step=0.0437, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  20%|â–ˆâ–ˆ        | 74/367 [01:37<06:26,  0.76it/s, v_num=b5lg, train/loss_step=0.0454, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  20%|â–ˆâ–ˆ        | 75/367 [01:38<06:24,  0.76it/s, v_num=b5lg, train/loss_step=0.0454, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  20%|â–ˆâ–ˆ        | 75/367 [01:38<06:24,  0.76it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  21%|â–ˆâ–ˆ        | 76/367 [01:40<06:22,  0.76it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  21%|â–ˆâ–ˆ        | 76/367 [01:40<06:22,  0.76it/s, v_num=b5lg, train/loss_step=0.044, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  21%|â–ˆâ–ˆ        | 77/367 [01:41<06:21,  0.76it/s, v_num=b5lg, train/loss_step=0.044, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  21%|â–ˆâ–ˆ        | 77/367 [01:41<06:21,  0.76it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  21%|â–ˆâ–ˆâ–       | 78/367 [01:42<06:19,  0.76it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  21%|â–ˆâ–ˆâ–       | 78/367 [01:42<06:19,  0.76it/s, v_num=b5lg, train/loss_step=0.0457, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  22%|â–ˆâ–ˆâ–       | 79/367 [01:43<06:18,  0.76it/s, v_num=b5lg, train/loss_step=0.0457, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  22%|â–ˆâ–ˆâ–       | 79/367 [01:43<06:18,  0.76it/s, v_num=b5lg, train/loss_step=0.0431, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  22%|â–ˆâ–ˆâ–       | 80/367 [01:44<06:16,  0.76it/s, v_num=b5lg, train/loss_step=0.0431, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  22%|â–ˆâ–ˆâ–       | 80/367 [01:44<06:16,  0.76it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  22%|â–ˆâ–ˆâ–       | 81/367 [01:46<06:14,  0.76it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  22%|â–ˆâ–ˆâ–       | 81/367 [01:46<06:14,  0.76it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  22%|â–ˆâ–ˆâ–       | 82/367 [01:47<06:13,  0.76it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  22%|â–ˆâ–ˆâ–       | 82/367 [01:47<06:13,  0.76it/s, v_num=b5lg, train/loss_step=0.0467, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  23%|â–ˆâ–ˆâ–       | 83/367 [01:48<06:11,  0.76it/s, v_num=b5lg, train/loss_step=0.0467, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  23%|â–ˆâ–ˆâ–       | 83/367 [01:48<06:11,  0.76it/s, v_num=b5lg, train/loss_step=0.0392, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  23%|â–ˆâ–ˆâ–       | 84/367 [01:49<06:09,  0.76it/s, v_num=b5lg, train/loss_step=0.0392, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  23%|â–ˆâ–ˆâ–       | 84/367 [01:49<06:09,  0.76it/s, v_num=b5lg, train/loss_step=0.0395, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  23%|â–ˆâ–ˆâ–       | 85/367 [01:51<06:08,  0.77it/s, v_num=b5lg, train/loss_step=0.0395, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  23%|â–ˆâ–ˆâ–       | 85/367 [01:51<06:08,  0.77it/s, v_num=b5lg, train/loss_step=0.0492, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  23%|â–ˆâ–ˆâ–       | 86/367 [01:52<06:06,  0.77it/s, v_num=b5lg, train/loss_step=0.0492, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  23%|â–ˆâ–ˆâ–       | 86/367 [01:52<06:06,  0.77it/s, v_num=b5lg, train/loss_step=0.0436, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  24%|â–ˆâ–ˆâ–       | 87/367 [01:53<06:05,  0.77it/s, v_num=b5lg, train/loss_step=0.0436, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  24%|â–ˆâ–ˆâ–       | 87/367 [01:53<06:05,  0.77it/s, v_num=b5lg, train/loss_step=0.042, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  24%|â–ˆâ–ˆâ–       | 88/367 [01:54<06:03,  0.77it/s, v_num=b5lg, train/loss_step=0.042, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  24%|â–ˆâ–ˆâ–       | 88/367 [01:54<06:03,  0.77it/s, v_num=b5lg, train/loss_step=0.0414, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  24%|â–ˆâ–ˆâ–       | 89/367 [01:55<06:02,  0.77it/s, v_num=b5lg, train/loss_step=0.0414, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  24%|â–ˆâ–ˆâ–       | 89/367 [01:55<06:02,  0.77it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  25%|â–ˆâ–ˆâ–       | 90/367 [01:57<06:00,  0.77it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  25%|â–ˆâ–ˆâ–       | 90/367 [01:57<06:00,  0.77it/s, v_num=b5lg, train/loss_step=0.0452, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  25%|â–ˆâ–ˆâ–       | 91/367 [01:58<05:58,  0.77it/s, v_num=b5lg, train/loss_step=0.0452, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  25%|â–ˆâ–ˆâ–       | 91/367 [01:58<05:58,  0.77it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [01:59<05:57,  0.77it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [01:59<05:57,  0.77it/s, v_num=b5lg, train/loss_step=0.0472, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [02:00<05:55,  0.77it/s, v_num=b5lg, train/loss_step=0.0472, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [02:00<05:55,  0.77it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [02:02<05:54,  0.77it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [02:02<05:54,  0.77it/s, v_num=b5lg, train/loss_step=0.0422, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [02:03<05:53,  0.77it/s, v_num=b5lg, train/loss_step=0.0422, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [02:03<05:53,  0.77it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [02:04<05:51,  0.77it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [02:04<05:51,  0.77it/s, v_num=b5lg, train/loss_step=0.0403, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:05<05:50,  0.77it/s, v_num=b5lg, train/loss_step=0.0403, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:05<05:50,  0.77it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:07<05:48,  0.77it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:07<05:48,  0.77it/s, v_num=b5lg, train/loss_step=0.0466, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:08<05:47,  0.77it/s, v_num=b5lg, train/loss_step=0.0466, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:08<05:47,  0.77it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:09<05:45,  0.77it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:09<05:45,  0.77it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:10<05:44,  0.77it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:10<05:44,  0.77it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:12<05:43,  0.77it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:12<05:43,  0.77it/s, v_num=b5lg, train/loss_step=0.0434, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:13<05:41,  0.77it/s, v_num=b5lg, train/loss_step=0.0434, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:13<05:41,  0.77it/s, v_num=b5lg, train/loss_step=0.0415, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:14<05:40,  0.77it/s, v_num=b5lg, train/loss_step=0.0415, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:14<05:40,  0.77it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:15<05:39,  0.77it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:15<05:39,  0.77it/s, v_num=b5lg, train/loss_step=0.0412, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:17<05:37,  0.77it/s, v_num=b5lg, train/loss_step=0.0412, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:17<05:37,  0.77it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:18<05:36,  0.77it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:18<05:36,  0.77it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:19<05:35,  0.77it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:19<05:35,  0.77it/s, v_num=b5lg, train/loss_step=0.0442, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:21<05:33,  0.77it/s, v_num=b5lg, train/loss_step=0.0442, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:21<05:33,  0.77it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:22<05:32,  0.77it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:22<05:32,  0.77it/s, v_num=b5lg, train/loss_step=0.0423, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:23<05:31,  0.77it/s, v_num=b5lg, train/loss_step=0.0423, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:23<05:31,  0.77it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:24<05:29,  0.77it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:24<05:29,  0.77it/s, v_num=b5lg, train/loss_step=0.0406, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:26<05:28,  0.77it/s, v_num=b5lg, train/loss_step=0.0406, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:26<05:28,  0.77it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:27<05:27,  0.77it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:27<05:27,  0.77it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:28<05:25,  0.77it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:28<05:25,  0.77it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:29<05:24,  0.77it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:29<05:24,  0.77it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:31<05:22,  0.77it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:31<05:22,  0.77it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:32<05:21,  0.77it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:32<05:21,  0.77it/s, v_num=b5lg, train/loss_step=0.0428, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:33<05:20,  0.77it/s, v_num=b5lg, train/loss_step=0.0428, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:33<05:20,  0.77it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:34<05:18,  0.78it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:34<05:18,  0.78it/s, v_num=b5lg, train/loss_step=0.0422, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:36<05:17,  0.78it/s, v_num=b5lg, train/loss_step=0.0422, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:36<05:17,  0.78it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:37<05:15,  0.78it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:37<05:15,  0.78it/s, v_num=b5lg, train/loss_step=0.0469, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:38<05:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0469, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:38<05:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:39<05:13,  0.78it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:39<05:13,  0.78it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:41<05:11,  0.78it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:41<05:11,  0.78it/s, v_num=b5lg, train/loss_step=0.043, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:42<05:10,  0.78it/s, v_num=b5lg, train/loss_step=0.043, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:42<05:10,  0.78it/s, v_num=b5lg, train/loss_step=0.039, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:43<05:08,  0.78it/s, v_num=b5lg, train/loss_step=0.039, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:43<05:08,  0.78it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:44<05:07,  0.78it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:44<05:07,  0.78it/s, v_num=b5lg, train/loss_step=0.0446, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:45<05:06,  0.78it/s, v_num=b5lg, train/loss_step=0.0446, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:45<05:06,  0.78it/s, v_num=b5lg, train/loss_step=0.045, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:47<05:04,  0.78it/s, v_num=b5lg, train/loss_step=0.045, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:47<05:04,  0.78it/s, v_num=b5lg, train/loss_step=0.0433, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:48<05:03,  0.78it/s, v_num=b5lg, train/loss_step=0.0433, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:48<05:03,  0.78it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:49<05:02,  0.78it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:49<05:02,  0.78it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:50<05:00,  0.78it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:50<05:00,  0.78it/s, v_num=b5lg, train/loss_step=0.045, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:52<04:59,  0.78it/s, v_num=b5lg, train/loss_step=0.045, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:52<04:59,  0.78it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [02:53<04:57,  0.78it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [02:53<04:57,  0.78it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [02:54<04:56,  0.78it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [02:54<04:56,  0.78it/s, v_num=b5lg, train/loss_step=0.0426, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [02:55<04:55,  0.78it/s, v_num=b5lg, train/loss_step=0.0426, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [02:55<04:55,  0.78it/s, v_num=b5lg, train/loss_step=0.0409, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [02:57<04:53,  0.78it/s, v_num=b5lg, train/loss_step=0.0409, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [02:57<04:53,  0.78it/s, v_num=b5lg, train/loss_step=0.0409, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [02:58<04:52,  0.78it/s, v_num=b5lg, train/loss_step=0.0409, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [02:58<04:52,  0.78it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [02:59<04:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [02:59<04:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [03:00<04:49,  0.78it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [03:00<04:49,  0.78it/s, v_num=b5lg, train/loss_step=0.0402, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [03:01<04:48,  0.78it/s, v_num=b5lg, train/loss_step=0.0402, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [03:01<04:48,  0.78it/s, v_num=b5lg, train/loss_step=0.0431, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [03:03<04:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0431, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [03:03<04:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [03:04<04:45,  0.78it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [03:04<04:45,  0.78it/s, v_num=b5lg, train/loss_step=0.0454, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [03:05<04:44,  0.78it/s, v_num=b5lg, train/loss_step=0.0454, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [03:05<04:44,  0.78it/s, v_num=b5lg, train/loss_step=0.0451, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:06<04:42,  0.78it/s, v_num=b5lg, train/loss_step=0.0451, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:06<04:42,  0.78it/s, v_num=b5lg, train/loss_step=0.0401, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:08<04:41,  0.78it/s, v_num=b5lg, train/loss_step=0.0401, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:08<04:41,  0.78it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:09<04:40,  0.78it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:09<04:40,  0.78it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:10<04:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:10<04:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:11<04:37,  0.78it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:11<04:37,  0.78it/s, v_num=b5lg, train/loss_step=0.0428, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:13<04:36,  0.78it/s, v_num=b5lg, train/loss_step=0.0428, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:13<04:36,  0.78it/s, v_num=b5lg, train/loss_step=0.0434, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:14<04:34,  0.78it/s, v_num=b5lg, train/loss_step=0.0434, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:14<04:34,  0.78it/s, v_num=b5lg, train/loss_step=0.042, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:15<04:33,  0.78it/s, v_num=b5lg, train/loss_step=0.042, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:15<04:33,  0.78it/s, v_num=b5lg, train/loss_step=0.0431, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:16<04:32,  0.78it/s, v_num=b5lg, train/loss_step=0.0431, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:16<04:32,  0.78it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:17<04:30,  0.78it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:17<04:30,  0.78it/s, v_num=b5lg, train/loss_step=0.0461, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:19<04:29,  0.78it/s, v_num=b5lg, train/loss_step=0.0461, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:19<04:29,  0.78it/s, v_num=b5lg, train/loss_step=0.0401, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:20<04:28,  0.78it/s, v_num=b5lg, train/loss_step=0.0401, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:20<04:28,  0.78it/s, v_num=b5lg, train/loss_step=0.042, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:21<04:26,  0.78it/s, v_num=b5lg, train/loss_step=0.042, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:21<04:26,  0.78it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:22<04:25,  0.78it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:22<04:25,  0.78it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:24<04:24,  0.78it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:24<04:24,  0.78it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:25<04:22,  0.78it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:25<04:22,  0.78it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:26<04:21,  0.78it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:26<04:21,  0.78it/s, v_num=b5lg, train/loss_step=0.0474, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:27<04:20,  0.78it/s, v_num=b5lg, train/loss_step=0.0474, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:27<04:20,  0.78it/s, v_num=b5lg, train/loss_step=0.0419, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:29<04:18,  0.78it/s, v_num=b5lg, train/loss_step=0.0419, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:29<04:18,  0.78it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:30<04:17,  0.78it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:30<04:17,  0.78it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:31<04:16,  0.78it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:31<04:16,  0.78it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:32<04:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:32<04:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:33<04:13,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:33<04:13,  0.79it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:35<04:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:35<04:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:36<04:10,  0.79it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:36<04:10,  0.79it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:37<04:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:37<04:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:38<04:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:38<04:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0472, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:40<04:06,  0.79it/s, v_num=b5lg, train/loss_step=0.0472, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:40<04:06,  0.79it/s, v_num=b5lg, train/loss_step=0.0363, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:41<04:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0363, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:41<04:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:42<04:04,  0.79it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:42<04:04,  0.79it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:43<04:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:43<04:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0425, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:45<04:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0425, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:45<04:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:46<04:00,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:46<04:00,  0.79it/s, v_num=b5lg, train/loss_step=0.046, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:47<03:59,  0.79it/s, v_num=b5lg, train/loss_step=0.046, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:47<03:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0406, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:48<03:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0406, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:48<03:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:50<03:56,  0.79it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:50<03:56,  0.79it/s, v_num=b5lg, train/loss_step=0.0388, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:51<03:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0388, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:51<03:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:52<03:53,  0.79it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:52<03:53,  0.79it/s, v_num=b5lg, train/loss_step=0.0401, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [03:53<03:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0401, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [03:53<03:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0406, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [03:54<03:51,  0.79it/s, v_num=b5lg, train/loss_step=0.0406, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [03:54<03:51,  0.79it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [03:56<03:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [03:56<03:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [03:57<03:48,  0.79it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [03:57<03:48,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [03:58<03:47,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [03:58<03:47,  0.79it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [03:59<03:46,  0.79it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [03:59<03:46,  0.79it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [04:01<03:44,  0.79it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [04:01<03:44,  0.79it/s, v_num=b5lg, train/loss_step=0.0393, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [04:02<03:43,  0.79it/s, v_num=b5lg, train/loss_step=0.0393, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [04:02<03:43,  0.79it/s, v_num=b5lg, train/loss_step=0.0422, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [04:03<03:42,  0.79it/s, v_num=b5lg, train/loss_step=0.0422, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [04:03<03:42,  0.79it/s, v_num=b5lg, train/loss_step=0.0456, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [04:04<03:40,  0.79it/s, v_num=b5lg, train/loss_step=0.0456, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [04:04<03:40,  0.79it/s, v_num=b5lg, train/loss_step=0.0407, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:06<03:39,  0.79it/s, v_num=b5lg, train/loss_step=0.0407, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:06<03:39,  0.79it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:07<03:38,  0.79it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:07<03:38,  0.79it/s, v_num=b5lg, train/loss_step=0.041, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:08<03:36,  0.79it/s, v_num=b5lg, train/loss_step=0.041, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:08<03:36,  0.79it/s, v_num=b5lg, train/loss_step=0.0423, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:09<03:35,  0.79it/s, v_num=b5lg, train/loss_step=0.0423, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:09<03:35,  0.79it/s, v_num=b5lg, train/loss_step=0.0394, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:11<03:34,  0.79it/s, v_num=b5lg, train/loss_step=0.0394, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:11<03:34,  0.79it/s, v_num=b5lg, train/loss_step=0.0413, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:12<03:33,  0.79it/s, v_num=b5lg, train/loss_step=0.0413, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:12<03:33,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:13<03:31,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:13<03:31,  0.79it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:14<03:30,  0.79it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:14<03:30,  0.79it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:16<03:29,  0.79it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:16<03:29,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:17<03:27,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:17<03:27,  0.79it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:18<03:26,  0.79it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:18<03:26,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:19<03:25,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:19<03:25,  0.79it/s, v_num=b5lg, train/loss_step=0.0412, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:20<03:23,  0.79it/s, v_num=b5lg, train/loss_step=0.0412, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:20<03:23,  0.79it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:22<03:22,  0.79it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:22<03:22,  0.79it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:23<03:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:23<03:21,  0.79it/s, v_num=b5lg, train/loss_step=0.038, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:24<03:20,  0.79it/s, v_num=b5lg, train/loss_step=0.038, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:24<03:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:25<03:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:25<03:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:27<03:17,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:27<03:17,  0.79it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:28<03:16,  0.79it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:28<03:16,  0.79it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:29<03:14,  0.79it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:29<03:14,  0.79it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:30<03:13,  0.79it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:30<03:13,  0.79it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:32<03:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:32<03:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:33<03:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:33<03:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0395, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:34<03:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0395, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:34<03:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:35<03:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:35<03:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:36<03:07,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:36<03:07,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:38<03:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:38<03:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:39<03:04,  0.79it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:39<03:04,  0.79it/s, v_num=b5lg, train/loss_step=0.043, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:40<03:03,  0.79it/s, v_num=b5lg, train/loss_step=0.043, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:40<03:03,  0.79it/s, v_num=b5lg, train/loss_step=0.0429, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:42<03:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0429, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:42<03:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:43<03:00,  0.79it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:43<03:00,  0.79it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:44<02:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:44<02:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0394, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:45<02:58,  0.79it/s, v_num=b5lg, train/loss_step=0.0394, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:45<02:58,  0.79it/s, v_num=b5lg, train/loss_step=0.0394, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:47<02:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0394, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:47<02:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:48<02:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:48<02:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:49<02:54,  0.79it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:49<02:54,  0.79it/s, v_num=b5lg, train/loss_step=0.0444, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:50<02:53,  0.79it/s, v_num=b5lg, train/loss_step=0.0444, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:50<02:53,  0.79it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:51<02:51,  0.79it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:51<02:51,  0.79it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [04:53<02:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [04:53<02:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0419, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [04:54<02:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0419, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [04:54<02:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0388, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [04:55<02:48,  0.79it/s, v_num=b5lg, train/loss_step=0.0388, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [04:55<02:48,  0.79it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [04:56<02:46,  0.79it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [04:56<02:46,  0.79it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [04:58<02:45,  0.79it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [04:58<02:45,  0.79it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [04:59<02:44,  0.79it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [04:59<02:44,  0.79it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [05:00<02:42,  0.79it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [05:00<02:42,  0.79it/s, v_num=b5lg, train/loss_step=0.038, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [05:01<02:41,  0.79it/s, v_num=b5lg, train/loss_step=0.038, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [05:01<02:41,  0.79it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [05:03<02:40,  0.79it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [05:03<02:40,  0.79it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [05:04<02:39,  0.79it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [05:04<02:39,  0.79it/s, v_num=b5lg, train/loss_step=0.0407, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [05:05<02:37,  0.79it/s, v_num=b5lg, train/loss_step=0.0407, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [05:05<02:37,  0.79it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:06<02:36,  0.79it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:06<02:36,  0.79it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:07<02:35,  0.79it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:07<02:35,  0.79it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:09<02:33,  0.79it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:09<02:33,  0.79it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:10<02:32,  0.79it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:10<02:32,  0.79it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:11<02:31,  0.79it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:11<02:31,  0.79it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:12<02:30,  0.79it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:12<02:30,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:14<02:28,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:14<02:28,  0.79it/s, v_num=b5lg, train/loss_step=0.0421, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:15<02:27,  0.79it/s, v_num=b5lg, train/loss_step=0.0421, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:15<02:27,  0.79it/s, v_num=b5lg, train/loss_step=0.0402, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:16<02:26,  0.79it/s, v_num=b5lg, train/loss_step=0.0402, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:16<02:26,  0.79it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:18<02:25,  0.79it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:18<02:25,  0.79it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:19<02:23,  0.79it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:19<02:23,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:20<02:22,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:20<02:22,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:21<02:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:21<02:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:23<02:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:23<02:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:24<02:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:24<02:18,  0.79it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:25<02:17,  0.79it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:25<02:17,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:27<02:16,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:27<02:16,  0.79it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:28<02:15,  0.79it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:28<02:15,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:29<02:13,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:29<02:13,  0.79it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:30<02:12,  0.79it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:30<02:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:32<02:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:32<02:11,  0.79it/s, v_num=b5lg, train/loss_step=0.040, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:33<02:10,  0.79it/s, v_num=b5lg, train/loss_step=0.040, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:33<02:10,  0.79it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:34<02:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:34<02:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:35<02:07,  0.79it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:35<02:07,  0.79it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:37<02:06,  0.79it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:37<02:06,  0.79it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:38<02:04,  0.79it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:38<02:04,  0.79it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:39<02:03,  0.79it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:39<02:03,  0.79it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:40<02:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:40<02:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0409, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:42<02:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0409, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:42<02:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:43<01:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:43<01:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0395, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:44<01:58,  0.79it/s, v_num=b5lg, train/loss_step=0.0395, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:44<01:58,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:45<01:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:45<01:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:46<01:56,  0.79it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:46<01:56,  0.79it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:48<01:54,  0.79it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:48<01:54,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:49<01:53,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:49<01:53,  0.79it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:50<01:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:50<01:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0439, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:51<01:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0439, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:51<01:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [05:53<01:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [05:53<01:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [05:54<01:48,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [05:54<01:48,  0.79it/s, v_num=b5lg, train/loss_step=0.038, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [05:55<01:47,  0.79it/s, v_num=b5lg, train/loss_step=0.038, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [05:55<01:47,  0.79it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [05:56<01:45,  0.79it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [05:56<01:45,  0.79it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [05:58<01:44,  0.79it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [05:58<01:44,  0.79it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [05:59<01:43,  0.79it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [05:59<01:43,  0.79it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [06:00<01:42,  0.79it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [06:00<01:42,  0.79it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [06:01<01:40,  0.79it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [06:01<01:40,  0.79it/s, v_num=b5lg, train/loss_step=0.0427, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [06:02<01:39,  0.79it/s, v_num=b5lg, train/loss_step=0.0427, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [06:02<01:39,  0.79it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [06:04<01:38,  0.79it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [06:04<01:38,  0.79it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [06:05<01:37,  0.79it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [06:05<01:37,  0.79it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [06:06<01:35,  0.79it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [06:06<01:35,  0.79it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:07<01:34,  0.79it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:07<01:34,  0.79it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:09<01:33,  0.79it/s, v_num=b5lg, train/loss_step=0.0441, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:09<01:33,  0.79it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:10<01:31,  0.79it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:10<01:31,  0.79it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:11<01:30,  0.79it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:11<01:30,  0.79it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:12<01:29,  0.79it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:12<01:29,  0.79it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:14<01:28,  0.79it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:14<01:28,  0.79it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:15<01:26,  0.79it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:15<01:26,  0.79it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:16<01:25,  0.79it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:16<01:25,  0.79it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:17<01:24,  0.79it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:17<01:24,  0.79it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:18<01:23,  0.79it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:18<01:23,  0.79it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:20<01:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:20<01:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:21<01:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:21<01:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:22<01:19,  0.79it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:22<01:19,  0.79it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:23<01:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:23<01:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:25<01:16,  0.79it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:25<01:16,  0.79it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:26<01:15,  0.79it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:26<01:15,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:27<01:14,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:27<01:14,  0.79it/s, v_num=b5lg, train/loss_step=0.0388, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:28<01:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0388, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:28<01:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:30<01:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:30<01:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:31<01:10,  0.79it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:31<01:10,  0.79it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:32<01:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:32<01:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0459, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:33<01:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0459, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:33<01:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:34<01:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:34<01:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:36<01:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:36<01:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:37<01:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:37<01:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0422, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:38<01:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0422, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:38<01:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0403, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:39<01:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0403, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:39<01:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:41<01:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:41<01:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:42<00:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:42<00:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:43<00:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:43<00:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:44<00:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:44<00:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:46<00:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:46<00:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:47<00:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:47<00:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:48<00:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:48<00:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:49<00:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:49<00:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0418, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:51<00:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0418, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:51<00:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:52<00:49,  0.80it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:52<00:49,  0.80it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [06:53<00:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [06:53<00:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [06:54<00:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [06:54<00:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [06:56<00:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0405, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [06:56<00:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [06:57<00:44,  0.80it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [06:57<00:44,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [06:58<00:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [06:58<00:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [07:00<00:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [07:00<00:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [07:01<00:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [07:01<00:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0409, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [07:02<00:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0409, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [07:02<00:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [07:03<00:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [07:03<00:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [07:05<00:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [07:05<00:36,  0.80it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250] Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [07:06<00:35,  0.80it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [07:06<00:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [07:07<00:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [07:07<00:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:08<00:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:08<00:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:09<00:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:09<00:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:11<00:30,  0.80it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:11<00:30,  0.80it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:12<00:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:12<00:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0434, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:13<00:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0434, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:13<00:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:14<00:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:14<00:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:16<00:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:16<00:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:17<00:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:17<00:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:18<00:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:18<00:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:19<00:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:19<00:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:21<00:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:21<00:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:22<00:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:22<00:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:23<00:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:23<00:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:24<00:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:24<00:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:25<00:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:26<00:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:27<00:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:27<00:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:28<00:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:28<00:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:29<00:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:29<00:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:30<00:10,  0.80it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:30<00:10,  0.80it/s, v_num=b5lg, train/loss_step=0.0414, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:32<00:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0414, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:32<00:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:33<00:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:33<00:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:34<00:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:34<00:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:35<00:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:35<00:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:36<00:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:36<00:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0392, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:38<00:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0392, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:38<00:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:39<00:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:39<00:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:40<00:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:40<00:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=0.879, val/mAP=0.0754, val/mAP_best=0.0754, train/loss_epoch=0.250]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   2%|â–         | 1/41 [00:00<00:25,  1.59it/s][A
Validation DataLoader 0:   5%|â–         | 2/41 [00:01<00:24,  1.59it/s][A
Validation DataLoader 0:   7%|â–‹         | 3/41 [00:01<00:24,  1.58it/s][A
Validation DataLoader 0:  10%|â–‰         | 4/41 [00:02<00:23,  1.58it/s][A
Validation DataLoader 0:  12%|â–ˆâ–        | 5/41 [00:03<00:22,  1.58it/s][A
Validation DataLoader 0:  15%|â–ˆâ–        | 6/41 [00:03<00:22,  1.57it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 7/41 [00:04<00:21,  1.57it/s][A
Validation DataLoader 0:  20%|â–ˆâ–‰        | 8/41 [00:05<00:20,  1.57it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 9/41 [00:05<00:20,  1.58it/s][A
Validation DataLoader 0:  24%|â–ˆâ–ˆâ–       | 10/41 [00:06<00:19,  1.58it/s][A
Validation DataLoader 0:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:06<00:19,  1.58it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:07<00:18,  1.58it/s][A
Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:08<00:17,  1.58it/s][A
Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:08<00:17,  1.58it/s][A
Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:09<00:16,  1.59it/s][A
Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:10<00:15,  1.59it/s][A
Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:10<00:15,  1.59it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:11<00:14,  1.59it/s][A
Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:11<00:13,  1.59it/s][A
Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:12<00:13,  1.59it/s][A
Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:13<00:12,  1.59it/s][A
Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [00:13<00:11,  1.59it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:14<00:11,  1.59it/s][A
Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:15<00:10,  1.59it/s][A
Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:15<00:10,  1.59it/s][A
Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/41 [00:16<00:09,  1.60it/s][A
Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [00:16<00:08,  1.60it/s][A
Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [00:17<00:08,  1.60it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [00:18<00:07,  1.60it/s][A
Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/41 [00:18<00:06,  1.60it/s][A
Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [00:19<00:06,  1.60it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [00:20<00:05,  1.60it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:20<00:05,  1.60it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/41 [00:21<00:04,  1.60it/s][A
Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [00:21<00:03,  1.60it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [00:22<00:03,  1.60it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [00:23<00:02,  1.60it/s][A
Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 38/41 [00:23<00:01,  1.60it/s][A
Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [00:24<00:01,  1.60it/s][A
Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [00:24<00:00,  1.60it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.60it/s][A
                                                                        [AEpoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:08<00:00,  0.75it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.250]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:08<00:00,  0.75it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 1:   0%|          | 0/367 [00:00<?, ?it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]          Epoch 2:   0%|          | 0/367 [00:00<?, ?it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
Epoch 2:   0%|          | 1/367 [00:03<19:50,  0.31it/s, v_num=b5lg, train/loss_step=0.0374, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   0%|          | 1/367 [00:03<19:59,  0.31it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   1%|          | 2/367 [00:04<13:47,  0.44it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   1%|          | 2/367 [00:04<13:47,  0.44it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   1%|          | 3/367 [00:05<11:41,  0.52it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   1%|          | 3/367 [00:05<11:41,  0.52it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   1%|          | 4/367 [00:07<10:37,  0.57it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   1%|          | 4/367 [00:07<10:37,  0.57it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   1%|â–         | 5/367 [00:08<10:01,  0.60it/s, v_num=b5lg, train/loss_step=0.0416, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   1%|â–         | 5/367 [00:08<10:01,  0.60it/s, v_num=b5lg, train/loss_step=0.040, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:   2%|â–         | 6/367 [00:09<09:35,  0.63it/s, v_num=b5lg, train/loss_step=0.040, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   2%|â–         | 6/367 [00:09<09:35,  0.63it/s, v_num=b5lg, train/loss_step=0.0393, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   2%|â–         | 7/367 [00:10<09:15,  0.65it/s, v_num=b5lg, train/loss_step=0.0393, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   2%|â–         | 7/367 [00:10<09:15,  0.65it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   2%|â–         | 8/367 [00:12<09:01,  0.66it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   2%|â–         | 8/367 [00:12<09:01,  0.66it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   2%|â–         | 9/367 [00:13<08:49,  0.68it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   2%|â–         | 9/367 [00:13<08:49,  0.68it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   3%|â–         | 10/367 [00:14<08:39,  0.69it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   3%|â–         | 10/367 [00:14<08:39,  0.69it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   3%|â–         | 11/367 [00:15<08:31,  0.70it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   3%|â–         | 11/367 [00:15<08:31,  0.70it/s, v_num=b5lg, train/loss_step=0.0407, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   3%|â–         | 12/367 [00:17<08:23,  0.70it/s, v_num=b5lg, train/loss_step=0.0407, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   3%|â–         | 12/367 [00:17<08:23,  0.70it/s, v_num=b5lg, train/loss_step=0.041, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:   4%|â–         | 13/367 [00:18<08:17,  0.71it/s, v_num=b5lg, train/loss_step=0.041, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   4%|â–         | 13/367 [00:18<08:17,  0.71it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   4%|â–         | 14/367 [00:19<08:11,  0.72it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   4%|â–         | 14/367 [00:19<08:11,  0.72it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   4%|â–         | 15/367 [00:20<08:06,  0.72it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   4%|â–         | 15/367 [00:20<08:06,  0.72it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   4%|â–         | 16/367 [00:21<08:01,  0.73it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   4%|â–         | 16/367 [00:21<08:01,  0.73it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   5%|â–         | 17/367 [00:23<07:56,  0.73it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   5%|â–         | 17/367 [00:23<07:56,  0.73it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   5%|â–         | 18/367 [00:24<07:52,  0.74it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   5%|â–         | 18/367 [00:24<07:52,  0.74it/s, v_num=b5lg, train/loss_step=0.040, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:   5%|â–Œ         | 19/367 [00:25<07:48,  0.74it/s, v_num=b5lg, train/loss_step=0.040, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   5%|â–Œ         | 19/367 [00:25<07:48,  0.74it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   5%|â–Œ         | 20/367 [00:26<07:45,  0.75it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   5%|â–Œ         | 20/367 [00:26<07:45,  0.75it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   6%|â–Œ         | 21/367 [00:28<07:42,  0.75it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   6%|â–Œ         | 21/367 [00:28<07:42,  0.75it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   6%|â–Œ         | 22/367 [00:29<07:38,  0.75it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   6%|â–Œ         | 22/367 [00:29<07:38,  0.75it/s, v_num=b5lg, train/loss_step=0.036, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:   6%|â–‹         | 23/367 [00:30<07:36,  0.75it/s, v_num=b5lg, train/loss_step=0.036, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   6%|â–‹         | 23/367 [00:30<07:36,  0.75it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   7%|â–‹         | 24/367 [00:31<07:33,  0.76it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   7%|â–‹         | 24/367 [00:31<07:33,  0.76it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   7%|â–‹         | 25/367 [00:32<07:30,  0.76it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   7%|â–‹         | 25/367 [00:32<07:30,  0.76it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   7%|â–‹         | 26/367 [00:34<07:28,  0.76it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   7%|â–‹         | 26/367 [00:34<07:28,  0.76it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   7%|â–‹         | 27/367 [00:35<07:25,  0.76it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   7%|â–‹         | 27/367 [00:35<07:25,  0.76it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   8%|â–Š         | 28/367 [00:36<07:23,  0.76it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   8%|â–Š         | 28/367 [00:36<07:23,  0.76it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   8%|â–Š         | 29/367 [00:38<07:24,  0.76it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   8%|â–Š         | 29/367 [00:38<07:24,  0.76it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   8%|â–Š         | 30/367 [00:39<07:21,  0.76it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   8%|â–Š         | 30/367 [00:39<07:21,  0.76it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   8%|â–Š         | 31/367 [00:40<07:19,  0.76it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   8%|â–Š         | 31/367 [00:40<07:19,  0.76it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   9%|â–Š         | 32/367 [00:41<07:17,  0.77it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   9%|â–Š         | 32/367 [00:41<07:17,  0.77it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   9%|â–‰         | 33/367 [00:42<07:15,  0.77it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   9%|â–‰         | 33/367 [00:43<07:15,  0.77it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   9%|â–‰         | 34/367 [00:44<07:13,  0.77it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:   9%|â–‰         | 34/367 [00:44<07:13,  0.77it/s, v_num=b5lg, train/loss_step=0.0412, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  10%|â–‰         | 35/367 [00:45<07:10,  0.77it/s, v_num=b5lg, train/loss_step=0.0412, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  10%|â–‰         | 35/367 [00:45<07:10,  0.77it/s, v_num=b5lg, train/loss_step=0.036, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  10%|â–‰         | 36/367 [00:46<07:08,  0.77it/s, v_num=b5lg, train/loss_step=0.036, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  10%|â–‰         | 36/367 [00:46<07:08,  0.77it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  10%|â–ˆ         | 37/367 [00:47<07:06,  0.77it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  10%|â–ˆ         | 37/367 [00:47<07:06,  0.77it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  10%|â–ˆ         | 38/367 [00:49<07:05,  0.77it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  10%|â–ˆ         | 38/367 [00:49<07:05,  0.77it/s, v_num=b5lg, train/loss_step=0.035, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  11%|â–ˆ         | 39/367 [00:50<07:03,  0.77it/s, v_num=b5lg, train/loss_step=0.035, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  11%|â–ˆ         | 39/367 [00:50<07:03,  0.77it/s, v_num=b5lg, train/loss_step=0.0426, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  11%|â–ˆ         | 40/367 [00:51<07:01,  0.78it/s, v_num=b5lg, train/loss_step=0.0426, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  11%|â–ˆ         | 40/367 [00:51<07:01,  0.78it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  11%|â–ˆ         | 41/367 [00:52<06:59,  0.78it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  11%|â–ˆ         | 41/367 [00:52<06:59,  0.78it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  11%|â–ˆâ–        | 42/367 [00:54<06:58,  0.78it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  11%|â–ˆâ–        | 42/367 [00:54<06:58,  0.78it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  12%|â–ˆâ–        | 43/367 [00:55<06:56,  0.78it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  12%|â–ˆâ–        | 43/367 [00:55<06:56,  0.78it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  12%|â–ˆâ–        | 44/367 [00:56<06:55,  0.78it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  12%|â–ˆâ–        | 44/367 [00:56<06:55,  0.78it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  12%|â–ˆâ–        | 45/367 [00:57<06:53,  0.78it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  12%|â–ˆâ–        | 45/367 [00:57<06:53,  0.78it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  13%|â–ˆâ–        | 46/367 [00:59<06:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  13%|â–ˆâ–        | 46/367 [00:59<06:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  13%|â–ˆâ–        | 47/367 [01:00<06:50,  0.78it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  13%|â–ˆâ–        | 47/367 [01:00<06:50,  0.78it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  13%|â–ˆâ–        | 48/367 [01:01<06:48,  0.78it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  13%|â–ˆâ–        | 48/367 [01:01<06:48,  0.78it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  13%|â–ˆâ–        | 49/367 [01:02<06:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  13%|â–ˆâ–        | 49/367 [01:02<06:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  14%|â–ˆâ–        | 50/367 [01:03<06:45,  0.78it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  14%|â–ˆâ–        | 50/367 [01:03<06:45,  0.78it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  14%|â–ˆâ–        | 51/367 [01:05<06:43,  0.78it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  14%|â–ˆâ–        | 51/367 [01:05<06:43,  0.78it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  14%|â–ˆâ–        | 52/367 [01:06<06:42,  0.78it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  14%|â–ˆâ–        | 52/367 [01:06<06:42,  0.78it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  14%|â–ˆâ–        | 53/367 [01:07<06:40,  0.78it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  14%|â–ˆâ–        | 53/367 [01:07<06:40,  0.78it/s, v_num=b5lg, train/loss_step=0.029, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  15%|â–ˆâ–        | 54/367 [01:08<06:38,  0.78it/s, v_num=b5lg, train/loss_step=0.029, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  15%|â–ˆâ–        | 54/367 [01:08<06:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  15%|â–ˆâ–        | 55/367 [01:10<06:37,  0.79it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  15%|â–ˆâ–        | 55/367 [01:10<06:37,  0.79it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  15%|â–ˆâ–Œ        | 56/367 [01:11<06:35,  0.79it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  15%|â–ˆâ–Œ        | 56/367 [01:11<06:35,  0.79it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  16%|â–ˆâ–Œ        | 57/367 [01:12<06:34,  0.79it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  16%|â–ˆâ–Œ        | 57/367 [01:12<06:34,  0.79it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  16%|â–ˆâ–Œ        | 58/367 [01:13<06:32,  0.79it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  16%|â–ˆâ–Œ        | 58/367 [01:13<06:32,  0.79it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  16%|â–ˆâ–Œ        | 59/367 [01:14<06:31,  0.79it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  16%|â–ˆâ–Œ        | 59/367 [01:14<06:31,  0.79it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  16%|â–ˆâ–‹        | 60/367 [01:16<06:29,  0.79it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  16%|â–ˆâ–‹        | 60/367 [01:16<06:29,  0.79it/s, v_num=b5lg, train/loss_step=0.0414, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  17%|â–ˆâ–‹        | 61/367 [01:17<06:28,  0.79it/s, v_num=b5lg, train/loss_step=0.0414, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  17%|â–ˆâ–‹        | 61/367 [01:17<06:28,  0.79it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  17%|â–ˆâ–‹        | 62/367 [01:18<06:26,  0.79it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  17%|â–ˆâ–‹        | 62/367 [01:18<06:26,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  17%|â–ˆâ–‹        | 63/367 [01:19<06:24,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  17%|â–ˆâ–‹        | 63/367 [01:19<06:24,  0.79it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  17%|â–ˆâ–‹        | 64/367 [01:20<06:23,  0.79it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  17%|â–ˆâ–‹        | 64/367 [01:20<06:23,  0.79it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  18%|â–ˆâ–Š        | 65/367 [01:22<06:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  18%|â–ˆâ–Š        | 65/367 [01:22<06:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  18%|â–ˆâ–Š        | 66/367 [01:23<06:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  18%|â–ˆâ–Š        | 66/367 [01:23<06:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0421, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  18%|â–ˆâ–Š        | 67/367 [01:24<06:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0421, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  18%|â–ˆâ–Š        | 67/367 [01:24<06:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  19%|â–ˆâ–Š        | 68/367 [01:25<06:17,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  19%|â–ˆâ–Š        | 68/367 [01:25<06:17,  0.79it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  19%|â–ˆâ–‰        | 69/367 [01:27<06:16,  0.79it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  19%|â–ˆâ–‰        | 69/367 [01:27<06:16,  0.79it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  19%|â–ˆâ–‰        | 70/367 [01:28<06:14,  0.79it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  19%|â–ˆâ–‰        | 70/367 [01:28<06:14,  0.79it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  19%|â–ˆâ–‰        | 71/367 [01:29<06:13,  0.79it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  19%|â–ˆâ–‰        | 71/367 [01:29<06:13,  0.79it/s, v_num=b5lg, train/loss_step=0.0363, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  20%|â–ˆâ–‰        | 72/367 [01:30<06:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0363, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  20%|â–ˆâ–‰        | 72/367 [01:30<06:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  20%|â–ˆâ–‰        | 73/367 [01:32<06:10,  0.79it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  20%|â–ˆâ–‰        | 73/367 [01:32<06:10,  0.79it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  20%|â–ˆâ–ˆ        | 74/367 [01:33<06:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  20%|â–ˆâ–ˆ        | 74/367 [01:33<06:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  20%|â–ˆâ–ˆ        | 75/367 [01:34<06:07,  0.79it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  20%|â–ˆâ–ˆ        | 75/367 [01:34<06:07,  0.79it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  21%|â–ˆâ–ˆ        | 76/367 [01:35<06:06,  0.79it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  21%|â–ˆâ–ˆ        | 76/367 [01:35<06:06,  0.79it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  21%|â–ˆâ–ˆ        | 77/367 [01:36<06:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0424, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  21%|â–ˆâ–ˆ        | 77/367 [01:36<06:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  21%|â–ˆâ–ˆâ–       | 78/367 [01:38<06:03,  0.79it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  21%|â–ˆâ–ˆâ–       | 78/367 [01:38<06:03,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  22%|â–ˆâ–ˆâ–       | 79/367 [01:39<06:02,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  22%|â–ˆâ–ˆâ–       | 79/367 [01:39<06:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  22%|â–ˆâ–ˆâ–       | 80/367 [01:40<06:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  22%|â–ˆâ–ˆâ–       | 80/367 [01:40<06:01,  0.79it/s, v_num=b5lg, train/loss_step=0.038, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  22%|â–ˆâ–ˆâ–       | 81/367 [01:42<06:00,  0.79it/s, v_num=b5lg, train/loss_step=0.038, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  22%|â–ˆâ–ˆâ–       | 81/367 [01:42<06:00,  0.79it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  22%|â–ˆâ–ˆâ–       | 82/367 [01:43<05:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  22%|â–ˆâ–ˆâ–       | 82/367 [01:43<05:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  23%|â–ˆâ–ˆâ–       | 83/367 [01:44<05:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  23%|â–ˆâ–ˆâ–       | 83/367 [01:44<05:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  23%|â–ˆâ–ˆâ–       | 84/367 [01:45<05:56,  0.79it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  23%|â–ˆâ–ˆâ–       | 84/367 [01:45<05:56,  0.79it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  23%|â–ˆâ–ˆâ–       | 85/367 [01:47<05:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0396, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  23%|â–ˆâ–ˆâ–       | 85/367 [01:47<05:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  23%|â–ˆâ–ˆâ–       | 86/367 [01:48<05:54,  0.79it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  23%|â–ˆâ–ˆâ–       | 86/367 [01:48<05:54,  0.79it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  24%|â–ˆâ–ˆâ–       | 87/367 [01:49<05:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  24%|â–ˆâ–ˆâ–       | 87/367 [01:49<05:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  24%|â–ˆâ–ˆâ–       | 88/367 [01:50<05:51,  0.79it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  24%|â–ˆâ–ˆâ–       | 88/367 [01:50<05:51,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  24%|â–ˆâ–ˆâ–       | 89/367 [01:52<05:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  24%|â–ˆâ–ˆâ–       | 89/367 [01:52<05:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  25%|â–ˆâ–ˆâ–       | 90/367 [01:53<05:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  25%|â–ˆâ–ˆâ–       | 90/367 [01:53<05:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  25%|â–ˆâ–ˆâ–       | 91/367 [01:54<05:48,  0.79it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  25%|â–ˆâ–ˆâ–       | 91/367 [01:54<05:48,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [01:56<05:46,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [01:56<05:46,  0.79it/s, v_num=b5lg, train/loss_step=0.033, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [01:57<05:45,  0.79it/s, v_num=b5lg, train/loss_step=0.033, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [01:57<05:45,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [01:58<05:44,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [01:58<05:44,  0.79it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [01:59<05:42,  0.79it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [01:59<05:42,  0.79it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [02:00<05:41,  0.79it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [02:01<05:41,  0.79it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:02<05:40,  0.79it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:02<05:40,  0.79it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:03<05:38,  0.79it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:03<05:38,  0.79it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:04<05:37,  0.79it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:04<05:37,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:05<05:36,  0.79it/s, v_num=b5lg, train/loss_step=0.037, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:05<05:36,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:07<05:34,  0.79it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:07<05:34,  0.79it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:08<05:33,  0.79it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:08<05:33,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:09<05:32,  0.79it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:09<05:32,  0.79it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:10<05:30,  0.79it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:10<05:30,  0.79it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:12<05:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:12<05:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0363, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:13<05:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0363, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:13<05:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:14<05:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:14<05:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:15<05:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:15<05:25,  0.80it/s, v_num=b5lg, train/loss_step=0.027, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:16<05:24,  0.80it/s, v_num=b5lg, train/loss_step=0.027, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:16<05:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:18<05:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:18<05:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:19<05:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:19<05:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:20<05:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:20<05:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:21<05:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:21<05:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:23<05:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:23<05:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:24<05:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:24<05:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0407, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:25<05:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0407, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:25<05:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:26<05:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:26<05:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:28<05:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:28<05:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:29<05:10,  0.80it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:29<05:10,  0.80it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:30<05:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:30<05:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:31<05:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:31<05:08,  0.80it/s, v_num=b5lg, train/loss_step=0.039, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:32<05:07,  0.80it/s, v_num=b5lg, train/loss_step=0.039, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:32<05:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:34<05:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:34<05:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:35<05:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0404, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:35<05:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:36<05:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:36<05:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0403, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:37<05:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0403, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:37<05:01,  0.80it/s, v_num=b5lg, train/loss_step=0.034, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:39<05:00,  0.80it/s, v_num=b5lg, train/loss_step=0.034, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:39<05:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:40<04:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:40<04:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:41<04:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:41<04:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:42<04:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:42<04:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0419, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:43<04:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0419, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:43<04:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:45<04:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:45<04:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:46<04:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:46<04:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:47<04:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:47<04:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [02:48<04:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [02:48<04:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [02:50<04:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [02:50<04:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [02:51<04:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [02:51<04:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [02:52<04:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [02:52<04:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [02:53<04:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [02:53<04:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [02:55<04:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [02:55<04:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [02:56<04:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [02:56<04:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [02:57<04:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [02:57<04:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [02:58<04:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [02:58<04:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [02:59<04:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [02:59<04:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [03:01<04:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [03:01<04:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:02<04:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:02<04:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:03<04:34,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:03<04:34,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:04<04:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:04<04:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:06<04:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:06<04:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:07<04:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:07<04:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:08<04:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:08<04:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:09<04:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:09<04:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:11<04:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:11<04:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:12<04:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:12<04:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:13<04:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:13<04:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:14<04:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:14<04:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:15<04:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:15<04:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:17<04:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0389, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:17<04:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:18<04:19,  0.80it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:18<04:19,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:19<04:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:19<04:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:20<04:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:20<04:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:22<04:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:22<04:15,  0.80it/s, v_num=b5lg, train/loss_step=0.036, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:23<04:14,  0.80it/s, v_num=b5lg, train/loss_step=0.036, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:23<04:14,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:24<04:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:24<04:13,  0.80it/s, v_num=b5lg, train/loss_step=0.031, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:25<04:11,  0.80it/s, v_num=b5lg, train/loss_step=0.031, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:25<04:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:26<04:10,  0.80it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:26<04:10,  0.80it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:28<04:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:28<04:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:29<04:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0378, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:29<04:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:30<04:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:30<04:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:31<04:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:31<04:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:33<04:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:33<04:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:34<04:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:34<04:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:35<04:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:35<04:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:36<04:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:36<04:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:37<03:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:37<03:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:39<03:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:39<03:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:40<03:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:40<03:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:41<03:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:41<03:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:42<03:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:42<03:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:44<03:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:44<03:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:45<03:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:45<03:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:46<03:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:46<03:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:47<03:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:47<03:48,  0.80it/s, v_num=b5lg, train/loss_step=0.032, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [03:48<03:47,  0.80it/s, v_num=b5lg, train/loss_step=0.032, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [03:48<03:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [03:50<03:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [03:50<03:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [03:51<03:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [03:51<03:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [03:52<03:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [03:52<03:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [03:53<03:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [03:53<03:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [03:55<03:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [03:55<03:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [03:56<03:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [03:56<03:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [03:57<03:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [03:57<03:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0272, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [03:58<03:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0272, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [03:58<03:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [03:59<03:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [03:59<03:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:01<03:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0417, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:01<03:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:02<03:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:02<03:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:03<03:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:03<03:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:04<03:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:04<03:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:06<03:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:06<03:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:07<03:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:07<03:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:08<03:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:08<03:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:09<03:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:09<03:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:10<03:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0387, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:10<03:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:12<03:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:12<03:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:13<03:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:13<03:22,  0.81it/s, v_num=b5lg, train/loss_step=0.030, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:14<03:21,  0.81it/s, v_num=b5lg, train/loss_step=0.030, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:14<03:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:15<03:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:15<03:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:16<03:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:16<03:18,  0.81it/s, v_num=b5lg, train/loss_step=0.035, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:18<03:17,  0.81it/s, v_num=b5lg, train/loss_step=0.035, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:18<03:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:19<03:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:19<03:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:20<03:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:20<03:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:21<03:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:21<03:13,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:23<03:12,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:23<03:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:24<03:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:24<03:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:25<03:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:25<03:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:26<03:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:26<03:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:27<03:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:27<03:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:29<03:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:29<03:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:30<03:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:30<03:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0402, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:31<03:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0402, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:31<03:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:32<03:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:32<03:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:33<03:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:33<03:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:35<02:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:35<02:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:36<02:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:36<02:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:37<02:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:37<02:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:38<02:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:38<02:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:40<02:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:40<02:54,  0.81it/s, v_num=b5lg, train/loss_step=0.034, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:41<02:53,  0.81it/s, v_num=b5lg, train/loss_step=0.034, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:41<02:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:42<02:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:42<02:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:43<02:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0397, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:43<02:50,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:44<02:49,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:44<02:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:46<02:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:46<02:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [04:47<02:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [04:47<02:47,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [04:48<02:45,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [04:48<02:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [04:49<02:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [04:49<02:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [04:51<02:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0398, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [04:51<02:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [04:52<02:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [04:52<02:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [04:53<02:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [04:53<02:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [04:54<02:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [04:54<02:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [04:55<02:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [04:55<02:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [04:57<02:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [04:57<02:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [04:58<02:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [04:58<02:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [04:59<02:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [04:59<02:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:00<02:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:00<02:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:02<02:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:02<02:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:03<02:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:03<02:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:04<02:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:04<02:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0401, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:05<02:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0401, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:05<02:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:06<02:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:06<02:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:08<02:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:08<02:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:09<02:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0376, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:09<02:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:10<02:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:10<02:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:11<02:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:11<02:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:13<02:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:13<02:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:14<02:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:14<02:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:15<02:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:15<02:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:16<02:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:16<02:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:17<02:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:17<02:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:19<02:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:19<02:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:20<02:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:20<02:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:21<02:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:21<02:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:22<02:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:22<02:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:23<02:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:23<02:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:25<02:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:25<02:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:26<02:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:26<02:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:27<02:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:27<02:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0497, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:28<02:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0497, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:28<02:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:30<02:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:30<02:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:31<02:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:31<02:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:32<02:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:32<02:01,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:33<01:59,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:33<01:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:34<01:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:34<01:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:36<01:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:36<01:57,  0.81it/s, v_num=b5lg, train/loss_step=0.035, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:37<01:56,  0.81it/s, v_num=b5lg, train/loss_step=0.035, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:37<01:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:38<01:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:38<01:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:39<01:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:39<01:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:40<01:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:40<01:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:42<01:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:42<01:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:43<01:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:43<01:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:44<01:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:44<01:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [05:45<01:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [05:45<01:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [05:47<01:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [05:47<01:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [05:48<01:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [05:48<01:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [05:49<01:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0391, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [05:49<01:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0395, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [05:50<01:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0395, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [05:50<01:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [05:52<01:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [05:52<01:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [05:53<01:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [05:53<01:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [05:54<01:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [05:54<01:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [05:55<01:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [05:55<01:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [05:56<01:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [05:56<01:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [05:58<01:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [05:58<01:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [05:59<01:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [05:59<01:33,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:00<01:32,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:00<01:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:01<01:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:01<01:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:03<01:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:03<01:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:04<01:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:04<01:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:05<01:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:05<01:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:06<01:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:06<01:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0287, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:07<01:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0287, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:07<01:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:09<01:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:09<01:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0256, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:10<01:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0256, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:10<01:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:11<01:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:11<01:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:12<01:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:12<01:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:13<01:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:13<01:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:15<01:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:15<01:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0421, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:16<01:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0421, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:16<01:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:17<01:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:17<01:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:18<01:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:18<01:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:20<01:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:20<01:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:21<01:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:21<01:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:22<01:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:22<01:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:23<01:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:23<01:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0392, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:25<01:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0392, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:25<01:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:26<01:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:26<01:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:27<01:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:27<01:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:28<01:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:28<01:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:29<01:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:29<01:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:31<01:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:31<01:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:32<01:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:32<01:00,  0.81it/s, v_num=b5lg, train/loss_step=0.030, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:33<00:59,  0.81it/s, v_num=b5lg, train/loss_step=0.030, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:33<00:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:34<00:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:34<00:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:36<00:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:36<00:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:37<00:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:37<00:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:38<00:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:38<00:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0393, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:39<00:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0393, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:39<00:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:40<00:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:40<00:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:42<00:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0408, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:42<00:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:43<00:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:43<00:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:44<00:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:44<00:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [06:45<00:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [06:45<00:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [06:47<00:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [06:47<00:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [06:48<00:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0364, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [06:48<00:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [06:49<00:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [06:49<00:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [06:50<00:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [06:50<00:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [06:52<00:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0381, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [06:52<00:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [06:53<00:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [06:53<00:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [06:54<00:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [06:54<00:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [06:55<00:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [06:55<00:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [06:56<00:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [06:56<00:35,  0.81it/s, v_num=b5lg, train/loss_step=0.030, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [06:58<00:34,  0.81it/s, v_num=b5lg, train/loss_step=0.030, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [06:58<00:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [06:59<00:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [06:59<00:33,  0.81it/s, v_num=b5lg, train/loss_step=0.042, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397] Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:00<00:32,  0.81it/s, v_num=b5lg, train/loss_step=0.042, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:00<00:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:01<00:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:01<00:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:02<00:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:02<00:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:04<00:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:04<00:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:05<00:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:05<00:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:06<00:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:06<00:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:07<00:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:07<00:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:09<00:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:09<00:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:10<00:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:10<00:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:11<00:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:11<00:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:12<00:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:12<00:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:13<00:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:13<00:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:15<00:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:15<00:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:16<00:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:16<00:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:17<00:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:17<00:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:18<00:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:18<00:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:19<00:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:19<00:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:21<00:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:21<00:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:22<00:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:22<00:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:23<00:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:23<00:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:24<00:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:24<00:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:25<00:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:25<00:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:27<00:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:27<00:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:28<00:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:28<00:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:29<00:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:29<00:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:30<00:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:30<00:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:32<00:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:32<00:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=1.220, val/mAP=0.0673, val/mAP_best=0.0754, train/loss_epoch=0.0397]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   2%|â–         | 1/41 [00:00<00:25,  1.58it/s][A
Validation DataLoader 0:   5%|â–         | 2/41 [00:01<00:24,  1.58it/s][A
Validation DataLoader 0:   7%|â–‹         | 3/41 [00:01<00:24,  1.58it/s][A
Validation DataLoader 0:  10%|â–‰         | 4/41 [00:02<00:23,  1.58it/s][A
Validation DataLoader 0:  12%|â–ˆâ–        | 5/41 [00:03<00:22,  1.58it/s][A
Validation DataLoader 0:  15%|â–ˆâ–        | 6/41 [00:03<00:22,  1.58it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 7/41 [00:04<00:21,  1.58it/s][A
Validation DataLoader 0:  20%|â–ˆâ–‰        | 8/41 [00:05<00:20,  1.58it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 9/41 [00:05<00:20,  1.58it/s][A
Validation DataLoader 0:  24%|â–ˆâ–ˆâ–       | 10/41 [00:06<00:19,  1.58it/s][A
Validation DataLoader 0:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:06<00:18,  1.59it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:07<00:18,  1.59it/s][A
Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:08<00:17,  1.59it/s][A
Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:08<00:16,  1.59it/s][A
Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:09<00:16,  1.59it/s][A
Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:10<00:15,  1.59it/s][A
Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:10<00:15,  1.60it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:11<00:14,  1.60it/s][A
Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:11<00:13,  1.60it/s][A
Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:12<00:13,  1.60it/s][A
Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:13<00:12,  1.60it/s][A
Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [00:13<00:11,  1.60it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:14<00:11,  1.60it/s][A
Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:15<00:10,  1.60it/s][A
Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:15<00:10,  1.60it/s][A
Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/41 [00:16<00:09,  1.60it/s][A
Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [00:16<00:08,  1.60it/s][A
Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [00:17<00:08,  1.60it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [00:18<00:07,  1.60it/s][A
Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/41 [00:18<00:06,  1.60it/s][A
Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [00:19<00:06,  1.60it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [00:20<00:05,  1.60it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:20<00:04,  1.60it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/41 [00:21<00:04,  1.60it/s][A
Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [00:21<00:03,  1.60it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [00:22<00:03,  1.60it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [00:23<00:02,  1.60it/s][A
Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 38/41 [00:23<00:01,  1.60it/s][A
Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [00:24<00:01,  1.60it/s][A
Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [00:24<00:00,  1.60it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.60it/s][A
                                                                        [AEpoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:59<00:00,  0.77it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0397]  Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:59<00:00,  0.77it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 2:   0%|          | 0/367 [00:00<?, ?it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]          Epoch 3:   0%|          | 0/367 [00:00<?, ?it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
Epoch 3:   0%|          | 1/367 [00:02<18:16,  0.33it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   0%|          | 1/367 [00:03<18:21,  0.33it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   1%|          | 2/367 [00:04<12:58,  0.47it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   1%|          | 2/367 [00:04<12:59,  0.47it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   1%|          | 3/367 [00:05<11:10,  0.54it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   1%|          | 3/367 [00:05<11:10,  0.54it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   1%|          | 4/367 [00:06<10:14,  0.59it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   1%|          | 4/367 [00:06<10:14,  0.59it/s, v_num=b5lg, train/loss_step=0.030, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:   1%|â–         | 5/367 [00:08<09:42,  0.62it/s, v_num=b5lg, train/loss_step=0.030, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   1%|â–         | 5/367 [00:08<09:42,  0.62it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   2%|â–         | 6/367 [00:09<09:22,  0.64it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   2%|â–         | 6/367 [00:09<09:22,  0.64it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   2%|â–         | 7/367 [00:10<09:07,  0.66it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   2%|â–         | 7/367 [00:10<09:07,  0.66it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   2%|â–         | 8/367 [00:11<08:55,  0.67it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   2%|â–         | 8/367 [00:11<08:55,  0.67it/s, v_num=b5lg, train/loss_step=0.0394, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   2%|â–         | 9/367 [00:13<08:45,  0.68it/s, v_num=b5lg, train/loss_step=0.0394, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   2%|â–         | 9/367 [00:13<08:45,  0.68it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   3%|â–         | 10/367 [00:14<08:36,  0.69it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   3%|â–         | 10/367 [00:14<08:37,  0.69it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   3%|â–         | 11/367 [00:15<08:29,  0.70it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   3%|â–         | 11/367 [00:15<08:29,  0.70it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   3%|â–         | 12/367 [00:17<08:23,  0.70it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   3%|â–         | 12/367 [00:17<08:23,  0.70it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   4%|â–         | 13/367 [00:18<08:18,  0.71it/s, v_num=b5lg, train/loss_step=0.0386, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   4%|â–         | 13/367 [00:18<08:18,  0.71it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   4%|â–         | 14/367 [00:19<08:13,  0.71it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   4%|â–         | 14/367 [00:19<08:13,  0.71it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   4%|â–         | 15/367 [00:20<08:09,  0.72it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   4%|â–         | 15/367 [00:20<08:09,  0.72it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   4%|â–         | 16/367 [00:22<08:05,  0.72it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   4%|â–         | 16/367 [00:22<08:05,  0.72it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   5%|â–         | 17/367 [00:23<08:01,  0.73it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   5%|â–         | 17/367 [00:23<08:01,  0.73it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   5%|â–         | 18/367 [00:24<07:58,  0.73it/s, v_num=b5lg, train/loss_step=0.0399, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   5%|â–         | 18/367 [00:24<07:58,  0.73it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   5%|â–Œ         | 19/367 [00:25<07:55,  0.73it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   5%|â–Œ         | 19/367 [00:25<07:55,  0.73it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   5%|â–Œ         | 20/367 [00:27<07:52,  0.73it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   5%|â–Œ         | 20/367 [00:27<07:52,  0.73it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   6%|â–Œ         | 21/367 [00:28<07:49,  0.74it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   6%|â–Œ         | 21/367 [00:28<07:49,  0.74it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   6%|â–Œ         | 22/367 [00:29<07:46,  0.74it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   6%|â–Œ         | 22/367 [00:29<07:46,  0.74it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   6%|â–‹         | 23/367 [00:31<07:44,  0.74it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   6%|â–‹         | 23/367 [00:31<07:44,  0.74it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   7%|â–‹         | 24/367 [00:32<07:41,  0.74it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   7%|â–‹         | 24/367 [00:32<07:41,  0.74it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   7%|â–‹         | 25/367 [00:33<07:39,  0.74it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   7%|â–‹         | 25/367 [00:33<07:39,  0.74it/s, v_num=b5lg, train/loss_step=0.039, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:   7%|â–‹         | 26/367 [00:34<07:37,  0.75it/s, v_num=b5lg, train/loss_step=0.039, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   7%|â–‹         | 26/367 [00:34<07:37,  0.75it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   7%|â–‹         | 27/367 [00:36<07:35,  0.75it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   7%|â–‹         | 27/367 [00:36<07:35,  0.75it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   8%|â–Š         | 28/367 [00:37<07:32,  0.75it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   8%|â–Š         | 28/367 [00:37<07:32,  0.75it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   8%|â–Š         | 29/367 [00:38<07:30,  0.75it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   8%|â–Š         | 29/367 [00:38<07:30,  0.75it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   8%|â–Š         | 30/367 [00:39<07:27,  0.75it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   8%|â–Š         | 30/367 [00:39<07:27,  0.75it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   8%|â–Š         | 31/367 [00:41<07:25,  0.75it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   8%|â–Š         | 31/367 [00:41<07:25,  0.75it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   9%|â–Š         | 32/367 [00:42<07:22,  0.76it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   9%|â–Š         | 32/367 [00:42<07:22,  0.76it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   9%|â–‰         | 33/367 [00:43<07:20,  0.76it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   9%|â–‰         | 33/367 [00:43<07:20,  0.76it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   9%|â–‰         | 34/367 [00:44<07:18,  0.76it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:   9%|â–‰         | 34/367 [00:44<07:18,  0.76it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  10%|â–‰         | 35/367 [00:45<07:15,  0.76it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  10%|â–‰         | 35/367 [00:45<07:15,  0.76it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  10%|â–‰         | 36/367 [00:47<07:13,  0.76it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  10%|â–‰         | 36/367 [00:47<07:13,  0.76it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  10%|â–ˆ         | 37/367 [00:48<07:11,  0.76it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  10%|â–ˆ         | 37/367 [00:48<07:11,  0.76it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  10%|â–ˆ         | 38/367 [00:49<07:09,  0.77it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  10%|â–ˆ         | 38/367 [00:49<07:09,  0.77it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  11%|â–ˆ         | 39/367 [00:50<07:07,  0.77it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  11%|â–ˆ         | 39/367 [00:50<07:07,  0.77it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  11%|â–ˆ         | 40/367 [00:52<07:05,  0.77it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  11%|â–ˆ         | 40/367 [00:52<07:05,  0.77it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  11%|â–ˆ         | 41/367 [00:53<07:03,  0.77it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  11%|â–ˆ         | 41/367 [00:53<07:03,  0.77it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  11%|â–ˆâ–        | 42/367 [00:54<07:01,  0.77it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  11%|â–ˆâ–        | 42/367 [00:54<07:01,  0.77it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  12%|â–ˆâ–        | 43/367 [00:55<07:00,  0.77it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  12%|â–ˆâ–        | 43/367 [00:55<07:00,  0.77it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  12%|â–ˆâ–        | 44/367 [00:56<06:58,  0.77it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  12%|â–ˆâ–        | 44/367 [00:56<06:58,  0.77it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  12%|â–ˆâ–        | 45/367 [00:58<06:56,  0.77it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  12%|â–ˆâ–        | 45/367 [00:58<06:56,  0.77it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  13%|â–ˆâ–        | 46/367 [00:59<06:54,  0.77it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  13%|â–ˆâ–        | 46/367 [00:59<06:54,  0.77it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  13%|â–ˆâ–        | 47/367 [01:00<06:52,  0.77it/s, v_num=b5lg, train/loss_step=0.0368, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  13%|â–ˆâ–        | 47/367 [01:00<06:52,  0.77it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  13%|â–ˆâ–        | 48/367 [01:01<06:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  13%|â–ˆâ–        | 48/367 [01:01<06:51,  0.78it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  13%|â–ˆâ–        | 49/367 [01:03<06:49,  0.78it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  13%|â–ˆâ–        | 49/367 [01:03<06:49,  0.78it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  14%|â–ˆâ–        | 50/367 [01:04<06:47,  0.78it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  14%|â–ˆâ–        | 50/367 [01:04<06:47,  0.78it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  14%|â–ˆâ–        | 51/367 [01:05<06:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  14%|â–ˆâ–        | 51/367 [01:05<06:46,  0.78it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  14%|â–ˆâ–        | 52/367 [01:06<06:44,  0.78it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  14%|â–ˆâ–        | 52/367 [01:06<06:44,  0.78it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  14%|â–ˆâ–        | 53/367 [01:07<06:42,  0.78it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  14%|â–ˆâ–        | 53/367 [01:07<06:42,  0.78it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  15%|â–ˆâ–        | 54/367 [01:09<06:41,  0.78it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  15%|â–ˆâ–        | 54/367 [01:09<06:41,  0.78it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  15%|â–ˆâ–        | 55/367 [01:10<06:39,  0.78it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  15%|â–ˆâ–        | 55/367 [01:10<06:39,  0.78it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  15%|â–ˆâ–Œ        | 56/367 [01:11<06:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  15%|â–ˆâ–Œ        | 56/367 [01:11<06:38,  0.78it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  16%|â–ˆâ–Œ        | 57/367 [01:12<06:36,  0.78it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  16%|â–ˆâ–Œ        | 57/367 [01:12<06:36,  0.78it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  16%|â–ˆâ–Œ        | 58/367 [01:14<06:35,  0.78it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  16%|â–ˆâ–Œ        | 58/367 [01:14<06:35,  0.78it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  16%|â–ˆâ–Œ        | 59/367 [01:15<06:33,  0.78it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  16%|â–ˆâ–Œ        | 59/367 [01:15<06:33,  0.78it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  16%|â–ˆâ–‹        | 60/367 [01:16<06:32,  0.78it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  16%|â–ˆâ–‹        | 60/367 [01:16<06:32,  0.78it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  17%|â–ˆâ–‹        | 61/367 [01:17<06:30,  0.78it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  17%|â–ˆâ–‹        | 61/367 [01:17<06:30,  0.78it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  17%|â–ˆâ–‹        | 62/367 [01:19<06:29,  0.78it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  17%|â–ˆâ–‹        | 62/367 [01:19<06:29,  0.78it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  17%|â–ˆâ–‹        | 63/367 [01:20<06:27,  0.78it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  17%|â–ˆâ–‹        | 63/367 [01:20<06:27,  0.78it/s, v_num=b5lg, train/loss_step=0.0276, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  17%|â–ˆâ–‹        | 64/367 [01:21<06:26,  0.78it/s, v_num=b5lg, train/loss_step=0.0276, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  17%|â–ˆâ–‹        | 64/367 [01:21<06:26,  0.78it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  18%|â–ˆâ–Š        | 65/367 [01:22<06:24,  0.79it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  18%|â–ˆâ–Š        | 65/367 [01:22<06:24,  0.79it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  18%|â–ˆâ–Š        | 66/367 [01:24<06:23,  0.79it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  18%|â–ˆâ–Š        | 66/367 [01:24<06:23,  0.79it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  18%|â–ˆâ–Š        | 67/367 [01:25<06:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  18%|â–ˆâ–Š        | 67/367 [01:25<06:21,  0.79it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  19%|â–ˆâ–Š        | 68/367 [01:26<06:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  19%|â–ˆâ–Š        | 68/367 [01:26<06:20,  0.79it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  19%|â–ˆâ–‰        | 69/367 [01:27<06:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  19%|â–ˆâ–‰        | 69/367 [01:27<06:18,  0.79it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  19%|â–ˆâ–‰        | 70/367 [01:28<06:17,  0.79it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  19%|â–ˆâ–‰        | 70/367 [01:28<06:17,  0.79it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  19%|â–ˆâ–‰        | 71/367 [01:30<06:15,  0.79it/s, v_num=b5lg, train/loss_step=0.0373, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  19%|â–ˆâ–‰        | 71/367 [01:30<06:15,  0.79it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  20%|â–ˆâ–‰        | 72/367 [01:31<06:14,  0.79it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  20%|â–ˆâ–‰        | 72/367 [01:31<06:14,  0.79it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  20%|â–ˆâ–‰        | 73/367 [01:32<06:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  20%|â–ˆâ–‰        | 73/367 [01:32<06:12,  0.79it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  20%|â–ˆâ–ˆ        | 74/367 [01:33<06:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  20%|â–ˆâ–ˆ        | 74/367 [01:33<06:11,  0.79it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  20%|â–ˆâ–ˆ        | 75/367 [01:35<06:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  20%|â–ˆâ–ˆ        | 75/367 [01:35<06:09,  0.79it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  21%|â–ˆâ–ˆ        | 76/367 [01:36<06:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  21%|â–ˆâ–ˆ        | 76/367 [01:36<06:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  21%|â–ˆâ–ˆ        | 77/367 [01:37<06:07,  0.79it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  21%|â–ˆâ–ˆ        | 77/367 [01:37<06:07,  0.79it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  21%|â–ˆâ–ˆâ–       | 78/367 [01:38<06:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0385, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  21%|â–ˆâ–ˆâ–       | 78/367 [01:38<06:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  22%|â–ˆâ–ˆâ–       | 79/367 [01:39<06:04,  0.79it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  22%|â–ˆâ–ˆâ–       | 79/367 [01:39<06:04,  0.79it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  22%|â–ˆâ–ˆâ–       | 80/367 [01:41<06:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  22%|â–ˆâ–ˆâ–       | 80/367 [01:41<06:02,  0.79it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  22%|â–ˆâ–ˆâ–       | 81/367 [01:42<06:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  22%|â–ˆâ–ˆâ–       | 81/367 [01:42<06:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  22%|â–ˆâ–ˆâ–       | 82/367 [01:43<05:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  22%|â–ˆâ–ˆâ–       | 82/367 [01:43<05:59,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  23%|â–ˆâ–ˆâ–       | 83/367 [01:44<05:58,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  23%|â–ˆâ–ˆâ–       | 83/367 [01:44<05:58,  0.79it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  23%|â–ˆâ–ˆâ–       | 84/367 [01:45<05:56,  0.79it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  23%|â–ˆâ–ˆâ–       | 84/367 [01:45<05:56,  0.79it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  23%|â–ˆâ–ˆâ–       | 85/367 [01:47<05:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  23%|â–ˆâ–ˆâ–       | 85/367 [01:47<05:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  23%|â–ˆâ–ˆâ–       | 86/367 [01:48<05:54,  0.79it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  23%|â–ˆâ–ˆâ–       | 86/367 [01:48<05:54,  0.79it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  24%|â–ˆâ–ˆâ–       | 87/367 [01:49<05:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  24%|â–ˆâ–ˆâ–       | 87/367 [01:49<05:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  24%|â–ˆâ–ˆâ–       | 88/367 [01:50<05:51,  0.79it/s, v_num=b5lg, train/loss_step=0.0359, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  24%|â–ˆâ–ˆâ–       | 88/367 [01:50<05:51,  0.79it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  24%|â–ˆâ–ˆâ–       | 89/367 [01:52<05:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  24%|â–ˆâ–ˆâ–       | 89/367 [01:52<05:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  25%|â–ˆâ–ˆâ–       | 90/367 [01:53<05:48,  0.79it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  25%|â–ˆâ–ˆâ–       | 90/367 [01:53<05:48,  0.79it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  25%|â–ˆâ–ˆâ–       | 91/367 [01:54<05:47,  0.79it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  25%|â–ˆâ–ˆâ–       | 91/367 [01:54<05:47,  0.79it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [01:55<05:46,  0.79it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [01:55<05:46,  0.79it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [01:56<05:44,  0.79it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [01:56<05:44,  0.79it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [01:58<05:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [01:58<05:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [01:59<05:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [01:59<05:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [02:00<05:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [02:00<05:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:01<05:39,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:01<05:39,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:03<05:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:03<05:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:04<05:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:04<05:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:05<05:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:05<05:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:06<05:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:06<05:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:07<05:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:07<05:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:09<05:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:09<05:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:10<05:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:10<05:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0278, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:11<05:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0278, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:11<05:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:12<05:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:12<05:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:14<05:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:14<05:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:15<05:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:15<05:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:16<05:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:16<05:23,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:17<05:21,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:17<05:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0414, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:18<05:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0414, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:18<05:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0264, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:20<05:19,  0.80it/s, v_num=b5lg, train/loss_step=0.0264, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:20<05:19,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:21<05:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:21<05:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:22<05:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:22<05:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:23<05:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:23<05:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:25<05:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:25<05:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:26<05:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:26<05:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:27<05:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:27<05:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:28<05:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:28<05:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:29<05:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0361, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:29<05:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:31<05:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:31<05:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:32<05:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:32<05:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:33<05:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:33<05:04,  0.80it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:34<05:03,  0.80it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:34<05:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:36<05:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:36<05:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:37<05:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:37<05:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:38<04:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:38<04:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:39<04:58,  0.80it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:39<04:58,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:40<04:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:40<04:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:42<04:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:42<04:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:43<04:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:43<04:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:44<04:53,  0.80it/s, v_num=b5lg, train/loss_step=0.0356, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:44<04:53,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:45<04:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:45<04:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:47<04:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:47<04:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [02:48<04:49,  0.80it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [02:48<04:49,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [02:49<04:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [02:49<04:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [02:50<04:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [02:50<04:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [02:52<04:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [02:52<04:45,  0.80it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [02:53<04:44,  0.80it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [02:53<04:44,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [02:54<04:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [02:54<04:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [02:55<04:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [02:55<04:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [02:56<04:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [02:56<04:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [02:58<04:39,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [02:58<04:39,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [02:59<04:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [02:59<04:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [03:00<04:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [03:00<04:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:01<04:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:01<04:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:03<04:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:03<04:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:04<04:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:04<04:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:05<04:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:05<04:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:06<04:30,  0.80it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:06<04:30,  0.80it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:07<04:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:07<04:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:09<04:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:09<04:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:10<04:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:10<04:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:11<04:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:11<04:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:12<04:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:12<04:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:14<04:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:14<04:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:15<04:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:15<04:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:16<04:19,  0.80it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:16<04:19,  0.80it/s, v_num=b5lg, train/loss_step=0.0256, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:17<04:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0256, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:17<04:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:18<04:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:18<04:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:20<04:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:20<04:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:21<04:14,  0.80it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:21<04:14,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:22<04:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:22<04:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:23<04:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:23<04:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:25<04:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:25<04:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:26<04:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:26<04:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:27<04:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:27<04:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:28<04:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:28<04:07,  0.80it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:29<04:06,  0.80it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:30<04:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:31<04:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:31<04:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:32<04:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:32<04:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:33<04:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:33<04:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:34<04:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:34<04:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:36<03:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:36<03:59,  0.80it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:37<03:58,  0.80it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:37<03:58,  0.80it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:38<03:57,  0.80it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:38<03:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0271, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:40<03:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0271, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:40<03:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:41<03:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:41<03:55,  0.80it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:42<03:53,  0.80it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:42<03:53,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:43<03:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:43<03:52,  0.80it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:45<03:51,  0.80it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:45<03:51,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:46<03:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:46<03:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0403, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:47<03:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0403, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:47<03:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [03:48<03:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [03:48<03:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [03:50<03:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [03:50<03:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [03:51<03:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [03:51<03:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [03:52<03:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [03:52<03:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0393, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [03:53<03:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0393, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [03:53<03:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [03:55<03:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [03:55<03:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [03:56<03:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [03:56<03:40,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [03:57<03:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [03:57<03:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [03:58<03:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [03:58<03:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [04:00<03:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [04:00<03:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:01<03:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:01<03:35,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:02<03:33,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:02<03:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:03<03:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:03<03:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:04<03:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:04<03:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:06<03:30,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:06<03:30,  0.80it/s, v_num=b5lg, train/loss_step=0.025, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:07<03:28,  0.80it/s, v_num=b5lg, train/loss_step=0.025, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:07<03:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:08<03:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:08<03:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:09<03:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:09<03:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:11<03:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:11<03:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:12<03:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0379, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:12<03:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:13<03:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:13<03:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:14<03:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:14<03:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:15<03:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:15<03:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:17<03:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:17<03:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:18<03:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:18<03:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:19<03:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:19<03:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:20<03:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:20<03:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:22<03:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:22<03:13,  0.80it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:23<03:12,  0.81it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:23<03:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:24<03:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:24<03:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:25<03:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:25<03:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:27<03:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:27<03:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:28<03:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:28<03:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:29<03:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:29<03:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:30<03:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:30<03:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0412, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:32<03:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0412, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:32<03:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0275, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:33<03:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0275, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:33<03:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:34<03:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:34<03:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0276, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:35<03:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0276, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:35<03:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:37<02:58,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:37<02:58,  0.80it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:38<02:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:38<02:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:39<02:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:39<02:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:40<02:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:40<02:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:41<02:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0322, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:41<02:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:43<02:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0377, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:43<02:52,  0.81it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:44<02:51,  0.81it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:44<02:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:45<02:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:45<02:50,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:46<02:48,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:46<02:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [04:48<02:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [04:48<02:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [04:49<02:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [04:49<02:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [04:50<02:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [04:50<02:45,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [04:51<02:43,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [04:51<02:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [04:52<02:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [04:52<02:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [04:54<02:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [04:54<02:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [04:55<02:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [04:55<02:40,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [04:56<02:38,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [04:56<02:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [04:57<02:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [04:57<02:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [04:59<02:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [04:59<02:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [05:00<02:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [05:00<02:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:01<02:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:01<02:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:02<02:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:02<02:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:04<02:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0372, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:04<02:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:05<02:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:05<02:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:06<02:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:06<02:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:07<02:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0369, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:07<02:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0252, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:08<02:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0252, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:08<02:26,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:10<02:25,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:10<02:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0263, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:11<02:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0263, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:11<02:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:12<02:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:12<02:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:13<02:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:13<02:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:15<02:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:15<02:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:16<02:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:16<02:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:17<02:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:17<02:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:18<02:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:18<02:16,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:19<02:15,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:19<02:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:21<02:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0348, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:21<02:13,  0.81it/s, v_num=b5lg, train/loss_step=0.037, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:22<02:12,  0.81it/s, v_num=b5lg, train/loss_step=0.037, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:22<02:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0278, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:23<02:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0278, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:23<02:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:24<02:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:24<02:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:26<02:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:26<02:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:27<02:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:27<02:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:28<02:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:28<02:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:29<02:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:29<02:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:31<02:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:31<02:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:32<02:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:32<02:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:33<02:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:33<02:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:34<02:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:34<02:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:36<01:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:36<01:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:37<01:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:37<01:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:38<01:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:38<01:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:40<01:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:40<01:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:41<01:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:41<01:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:42<01:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:42<01:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0251, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:43<01:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0251, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:43<01:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:45<01:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:45<01:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:46<01:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:46<01:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [05:47<01:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [05:47<01:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [05:48<01:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [05:48<01:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [05:49<01:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [05:49<01:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [05:51<01:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [05:51<01:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [05:52<01:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [05:52<01:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [05:53<01:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [05:53<01:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [05:54<01:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [05:54<01:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [05:56<01:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [05:56<01:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [05:57<01:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [05:57<01:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [05:58<01:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [05:58<01:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [05:59<01:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [05:59<01:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [06:01<01:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [06:01<01:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:02<01:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:02<01:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:03<01:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0352, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:03<01:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:04<01:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:04<01:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:05<01:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:05<01:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:07<01:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:07<01:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:08<01:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:08<01:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:09<01:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:09<01:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:10<01:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:10<01:24,  0.81it/s, v_num=b5lg, train/loss_step=0.030, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:12<01:23,  0.81it/s, v_num=b5lg, train/loss_step=0.030, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:12<01:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:13<01:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:13<01:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:14<01:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:14<01:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:15<01:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:15<01:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:17<01:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:17<01:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:18<01:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:18<01:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:19<01:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:19<01:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:20<01:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:20<01:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:21<01:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:21<01:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:23<01:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:23<01:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:24<01:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:24<01:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0392, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:25<01:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0392, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:25<01:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0276, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:26<01:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0276, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:26<01:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:28<01:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0375, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:28<01:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:29<01:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:29<01:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:30<01:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:30<01:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:31<01:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0411, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:31<01:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:33<01:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:33<01:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:34<01:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:34<01:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:35<00:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:35<00:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:36<00:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:36<00:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:37<00:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:37<00:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:39<00:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:39<00:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:40<00:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:40<00:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:41<00:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:41<00:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:42<00:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:42<00:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:44<00:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:44<00:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:45<00:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:45<00:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0402, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:46<00:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0402, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:46<00:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [06:47<00:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [06:47<00:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [06:49<00:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [06:49<00:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [06:50<00:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [06:50<00:44,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [06:51<00:43,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [06:51<00:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [06:52<00:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [06:54<00:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [06:55<00:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [06:55<00:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [06:57<00:39,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [06:57<00:39,  0.80it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [06:58<00:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [06:58<00:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [06:59<00:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [06:59<00:37,  0.80it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [07:00<00:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [07:00<00:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [07:02<00:34,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [07:02<00:34,  0.80it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [07:03<00:33,  0.80it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [07:03<00:33,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:04<00:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:04<00:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:05<00:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:05<00:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0269, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:06<00:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0269, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:06<00:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:08<00:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:08<00:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:09<00:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:09<00:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:10<00:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:10<00:26,  0.80it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:11<00:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:11<00:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:13<00:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:13<00:23,  0.80it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:14<00:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:14<00:22,  0.80it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:15<00:21,  0.80it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:15<00:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:16<00:19,  0.80it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:16<00:19,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:18<00:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0338, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:18<00:18,  0.80it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:19<00:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:19<00:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:20<00:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0329, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:20<00:16,  0.80it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:21<00:14,  0.80it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:21<00:14,  0.80it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:22<00:13,  0.80it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:22<00:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:24<00:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0357, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:24<00:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:25<00:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:25<00:11,  0.80it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:26<00:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:26<00:09,  0.80it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349] Epoch 3:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:27<00:08,  0.80it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:27<00:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:29<00:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:29<00:07,  0.80it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:30<00:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:30<00:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:31<00:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:31<00:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:32<00:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:32<00:03,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:33<00:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:33<00:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:35<00:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:35<00:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:36<00:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:36<00:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.597, val/mAP=0.193, val/mAP_best=0.193, train/loss_epoch=0.0349]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   2%|â–         | 1/41 [00:00<00:25,  1.55it/s][A
Validation DataLoader 0:   5%|â–         | 2/41 [00:01<00:25,  1.54it/s][A
Validation DataLoader 0:   7%|â–‹         | 3/41 [00:01<00:24,  1.53it/s][A
Validation DataLoader 0:  10%|â–‰         | 4/41 [00:02<00:23,  1.54it/s][A
Validation DataLoader 0:  12%|â–ˆâ–        | 5/41 [00:03<00:23,  1.54it/s][A
Validation DataLoader 0:  15%|â–ˆâ–        | 6/41 [00:03<00:22,  1.55it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 7/41 [00:04<00:21,  1.55it/s][A
Validation DataLoader 0:  20%|â–ˆâ–‰        | 8/41 [00:05<00:21,  1.55it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 9/41 [00:05<00:20,  1.55it/s][A
Validation DataLoader 0:  24%|â–ˆâ–ˆâ–       | 10/41 [00:06<00:19,  1.55it/s][A
Validation DataLoader 0:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:07<00:19,  1.55it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:07<00:18,  1.55it/s][A
Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:08<00:18,  1.55it/s][A
Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:08<00:17,  1.56it/s][A
Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:09<00:16,  1.56it/s][A
Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:10<00:16,  1.56it/s][A
Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:10<00:15,  1.56it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:11<00:14,  1.56it/s][A
Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:12<00:14,  1.56it/s][A
Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:12<00:13,  1.56it/s][A
Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:13<00:12,  1.56it/s][A
Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [00:14<00:12,  1.56it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:14<00:11,  1.56it/s][A
Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:15<00:10,  1.56it/s][A
Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:16<00:10,  1.56it/s][A
Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/41 [00:16<00:09,  1.56it/s][A
Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [00:17<00:08,  1.56it/s][A
Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [00:17<00:08,  1.56it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [00:18<00:07,  1.57it/s][A
Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/41 [00:19<00:07,  1.57it/s][A
Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [00:19<00:06,  1.57it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [00:20<00:05,  1.57it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:20<00:05,  1.57it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/41 [00:21<00:04,  1.57it/s][A
Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [00:22<00:03,  1.57it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [00:22<00:03,  1.58it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [00:23<00:02,  1.58it/s][A
Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 38/41 [00:24<00:01,  1.58it/s][A
Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [00:24<00:01,  1.58it/s][A
Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [00:25<00:00,  1.58it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.58it/s][A
                                                                        [AEpoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:03<00:00,  0.76it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0349]Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:03<00:00,  0.76it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 3:   0%|          | 0/367 [00:00<?, ?it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]          Epoch 4:   0%|          | 0/367 [00:00<?, ?it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
Epoch 4:   0%|          | 1/367 [00:02<14:54,  0.41it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   0%|          | 1/367 [00:02<15:05,  0.40it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   1%|          | 2/367 [00:03<11:18,  0.54it/s, v_num=b5lg, train/loss_step=0.0307, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   1%|          | 2/367 [00:03<11:18,  0.54it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:   1%|          | 3/367 [00:04<10:01,  0.61it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   1%|          | 3/367 [00:04<10:01,  0.61it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   1%|          | 4/367 [00:06<09:22,  0.65it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   1%|          | 4/367 [00:06<09:22,  0.65it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   1%|â–         | 5/367 [00:07<08:58,  0.67it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   1%|â–         | 5/367 [00:07<08:58,  0.67it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   2%|â–         | 6/367 [00:08<08:41,  0.69it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   2%|â–         | 6/367 [00:08<08:41,  0.69it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   2%|â–         | 7/367 [00:09<08:29,  0.71it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   2%|â–         | 7/367 [00:09<08:29,  0.71it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   2%|â–         | 8/367 [00:11<08:19,  0.72it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   2%|â–         | 8/367 [00:11<08:19,  0.72it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   2%|â–         | 9/367 [00:12<08:11,  0.73it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   2%|â–         | 9/367 [00:12<08:11,  0.73it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   3%|â–         | 10/367 [00:13<08:05,  0.74it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   3%|â–         | 10/367 [00:13<08:05,  0.74it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   3%|â–         | 11/367 [00:14<07:59,  0.74it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   3%|â–         | 11/367 [00:14<07:59,  0.74it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   3%|â–         | 12/367 [00:16<07:54,  0.75it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   3%|â–         | 12/367 [00:16<07:54,  0.75it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   4%|â–         | 13/367 [00:17<07:50,  0.75it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   4%|â–         | 13/367 [00:17<07:50,  0.75it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   4%|â–         | 14/367 [00:18<07:46,  0.76it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   4%|â–         | 14/367 [00:18<07:46,  0.76it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   4%|â–         | 15/367 [00:19<07:43,  0.76it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   4%|â–         | 15/367 [00:19<07:43,  0.76it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   4%|â–         | 16/367 [00:20<07:39,  0.76it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   4%|â–         | 16/367 [00:20<07:39,  0.76it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   5%|â–         | 17/367 [00:22<07:36,  0.77it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   5%|â–         | 17/367 [00:22<07:36,  0.77it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   5%|â–         | 18/367 [00:23<07:34,  0.77it/s, v_num=b5lg, train/loss_step=0.0344, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   5%|â–         | 18/367 [00:23<07:34,  0.77it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   5%|â–Œ         | 19/367 [00:24<07:31,  0.77it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   5%|â–Œ         | 19/367 [00:24<07:31,  0.77it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   5%|â–Œ         | 20/367 [00:25<07:28,  0.77it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   5%|â–Œ         | 20/367 [00:25<07:28,  0.77it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   6%|â–Œ         | 21/367 [00:27<07:26,  0.77it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   6%|â–Œ         | 21/367 [00:27<07:26,  0.77it/s, v_num=b5lg, train/loss_step=0.0287, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   6%|â–Œ         | 22/367 [00:28<07:24,  0.78it/s, v_num=b5lg, train/loss_step=0.0287, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   6%|â–Œ         | 22/367 [00:28<07:24,  0.78it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   6%|â–‹         | 23/367 [00:29<07:22,  0.78it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   6%|â–‹         | 23/367 [00:29<07:22,  0.78it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   7%|â–‹         | 24/367 [00:30<07:19,  0.78it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   7%|â–‹         | 24/367 [00:30<07:19,  0.78it/s, v_num=b5lg, train/loss_step=0.030, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:   7%|â–‹         | 25/367 [00:32<07:17,  0.78it/s, v_num=b5lg, train/loss_step=0.030, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   7%|â–‹         | 25/367 [00:32<07:17,  0.78it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   7%|â–‹         | 26/367 [00:33<07:16,  0.78it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   7%|â–‹         | 26/367 [00:33<07:16,  0.78it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   7%|â–‹         | 27/367 [00:34<07:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   7%|â–‹         | 27/367 [00:34<07:14,  0.78it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   8%|â–Š         | 28/367 [00:35<07:12,  0.78it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   8%|â–Š         | 28/367 [00:35<07:12,  0.78it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   8%|â–Š         | 29/367 [00:36<07:10,  0.79it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   8%|â–Š         | 29/367 [00:36<07:10,  0.79it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:   8%|â–Š         | 30/367 [00:38<07:08,  0.79it/s, v_num=b5lg, train/loss_step=0.035, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   8%|â–Š         | 30/367 [00:38<07:08,  0.79it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   8%|â–Š         | 31/367 [00:39<07:06,  0.79it/s, v_num=b5lg, train/loss_step=0.0335, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   8%|â–Š         | 31/367 [00:39<07:06,  0.79it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   9%|â–Š         | 32/367 [00:40<07:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   9%|â–Š         | 32/367 [00:40<07:05,  0.79it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   9%|â–‰         | 33/367 [00:41<07:03,  0.79it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   9%|â–‰         | 33/367 [00:41<07:03,  0.79it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   9%|â–‰         | 34/367 [00:43<07:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:   9%|â–‰         | 34/367 [00:43<07:01,  0.79it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  10%|â–‰         | 35/367 [00:44<07:00,  0.79it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  10%|â–‰         | 35/367 [00:44<07:00,  0.79it/s, v_num=b5lg, train/loss_step=0.0246, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  10%|â–‰         | 36/367 [00:45<06:58,  0.79it/s, v_num=b5lg, train/loss_step=0.0246, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  10%|â–‰         | 36/367 [00:45<06:58,  0.79it/s, v_num=b5lg, train/loss_step=0.0231, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  10%|â–ˆ         | 37/367 [00:46<06:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0231, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  10%|â–ˆ         | 37/367 [00:46<06:57,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  10%|â–ˆ         | 38/367 [00:47<06:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  10%|â–ˆ         | 38/367 [00:47<06:55,  0.79it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  11%|â–ˆ         | 39/367 [00:49<06:53,  0.79it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  11%|â–ˆ         | 39/367 [00:49<06:53,  0.79it/s, v_num=b5lg, train/loss_step=0.0253, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  11%|â–ˆ         | 40/367 [00:50<06:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0253, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  11%|â–ˆ         | 40/367 [00:50<06:52,  0.79it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  11%|â–ˆ         | 41/367 [00:51<06:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  11%|â–ˆ         | 41/367 [00:51<06:50,  0.79it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  11%|â–ˆâ–        | 42/367 [00:52<06:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  11%|â–ˆâ–        | 42/367 [00:52<06:49,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  12%|â–ˆâ–        | 43/367 [00:54<06:47,  0.79it/s, v_num=b5lg, train/loss_step=0.0367, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  12%|â–ˆâ–        | 43/367 [00:54<06:47,  0.79it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  12%|â–ˆâ–        | 44/367 [00:55<06:46,  0.79it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  12%|â–ˆâ–        | 44/367 [00:55<06:46,  0.79it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  12%|â–ˆâ–        | 45/367 [00:56<06:45,  0.79it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  12%|â–ˆâ–        | 45/367 [00:56<06:45,  0.79it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  13%|â–ˆâ–        | 46/367 [00:57<06:43,  0.79it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  13%|â–ˆâ–        | 46/367 [00:57<06:43,  0.79it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  13%|â–ˆâ–        | 47/367 [00:59<06:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  13%|â–ˆâ–        | 47/367 [00:59<06:42,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  13%|â–ˆâ–        | 48/367 [01:00<06:41,  0.80it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  13%|â–ˆâ–        | 48/367 [01:00<06:41,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  13%|â–ˆâ–        | 49/367 [01:01<06:39,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  13%|â–ˆâ–        | 49/367 [01:01<06:39,  0.80it/s, v_num=b5lg, train/loss_step=0.0248, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  14%|â–ˆâ–        | 50/367 [01:02<06:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0248, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  14%|â–ˆâ–        | 50/367 [01:02<06:38,  0.80it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  14%|â–ˆâ–        | 51/367 [01:04<06:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  14%|â–ˆâ–        | 51/367 [01:04<06:36,  0.80it/s, v_num=b5lg, train/loss_step=0.0287, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  14%|â–ˆâ–        | 52/367 [01:05<06:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0287, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  14%|â–ˆâ–        | 52/367 [01:05<06:35,  0.80it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  14%|â–ˆâ–        | 53/367 [01:06<06:34,  0.80it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  14%|â–ˆâ–        | 53/367 [01:06<06:34,  0.80it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  15%|â–ˆâ–        | 54/367 [01:07<06:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  15%|â–ˆâ–        | 54/367 [01:07<06:32,  0.80it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  15%|â–ˆâ–        | 55/367 [01:08<06:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  15%|â–ˆâ–        | 55/367 [01:08<06:31,  0.80it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  15%|â–ˆâ–Œ        | 56/367 [01:10<06:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  15%|â–ˆâ–Œ        | 56/367 [01:10<06:29,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  16%|â–ˆâ–Œ        | 57/367 [01:11<06:28,  0.80it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  16%|â–ˆâ–Œ        | 57/367 [01:11<06:28,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  16%|â–ˆâ–Œ        | 58/367 [01:12<06:27,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  16%|â–ˆâ–Œ        | 58/367 [01:12<06:27,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  16%|â–ˆâ–Œ        | 59/367 [01:13<06:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  16%|â–ˆâ–Œ        | 59/367 [01:13<06:25,  0.80it/s, v_num=b5lg, train/loss_step=0.0278, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  16%|â–ˆâ–‹        | 60/367 [01:15<06:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0278, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  16%|â–ˆâ–‹        | 60/367 [01:15<06:24,  0.80it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  17%|â–ˆâ–‹        | 61/367 [01:16<06:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  17%|â–ˆâ–‹        | 61/367 [01:16<06:22,  0.80it/s, v_num=b5lg, train/loss_step=0.0247, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  17%|â–ˆâ–‹        | 62/367 [01:17<06:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0247, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  17%|â–ˆâ–‹        | 62/367 [01:17<06:21,  0.80it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  17%|â–ˆâ–‹        | 63/367 [01:18<06:20,  0.80it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  17%|â–ˆâ–‹        | 63/367 [01:18<06:20,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  17%|â–ˆâ–‹        | 64/367 [01:20<06:18,  0.80it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  17%|â–ˆâ–‹        | 64/367 [01:20<06:18,  0.80it/s, v_num=b5lg, train/loss_step=0.030, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  18%|â–ˆâ–Š        | 65/367 [01:21<06:17,  0.80it/s, v_num=b5lg, train/loss_step=0.030, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  18%|â–ˆâ–Š        | 65/367 [01:21<06:17,  0.80it/s, v_num=b5lg, train/loss_step=0.0239, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  18%|â–ˆâ–Š        | 66/367 [01:22<06:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0239, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  18%|â–ˆâ–Š        | 66/367 [01:22<06:15,  0.80it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  18%|â–ˆâ–Š        | 67/367 [01:23<06:14,  0.80it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  18%|â–ˆâ–Š        | 67/367 [01:23<06:14,  0.80it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  19%|â–ˆâ–Š        | 68/367 [01:24<06:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  19%|â–ˆâ–Š        | 68/367 [01:24<06:13,  0.80it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  19%|â–ˆâ–‰        | 69/367 [01:26<06:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  19%|â–ˆâ–‰        | 69/367 [01:26<06:12,  0.80it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  19%|â–ˆâ–‰        | 70/367 [01:27<06:10,  0.80it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  19%|â–ˆâ–‰        | 70/367 [01:27<06:10,  0.80it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  19%|â–ˆâ–‰        | 71/367 [01:28<06:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  19%|â–ˆâ–‰        | 71/367 [01:28<06:09,  0.80it/s, v_num=b5lg, train/loss_step=0.0206, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  20%|â–ˆâ–‰        | 72/367 [01:29<06:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0206, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  20%|â–ˆâ–‰        | 72/367 [01:29<06:08,  0.80it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  20%|â–ˆâ–‰        | 73/367 [01:31<06:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  20%|â–ˆâ–‰        | 73/367 [01:31<06:06,  0.80it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  20%|â–ˆâ–ˆ        | 74/367 [01:32<06:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  20%|â–ˆâ–ˆ        | 74/367 [01:32<06:05,  0.80it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  20%|â–ˆâ–ˆ        | 75/367 [01:33<06:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  20%|â–ˆâ–ˆ        | 75/367 [01:33<06:04,  0.80it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  21%|â–ˆâ–ˆ        | 76/367 [01:34<06:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  21%|â–ˆâ–ˆ        | 76/367 [01:34<06:02,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  21%|â–ˆâ–ˆ        | 77/367 [01:35<06:01,  0.80it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  21%|â–ˆâ–ˆ        | 77/367 [01:35<06:01,  0.80it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  21%|â–ˆâ–ˆâ–       | 78/367 [01:37<06:00,  0.80it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  21%|â–ˆâ–ˆâ–       | 78/367 [01:37<06:00,  0.80it/s, v_num=b5lg, train/loss_step=0.0254, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  22%|â–ˆâ–ˆâ–       | 79/367 [01:38<05:58,  0.80it/s, v_num=b5lg, train/loss_step=0.0254, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  22%|â–ˆâ–ˆâ–       | 79/367 [01:38<05:58,  0.80it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  22%|â–ˆâ–ˆâ–       | 80/367 [01:39<05:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  22%|â–ˆâ–ˆâ–       | 80/367 [01:39<05:57,  0.80it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  22%|â–ˆâ–ˆâ–       | 81/367 [01:40<05:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  22%|â–ˆâ–ˆâ–       | 81/367 [01:40<05:56,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  22%|â–ˆâ–ˆâ–       | 82/367 [01:42<05:54,  0.80it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  22%|â–ˆâ–ˆâ–       | 82/367 [01:42<05:54,  0.80it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  23%|â–ˆâ–ˆâ–       | 83/367 [01:43<05:53,  0.80it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  23%|â–ˆâ–ˆâ–       | 83/367 [01:43<05:53,  0.80it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  23%|â–ˆâ–ˆâ–       | 84/367 [01:44<05:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  23%|â–ˆâ–ˆâ–       | 84/367 [01:44<05:52,  0.80it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  23%|â–ˆâ–ˆâ–       | 85/367 [01:45<05:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  23%|â–ˆâ–ˆâ–       | 85/367 [01:45<05:50,  0.80it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  23%|â–ˆâ–ˆâ–       | 86/367 [01:46<05:49,  0.80it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  23%|â–ˆâ–ˆâ–       | 86/367 [01:46<05:49,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  24%|â–ˆâ–ˆâ–       | 87/367 [01:48<05:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  24%|â–ˆâ–ˆâ–       | 87/367 [01:48<05:48,  0.80it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  24%|â–ˆâ–ˆâ–       | 88/367 [01:49<05:46,  0.80it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  24%|â–ˆâ–ˆâ–       | 88/367 [01:49<05:46,  0.80it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  24%|â–ˆâ–ˆâ–       | 89/367 [01:50<05:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  24%|â–ˆâ–ˆâ–       | 89/367 [01:50<05:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  25%|â–ˆâ–ˆâ–       | 90/367 [01:51<05:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  25%|â–ˆâ–ˆâ–       | 90/367 [01:51<05:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0254, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  25%|â–ˆâ–ˆâ–       | 91/367 [01:53<05:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0254, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  25%|â–ˆâ–ˆâ–       | 91/367 [01:53<05:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [01:54<05:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  25%|â–ˆâ–ˆâ–Œ       | 92/367 [01:54<05:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [01:55<05:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  25%|â–ˆâ–ˆâ–Œ       | 93/367 [01:55<05:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0252, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [01:56<05:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0252, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  26%|â–ˆâ–ˆâ–Œ       | 94/367 [01:56<05:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [01:57<05:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  26%|â–ˆâ–ˆâ–Œ       | 95/367 [01:57<05:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [01:59<05:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  26%|â–ˆâ–ˆâ–Œ       | 96/367 [01:59<05:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:00<05:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  26%|â–ˆâ–ˆâ–‹       | 97/367 [02:00<05:34,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:01<05:33,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  27%|â–ˆâ–ˆâ–‹       | 98/367 [02:01<05:33,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:02<05:32,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  27%|â–ˆâ–ˆâ–‹       | 99/367 [02:02<05:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:03<05:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0288, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  27%|â–ˆâ–ˆâ–‹       | 100/367 [02:03<05:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:05<05:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  28%|â–ˆâ–ˆâ–Š       | 101/367 [02:05<05:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:06<05:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  28%|â–ˆâ–ˆâ–Š       | 102/367 [02:06<05:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0259, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:07<05:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0259, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  28%|â–ˆâ–ˆâ–Š       | 103/367 [02:07<05:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:08<05:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0341, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  28%|â–ˆâ–ˆâ–Š       | 104/367 [02:08<05:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:10<05:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  29%|â–ˆâ–ˆâ–Š       | 105/367 [02:10<05:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:11<05:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  29%|â–ˆâ–ˆâ–‰       | 106/367 [02:11<05:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:12<05:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  29%|â–ˆâ–ˆâ–‰       | 107/367 [02:12<05:22,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:13<05:20,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  29%|â–ˆâ–ˆâ–‰       | 108/367 [02:13<05:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:14<05:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  30%|â–ˆâ–ˆâ–‰       | 109/367 [02:14<05:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:16<05:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  30%|â–ˆâ–ˆâ–‰       | 110/367 [02:16<05:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:17<05:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  30%|â–ˆâ–ˆâ–ˆ       | 111/367 [02:17<05:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:18<05:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0265, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  31%|â–ˆâ–ˆâ–ˆ       | 112/367 [02:18<05:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:19<05:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  31%|â–ˆâ–ˆâ–ˆ       | 113/367 [02:19<05:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:21<05:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  31%|â–ˆâ–ˆâ–ˆ       | 114/367 [02:21<05:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:22<05:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  31%|â–ˆâ–ˆâ–ˆâ–      | 115/367 [02:22<05:11,  0.81it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:23<05:10,  0.81it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/367 [02:23<05:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:24<05:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  32%|â–ˆâ–ˆâ–ˆâ–      | 117/367 [02:24<05:09,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:25<05:08,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  32%|â–ˆâ–ˆâ–ˆâ–      | 118/367 [02:25<05:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:27<05:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  32%|â–ˆâ–ˆâ–ˆâ–      | 119/367 [02:27<05:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:28<05:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  33%|â–ˆâ–ˆâ–ˆâ–      | 120/367 [02:28<05:05,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:29<05:04,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  33%|â–ˆâ–ˆâ–ˆâ–      | 121/367 [02:29<05:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:30<05:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  33%|â–ˆâ–ˆâ–ˆâ–      | 122/367 [02:30<05:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0237, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:32<05:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0237, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/367 [02:32<05:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:33<05:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/367 [02:33<05:00,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:34<04:59,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  34%|â–ˆâ–ˆâ–ˆâ–      | 125/367 [02:34<04:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:35<04:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  34%|â–ˆâ–ˆâ–ˆâ–      | 126/367 [02:35<04:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:37<04:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  35%|â–ˆâ–ˆâ–ˆâ–      | 127/367 [02:37<04:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:38<04:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  35%|â–ˆâ–ˆâ–ˆâ–      | 128/367 [02:38<04:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:39<04:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/367 [02:39<04:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:41<04:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/367 [02:41<04:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:42<04:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 131/367 [02:42<04:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:43<04:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 132/367 [02:43<04:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:44<04:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 133/367 [02:44<04:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0256, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:46<04:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0256, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/367 [02:46<04:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [02:47<04:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 135/367 [02:47<04:48,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [02:48<04:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 136/367 [02:48<04:47,  0.80it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [02:50<04:45,  0.80it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 137/367 [02:50<04:45,  0.80it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [02:51<04:44,  0.80it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/367 [02:51<04:44,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [02:52<04:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0299, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 139/367 [02:52<04:43,  0.80it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [02:53<04:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 140/367 [02:53<04:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [02:55<04:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 141/367 [02:55<04:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [02:56<04:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0342, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 142/367 [02:56<04:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [02:57<04:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 143/367 [02:57<04:38,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [02:58<04:36,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 144/367 [02:58<04:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [02:59<04:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 145/367 [02:59<04:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:01<04:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 146/367 [03:01<04:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:02<04:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/367 [03:02<04:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:03<04:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/367 [03:03<04:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:04<04:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0345, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 149/367 [03:04<04:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:06<04:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0333, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 150/367 [03:06<04:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:07<04:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0366, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 151/367 [03:07<04:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:08<04:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/367 [03:08<04:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:09<04:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/367 [03:09<04:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:11<04:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/367 [03:11<04:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0264, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:12<04:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0264, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/367 [03:12<04:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:13<04:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/367 [03:13<04:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:14<04:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/367 [03:14<04:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:15<04:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/367 [03:15<04:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:17<04:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/367 [03:17<04:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0253, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:18<04:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0253, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/367 [03:18<04:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0269, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:19<04:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0269, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/367 [03:19<04:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:20<04:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 162/367 [03:20<04:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:22<04:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 163/367 [03:22<04:12,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:23<04:11,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 164/367 [03:23<04:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:24<04:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0349, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 165/367 [03:24<04:10,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:25<04:09,  0.81it/s, v_num=b5lg, train/loss_step=0.027, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/367 [03:25<04:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:26<04:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 167/367 [03:26<04:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:28<04:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 168/367 [03:28<04:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:29<04:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 169/367 [03:29<04:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:30<04:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/367 [03:30<04:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0251, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:31<04:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0251, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 171/367 [03:31<04:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:33<04:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 172/367 [03:33<04:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:34<04:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 173/367 [03:34<04:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:35<03:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 174/367 [03:35<03:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:36<03:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/367 [03:36<03:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0234, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:38<03:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0234, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 176/367 [03:38<03:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:39<03:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 177/367 [03:39<03:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:40<03:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 178/367 [03:40<03:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:41<03:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0383, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/367 [03:41<03:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:42<03:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 180/367 [03:42<03:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0271, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:44<03:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0271, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/367 [03:44<03:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:45<03:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 182/367 [03:45<03:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:46<03:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 183/367 [03:46<03:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [03:47<03:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/367 [03:47<03:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [03:49<03:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 185/367 [03:49<03:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [03:50<03:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0285, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 186/367 [03:50<03:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [03:51<03:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 187/367 [03:51<03:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [03:52<03:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 188/367 [03:52<03:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [03:53<03:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0296, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/367 [03:53<03:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [03:55<03:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/367 [03:55<03:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [03:56<03:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/367 [03:56<03:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [03:57<03:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/367 [03:57<03:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [03:58<03:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/367 [03:58<03:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:00<03:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/367 [04:00<03:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:01<03:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/367 [04:01<03:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0255, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:02<03:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0255, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/367 [04:02<03:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:03<03:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0325, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/367 [04:03<03:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:05<03:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/367 [04:05<03:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:06<03:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0365, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/367 [04:06<03:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:07<03:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/367 [04:07<03:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:08<03:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/367 [04:08<03:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:10<03:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/367 [04:10<03:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:11<03:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 203/367 [04:11<03:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:12<03:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0326, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 204/367 [04:12<03:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:13<03:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 205/367 [04:13<03:20,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:15<03:19,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 206/367 [04:15<03:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:16<03:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0312, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207/367 [04:16<03:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:17<03:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0337, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208/367 [04:17<03:16,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:18<03:15,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 209/367 [04:18<03:15,  0.81it/s, v_num=b5lg, train/loss_step=0.025, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:19<03:14,  0.81it/s, v_num=b5lg, train/loss_step=0.025, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210/367 [04:19<03:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:21<03:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 211/367 [04:21<03:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:22<03:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 212/367 [04:22<03:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:23<03:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0351, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 213/367 [04:23<03:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:24<03:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 214/367 [04:24<03:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:26<03:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 215/367 [04:26<03:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:27<03:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 216/367 [04:27<03:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0228, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:28<03:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0228, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 217/367 [04:28<03:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:29<03:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0298, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 218/367 [04:29<03:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:31<03:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 219/367 [04:31<03:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0287, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:32<03:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0287, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 220/367 [04:32<03:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:33<03:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 221/367 [04:33<03:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:34<02:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 222/367 [04:34<02:59,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:36<02:58,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 223/367 [04:36<02:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:37<02:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 224/367 [04:37<02:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:38<02:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/367 [04:38<02:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:39<02:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/367 [04:39<02:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:40<02:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/367 [04:40<02:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:42<02:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/367 [04:42<02:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:43<02:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0297, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/367 [04:43<02:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:44<02:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/367 [04:44<02:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0245, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:45<02:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0245, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/367 [04:45<02:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [04:47<02:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/367 [04:47<02:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [04:48<02:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/367 [04:48<02:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [04:49<02:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/367 [04:49<02:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [04:50<02:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/367 [04:50<02:43,  0.81it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [04:51<02:42,  0.81it/s, v_num=b5lg, train/loss_step=0.036, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/367 [04:51<02:42,  0.81it/s, v_num=b5lg, train/loss_step=0.041, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [04:53<02:40,  0.81it/s, v_num=b5lg, train/loss_step=0.041, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/367 [04:53<02:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0251, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [04:54<02:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0251, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/367 [04:54<02:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0271, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [04:55<02:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0271, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 239/367 [04:55<02:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [04:56<02:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0303, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 240/367 [04:56<02:37,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [04:58<02:35,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 241/367 [04:58<02:35,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [04:59<02:34,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 242/367 [04:59<02:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:00<02:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0315, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 243/367 [05:00<02:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:01<02:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0268, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 244/367 [05:01<02:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:03<02:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0262, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 245/367 [05:03<02:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:04<02:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 246/367 [05:04<02:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:05<02:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0347, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 247/367 [05:05<02:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:06<02:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0309, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 248/367 [05:06<02:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0363, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:08<02:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0363, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 249/367 [05:08<02:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:09<02:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 250/367 [05:09<02:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:10<02:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 251/367 [05:10<02:23,  0.81it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:11<02:22,  0.81it/s, v_num=b5lg, train/loss_step=0.034, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 252/367 [05:11<02:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:12<02:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0324, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 253/367 [05:12<02:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0275, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:14<02:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0275, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 254/367 [05:14<02:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:15<02:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 255/367 [05:15<02:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:16<02:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 256/367 [05:16<02:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:17<02:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 257/367 [05:17<02:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:19<02:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 258/367 [05:19<02:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:20<02:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/367 [05:20<02:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:21<02:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 260/367 [05:21<02:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:22<02:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0318, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 261/367 [05:22<02:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:24<02:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/367 [05:24<02:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:25<02:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0317, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/367 [05:25<02:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:26<02:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/367 [05:26<02:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:27<02:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0362, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/367 [05:27<02:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:29<02:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/367 [05:29<02:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:30<02:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/367 [05:30<02:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0258, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:31<02:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0258, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/367 [05:31<02:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0259, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:32<02:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0259, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/367 [05:32<02:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:34<02:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 270/367 [05:34<02:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:35<01:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 271/367 [05:35<01:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:36<01:57,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/367 [05:36<01:57,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:38<01:56,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/367 [05:38<01:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:39<01:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0319, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/367 [05:39<01:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:40<01:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/367 [05:40<01:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0254, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:41<01:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0254, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 276/367 [05:41<01:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:43<01:51,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 277/367 [05:43<01:51,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:44<01:50,  0.81it/s, v_num=b5lg, train/loss_step=0.031, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 278/367 [05:44<01:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:45<01:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 279/367 [05:45<01:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [05:47<01:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0336, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 280/367 [05:47<01:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [05:48<01:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0302, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 281/367 [05:48<01:46,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [05:49<01:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0354, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 282/367 [05:49<01:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [05:50<01:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 283/367 [05:50<01:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [05:52<01:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 284/367 [05:52<01:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0272, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [05:53<01:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0272, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/367 [05:53<01:41,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [05:54<01:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 286/367 [05:54<01:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [05:55<01:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 287/367 [05:55<01:39,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [05:57<01:37,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 288/367 [05:57<01:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [05:58<01:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0286, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 289/367 [05:58<01:36,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [05:59<01:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 290/367 [05:59<01:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [06:00<01:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 291/367 [06:00<01:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:02<01:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 292/367 [06:02<01:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:03<01:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 293/367 [06:03<01:31,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:04<01:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 294/367 [06:04<01:30,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:05<01:29,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/367 [06:05<01:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:06<01:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0283, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296/367 [06:06<01:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:08<01:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 297/367 [06:08<01:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:09<01:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0328, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 298/367 [06:09<01:25,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:10<01:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0327, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/367 [06:10<01:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:11<01:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/367 [06:11<01:23,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:13<01:21,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/367 [06:13<01:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:14<01:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0313, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/367 [06:14<01:20,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:15<01:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/367 [06:15<01:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:16<01:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/367 [06:16<01:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:17<01:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0355, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/367 [06:17<01:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:19<01:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0273, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 306/367 [06:19<01:15,  0.81it/s, v_num=b5lg, train/loss_step=0.0264, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:20<01:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0264, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 307/367 [06:20<01:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0272, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:21<01:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0272, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 308/367 [06:21<01:13,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:22<01:11,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 309/367 [06:22<01:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:24<01:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0311, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/367 [06:24<01:10,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:25<01:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/367 [06:25<01:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:26<01:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 312/367 [06:26<01:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:27<01:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0304, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 313/367 [06:27<01:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:29<01:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0371, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 314/367 [06:29<01:05,  0.81it/s, v_num=b5lg, train/loss_step=0.0253, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:30<01:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0253, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 315/367 [06:30<01:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:31<01:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 316/367 [06:31<01:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0275, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:32<01:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0275, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 317/367 [06:32<01:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:33<01:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0353, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 318/367 [06:33<01:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0255, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:35<00:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0255, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 319/367 [06:35<00:59,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:36<00:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0316, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 320/367 [06:36<00:58,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:37<00:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0294, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 321/367 [06:37<00:56,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:38<00:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 322/367 [06:38<00:55,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:40<00:54,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 323/367 [06:40<00:54,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:41<00:53,  0.81it/s, v_num=b5lg, train/loss_step=0.029, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 324/367 [06:41<00:53,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:42<00:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 325/367 [06:42<00:52,  0.81it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:43<00:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0384, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 326/367 [06:43<00:50,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:44<00:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 327/367 [06:44<00:49,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:46<00:48,  0.81it/s, v_num=b5lg, train/loss_step=0.0343, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 328/367 [06:46<00:48,  0.81it/s, v_num=b5lg, train/loss_step=0.026, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [06:47<00:47,  0.81it/s, v_num=b5lg, train/loss_step=0.026, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 329/367 [06:47<00:47,  0.81it/s, v_num=b5lg, train/loss_step=0.0263, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [06:48<00:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0263, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 330/367 [06:48<00:45,  0.81it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [06:49<00:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0306, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 331/367 [06:49<00:44,  0.81it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [06:51<00:43,  0.81it/s, v_num=b5lg, train/loss_step=0.0293, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/367 [06:51<00:43,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [06:52<00:42,  0.81it/s, v_num=b5lg, train/loss_step=0.028, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 333/367 [06:52<00:42,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [06:53<00:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0346, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 334/367 [06:53<00:40,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [06:54<00:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0266, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/367 [06:54<00:39,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [06:56<00:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0323, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/367 [06:56<00:38,  0.81it/s, v_num=b5lg, train/loss_step=0.0251, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [06:57<00:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0251, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/367 [06:57<00:37,  0.81it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [06:58<00:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0295, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/367 [06:58<00:35,  0.81it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [06:59<00:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0382, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/367 [06:59<00:34,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [07:00<00:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0301, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/367 [07:00<00:33,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:02<00:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0339, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/367 [07:02<00:32,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:03<00:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0308, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/367 [07:03<00:30,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:04<00:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0305, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/367 [07:04<00:29,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:05<00:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/367 [07:05<00:28,  0.81it/s, v_num=b5lg, train/loss_step=0.0228, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:07<00:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0228, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/367 [07:07<00:27,  0.81it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:08<00:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0358, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 346/367 [07:08<00:26,  0.81it/s, v_num=b5lg, train/loss_step=0.0269, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:09<00:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0269, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 347/367 [07:09<00:24,  0.81it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:10<00:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0291, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/367 [07:10<00:23,  0.81it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:12<00:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0274, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 349/367 [07:12<00:22,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:13<00:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0314, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 350/367 [07:13<00:21,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:14<00:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0281, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 351/367 [07:14<00:19,  0.81it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:15<00:18,  0.81it/s, v_num=b5lg, train/loss_step=0.0279, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 352/367 [07:15<00:18,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:17<00:17,  0.81it/s, v_num=b5lg, train/loss_step=0.032, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 353/367 [07:17<00:17,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:18<00:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 354/367 [07:18<00:16,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:19<00:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0332, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 355/367 [07:19<00:14,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:20<00:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 356/367 [07:20<00:13,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:21<00:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0292, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 357/367 [07:21<00:12,  0.81it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:23<00:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0282, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 358/367 [07:23<00:11,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:24<00:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 359/367 [07:24<00:09,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:25<00:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0267, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 360/367 [07:25<00:08,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:26<00:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0321, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 361/367 [07:26<00:07,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:27<00:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 362/367 [07:27<00:06,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:29<00:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0334, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/367 [07:29<00:04,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:30<00:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0289, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 364/367 [07:30<00:03,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:31<00:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0331, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 365/367 [07:31<00:02,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:32<00:01,  0.81it/s, v_num=b5lg, train/loss_step=0.0277, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 366/367 [07:32<00:01,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324] Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:34<00:00,  0.81it/s, v_num=b5lg, train/loss_step=0.033, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [07:34<00:00,  0.81it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.431, val/mAP=0.272, val/mAP_best=0.272, train/loss_epoch=0.0324]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/41 [00:00<?, ?it/s][A
Validation DataLoader 0:   2%|â–         | 1/41 [00:00<00:25,  1.59it/s][A
Validation DataLoader 0:   5%|â–         | 2/41 [00:01<00:24,  1.59it/s][A
Validation DataLoader 0:   7%|â–‹         | 3/41 [00:01<00:23,  1.60it/s][A
Validation DataLoader 0:  10%|â–‰         | 4/41 [00:02<00:23,  1.60it/s][A
Validation DataLoader 0:  12%|â–ˆâ–        | 5/41 [00:03<00:22,  1.61it/s][A
Validation DataLoader 0:  15%|â–ˆâ–        | 6/41 [00:03<00:21,  1.61it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 7/41 [00:04<00:21,  1.61it/s][A
Validation DataLoader 0:  20%|â–ˆâ–‰        | 8/41 [00:04<00:20,  1.61it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 9/41 [00:05<00:19,  1.61it/s][A
Validation DataLoader 0:  24%|â–ˆâ–ˆâ–       | 10/41 [00:06<00:19,  1.61it/s][A
Validation DataLoader 0:  27%|â–ˆâ–ˆâ–‹       | 11/41 [00:06<00:18,  1.61it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:07<00:18,  1.61it/s][A
Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:08<00:17,  1.61it/s][A
Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:08<00:16,  1.61it/s][A
Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [00:09<00:16,  1.61it/s][A
Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [00:09<00:15,  1.61it/s][A
Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [00:10<00:14,  1.61it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:11<00:14,  1.61it/s][A
Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:11<00:13,  1.61it/s][A
Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:12<00:13,  1.61it/s][A
Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:13<00:12,  1.61it/s][A
Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [00:13<00:11,  1.61it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:14<00:11,  1.61it/s][A
Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [00:14<00:10,  1.61it/s][A
Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [00:15<00:09,  1.61it/s][A
Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/41 [00:16<00:09,  1.61it/s][A
Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [00:16<00:08,  1.61it/s][A
Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [00:17<00:08,  1.61it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [00:18<00:07,  1.61it/s][A
Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/41 [00:18<00:06,  1.61it/s][A
Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [00:19<00:06,  1.61it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [00:19<00:05,  1.61it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:20<00:04,  1.61it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/41 [00:21<00:04,  1.61it/s][A
Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [00:21<00:03,  1.61it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [00:22<00:03,  1.61it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [00:22<00:02,  1.61it/s][A
Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 38/41 [00:23<00:01,  1.61it/s][A
Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [00:24<00:01,  1.61it/s][A
Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [00:24<00:00,  1.61it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.61it/s][A
                                                                        [AEpoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:01<00:00,  0.76it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.638, val/mAP=0.308, val/mAP_best=0.308, train/loss_epoch=0.0324]Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:01<00:00,  0.76it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.638, val/mAP=0.308, val/mAP_best=0.308, train/loss_epoch=0.0302]`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367/367 [08:01<00:00,  0.76it/s, v_num=b5lg, train/loss_step=0.0284, val/loss=0.638, val/mAP=0.308, val/mAP_best=0.308, train/loss_epoch=0.0302]
[[36m2024-08-16 02:18:39,857[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Training completed![0m
[[36m2024-08-16 02:18:39,859[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Evaluating  for single model![0m
Restoring states from the checkpoint path at /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-16_01-36-41/checkpoints/epoch_004.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-16_01-36-41/checkpoints/epoch_004.ckpt
/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:232: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): SyncBatchNorm(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/103 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/103 [00:00<?, ?it/s]Testing DataLoader 0:   1%|          | 1/103 [00:00<01:07,  1.50it/s]Testing DataLoader 0:   2%|â–         | 2/103 [00:01<01:05,  1.55it/s]Testing DataLoader 0:   3%|â–         | 3/103 [00:01<01:03,  1.57it/s]Testing DataLoader 0:   4%|â–         | 4/103 [00:02<01:03,  1.57it/s]Testing DataLoader 0:   5%|â–         | 5/103 [00:03<01:02,  1.57it/s]Testing DataLoader 0:   6%|â–Œ         | 6/103 [00:03<01:01,  1.58it/s]Testing DataLoader 0:   7%|â–‹         | 7/103 [00:04<01:00,  1.58it/s]Testing DataLoader 0:   8%|â–Š         | 8/103 [00:05<01:00,  1.58it/s]Testing DataLoader 0:   9%|â–Š         | 9/103 [00:05<00:59,  1.58it/s]Testing DataLoader 0:  10%|â–‰         | 10/103 [00:06<00:58,  1.58it/s]Testing DataLoader 0:  11%|â–ˆ         | 11/103 [00:06<00:58,  1.58it/s]Testing DataLoader 0:  12%|â–ˆâ–        | 12/103 [00:07<00:57,  1.58it/s]Testing DataLoader 0:  13%|â–ˆâ–        | 13/103 [00:08<00:56,  1.58it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 14/103 [00:08<00:56,  1.58it/s]Testing DataLoader 0:  15%|â–ˆâ–        | 15/103 [00:09<00:55,  1.58it/s]Testing DataLoader 0:  16%|â–ˆâ–Œ        | 16/103 [00:10<00:54,  1.58it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 17/103 [00:10<00:54,  1.58it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 18/103 [00:11<00:53,  1.58it/s]Testing DataLoader 0:  18%|â–ˆâ–Š        | 19/103 [00:12<00:53,  1.58it/s]Testing DataLoader 0:  19%|â–ˆâ–‰        | 20/103 [00:12<00:52,  1.58it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 21/103 [00:13<00:51,  1.58it/s]Testing DataLoader 0:  21%|â–ˆâ–ˆâ–       | 22/103 [00:13<00:51,  1.59it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 23/103 [00:14<00:50,  1.59it/s]Testing DataLoader 0:  23%|â–ˆâ–ˆâ–       | 24/103 [00:15<00:49,  1.59it/s]Testing DataLoader 0:  24%|â–ˆâ–ˆâ–       | 25/103 [00:15<00:49,  1.59it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 26/103 [00:16<00:48,  1.59it/s]Testing DataLoader 0:  26%|â–ˆâ–ˆâ–Œ       | 27/103 [00:16<00:47,  1.59it/s]Testing DataLoader 0:  27%|â–ˆâ–ˆâ–‹       | 28/103 [00:17<00:47,  1.59it/s]Testing DataLoader 0:  28%|â–ˆâ–ˆâ–Š       | 29/103 [00:18<00:46,  1.59it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–‰       | 30/103 [00:18<00:45,  1.59it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 31/103 [00:19<00:45,  1.59it/s]Testing DataLoader 0:  31%|â–ˆâ–ˆâ–ˆ       | 32/103 [00:20<00:44,  1.59it/s]Testing DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 33/103 [00:20<00:43,  1.59it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 34/103 [00:21<00:43,  1.59it/s]Testing DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 35/103 [00:21<00:42,  1.59it/s]Testing DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 36/103 [00:22<00:42,  1.59it/s]Testing DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 37/103 [00:23<00:41,  1.59it/s]Testing DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 38/103 [00:23<00:40,  1.59it/s]Testing DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 39/103 [00:24<00:40,  1.59it/s]Testing DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 40/103 [00:25<00:39,  1.59it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 41/103 [00:25<00:38,  1.59it/s]Testing DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 42/103 [00:26<00:38,  1.59it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/103 [00:26<00:37,  1.59it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/103 [00:27<00:36,  1.59it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/103 [00:28<00:36,  1.59it/s]Testing DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/103 [00:28<00:35,  1.59it/s]Testing DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 47/103 [00:29<00:35,  1.59it/s]Testing DataLoader 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 48/103 [00:30<00:34,  1.59it/s]Testing DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/103 [00:30<00:33,  1.59it/s]Testing DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 50/103 [00:31<00:33,  1.59it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 51/103 [00:31<00:32,  1.59it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 52/103 [00:32<00:31,  1.60it/s]Testing DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/103 [00:33<00:31,  1.59it/s]Testing DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/103 [00:33<00:30,  1.59it/s]Testing DataLoader 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 55/103 [00:34<00:30,  1.59it/s]Testing DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/103 [00:35<00:29,  1.59it/s]Testing DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 57/103 [00:35<00:28,  1.59it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 58/103 [00:36<00:28,  1.60it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 59/103 [00:36<00:27,  1.60it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 60/103 [00:37<00:26,  1.60it/s]Testing DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 61/103 [00:38<00:26,  1.60it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 62/103 [00:38<00:25,  1.60it/s]Testing DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 63/103 [00:39<00:25,  1.60it/s]Testing DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/103 [00:40<00:24,  1.60it/s]Testing DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 65/103 [00:40<00:23,  1.60it/s]Testing DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 66/103 [00:41<00:23,  1.60it/s]Testing DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 67/103 [00:41<00:22,  1.60it/s]Testing DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 68/103 [00:42<00:21,  1.60it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 69/103 [00:43<00:21,  1.60it/s]Testing DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 70/103 [00:43<00:20,  1.60it/s]Testing DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 71/103 [00:44<00:20,  1.60it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 72/103 [00:45<00:19,  1.60it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 73/103 [00:45<00:18,  1.60it/s]Testing DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/103 [00:46<00:18,  1.60it/s]Testing DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 75/103 [00:46<00:17,  1.60it/s]Testing DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 76/103 [00:47<00:16,  1.60it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 77/103 [00:48<00:16,  1.60it/s]Testing DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 78/103 [00:48<00:15,  1.60it/s]Testing DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 79/103 [00:49<00:15,  1.60it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 80/103 [00:50<00:14,  1.60it/s]Testing DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 81/103 [00:50<00:13,  1.60it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 82/103 [00:51<00:13,  1.60it/s]Testing DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 83/103 [00:51<00:12,  1.60it/s]Testing DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/103 [00:52<00:11,  1.60it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 85/103 [00:53<00:11,  1.60it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 86/103 [00:53<00:10,  1.60it/s]Testing DataLoader 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 87/103 [00:54<00:10,  1.60it/s]Testing DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 88/103 [00:55<00:09,  1.60it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 89/103 [00:55<00:08,  1.60it/s]Testing DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 90/103 [00:56<00:08,  1.60it/s]Testing DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 91/103 [00:56<00:07,  1.60it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 92/103 [00:57<00:06,  1.60it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 93/103 [00:58<00:06,  1.60it/s]Testing DataLoader 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/103 [00:58<00:05,  1.60it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 95/103 [00:59<00:05,  1.60it/s]Testing DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 96/103 [01:00<00:04,  1.60it/s]Testing DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 97/103 [01:00<00:03,  1.60it/s]Testing DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 98/103 [01:01<00:03,  1.60it/s]Testing DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 99/103 [01:01<00:02,  1.60it/s]Testing DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 100/103 [01:02<00:01,  1.60it/s]Testing DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 101/103 [01:03<00:01,  1.60it/s]Testing DataLoader 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 102/103 [01:03<00:00,  1.60it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [01:04<00:00,  1.61it/s]Error executing job with overrides: ['trainer.max_epochs=5', 'trainer.min_epochs=3', 'trainer.devices=2', 'trainer.strategy=ddp', 'data.batch_size=50']
Traceback (most recent call last):
  File "/data/home/acw572/hgann/HGANN/src/train.py", line 197, in <module>
    main()
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/hgann/HGANN/src/train.py", line 181, in main
    metrics,_ = train(cfg)
                ^^^^^^^^^^
  File "/data/home/acw572/hgann/HGANN/src/utils/utils.py", line 111, in wrap
    raise ex
  File "/data/home/acw572/hgann/HGANN/src/utils/utils.py", line 101, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
                               ^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/hgann/HGANN/src/train.py", line 128, in train
    test_results = trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 754, in test
    return call._call_and_handle_interrupt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 102, in launch
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 794, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1028, in _run_stage
    return self._evaluation_loop.run()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 141, in run
    return self.on_run_end()
           ^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 253, in on_run_end
    self._on_evaluation_epoch_end()
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 329, in _on_evaluation_epoch_end
    call._call_lightning_module_hook(trainer, hook_name)
  File "/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/data/home/acw572/hgann/HGANN/src/models/tagging_module_test.py", line 237, in on_test_epoch_end
    return {'test_mAP': mAP}
                        ^^^
UnboundLocalError: cannot access local variable 'mAP' where it is not associated with a value
[[36m2024-08-16 02:19:52,126[0m][[34mroot[0m][[32mINFO[0m] - logging on rank 0[0m
[[36m2024-08-16 02:19:52,127[0m][[34mroot[0m][[32mINFO[0m] - test_mAP: 0.29954282307942187[0m
[rank: 1] Child process with PID 47860 terminated with code 1. Forcefully terminating all other processes to avoid zombies ğŸ§Ÿ
/opt/sge_spool/8.6.12b/rdg9/job_scripts/3796991: line 26: 46261 Killed                  HYDRA_FULL_ERROR=1 python src/train.py trainer.max_epochs=$max_epochs trainer.min_epochs=$min_epochs trainer.devices=$num_devices trainer.strategy=$strategy data.batch_size=$((batch_size * num_devices))
