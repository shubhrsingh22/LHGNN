Variable OMP_NUM_THREADS has been set to 24
--------------------
Hostname: rdg1
Thu Aug 15 12:56:02 BST 2024
Free GPU: 2 of 2
--------------------
GPU0: [92mNot in use.[39m
GPU1: [92mNot in use.[39m

User: [91macw572[39m JobID: [91m3793933[39m GPU Allocation: [91m2[39m Queue: [91mshort.q[39m
[91mWarning! GPUs requested but not used![39m
In main
[[36m2024-08-15 12:56:49,206[0m][[34mutils.utils[0m][[32mINFO[0m] - [rank: 0] Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2024-08-15 12:56:49,211[0m][[34mutils.utils[0m][[32mINFO[0m] - [rank: 0] Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.fsd_datamodule.FSDDataModule                         
â”‚       json_path: /data/scratch/acw572/LHGNN/datafiles/                        
â”‚       data_dir: /data/EECS-MachineListeningLab/datasets/AudioSet              
â”‚       meta_path: /data/EECS-MachineListeningLab/datasets/AudioSet/ground_truth
â”‚       label_csv_pth: /data/scratch/acw572/LHGNN/datafiles/class_labels_indices
â”‚       samplr_csv_pth: /data/scratch/acw572/LHGNN/datafiles/fsd50k_tr_full_weig
â”‚       balance_samplr: true                                                    
â”‚       batch_size: 80                                                          
â”‚       num_workers: 8                                                          
â”‚       pin_memory: true                                                        
â”‚       persistent_workers: true                                                
â”‚       sr: 16000                                                               
â”‚       fmin: 20                                                                
â”‚       fmax: 8000                                                              
â”‚       num_mels: 128                                                           
â”‚       window_type: hanning                                                    
â”‚       target_len: 1024                                                        
â”‚       freqm: 48                                                               
â”‚       timem: 192                                                              
â”‚       mixup: 0.5                                                              
â”‚       norm_mean: -4.6476                                                      
â”‚       norm_std: 4.5699                                                        
â”‚       num_devices: 2                                                          
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.tagging_module_test.TaggingModule                  
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0005                                                            
â”‚         weight_decay: 5.0e-07                                                 
â”‚         eps: 1.0e-08                                                          
â”‚         betas:                                                                
â”‚         - 0.95                                                                
â”‚         - 0.999                                                               
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.MultiStepLR                        
â”‚         _partial_: true                                                       
â”‚         milestones:                                                           
â”‚         - 10                                                                  
â”‚         - 15                                                                  
â”‚         - 20                                                                  
â”‚         - 25                                                                  
â”‚         - 30                                                                  
â”‚         - 35                                                                  
â”‚         - 40                                                                  
â”‚         gamma: 0.5                                                            
â”‚       net:                                                                    
â”‚         _target_: src.models.components.Hypergraph.HGCN                       
â”‚         k: 25                                                                 
â”‚         act: gelu                                                             
â”‚         norm: batch                                                           
â”‚         bias: true                                                            
â”‚         dropout: 0.0                                                          
â”‚         dilation: true                                                        
â”‚         epsilon: 0.2                                                          
â”‚         drop_path: 0.1                                                        
â”‚         size: s                                                               
â”‚         num_class: 200                                                        
â”‚         emb_dims: 1024                                                        
â”‚         freq_num: 128                                                         
â”‚         time_num: 1024                                                        
â”‚       compile: false                                                          
â”‚       loss: bce                                                               
â”‚       opt_warmup: true                                                        
â”‚       learning_rate: 0.0005                                                   
â”‚       lr_rate:                                                                
â”‚       - 0.05                                                                  
â”‚       - 0.02                                                                  
â”‚       - 0.01                                                                  
â”‚       - 0.005                                                                 
â”‚       - 0.002                                                                 
â”‚       - 0.001                                                                 
â”‚       - 0.0005                                                                
â”‚       - 0.0002                                                                
â”‚       lr_scheduler_epoch:                                                     
â”‚       - 10                                                                    
â”‚       - 15                                                                    
â”‚       - 20                                                                    
â”‚       - 25                                                                    
â”‚       - 30                                                                    
â”‚       - 35                                                                    
â”‚       - 50                                                                    
â”‚       - 45                                                                    
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-15_12-56-4
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mAP                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 20                                                        
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/loss                                                     
â”‚         min_delta: 0.0                                                        
â”‚         patience: 5                                                           
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       tqdm_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.TQDMProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.WandbLogger                       
â”‚         save_dir: /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-15_12-56-
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: audioset-bal                                                 
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: Tagging                                                        
â”‚         tags:                                                                 
â”‚         - fsd                                                                 
â”‚         - hgcn                                                                
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.trainer.Trainer                             
â”‚       default_root_dir: /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-15_
â”‚       num_sanity_val_steps: 0                                                 
â”‚       min_epochs: 3                                                           
â”‚       max_epochs: 5                                                           
â”‚       accelerator: gpu                                                        
â”‚       devices: 2                                                              
â”‚       gradient_clip_val: 0.5                                                  
â”‚       precision: 32                                                           
â”‚       detect_anomaly: false                                                   
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       strategy: ddp                                                           
â”‚       num_nodes: 1                                                            
â”‚       sync_batchnorm: false                                                   
â”‚       use_distributed_sampler: false                                          
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /data/home/acw572/hgann/HGANN                                 
â”‚       exp_dir: /data/scratch/acw572                                           
â”‚       data_dir: /data/EECS-MachineListeningLab/datasets/AudioSet              
â”‚       meta_dir: /data/EECS-MachineListeningLab/shubhr/hgann                   
â”‚       log_dir: /data/scratch/acw572/LHGNN/logs/                               
â”‚       output_dir: /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-15_12-56-
â”‚       work_dir: /data/home/acw572/hgann/HGANN                                 
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ pretrained
â”‚   â””â”€â”€ img                                                                     
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['dev']                                                                 
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ eval
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ wa
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ /data/EECS-MachineListeningLab/shubhr/imagenet_weights/model_best.pth.ta
â””â”€â”€ seed
    â””â”€â”€ None                                                                    
[[36m2024-08-15 12:56:49,306[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] In train[0m
[[36m2024-08-15 12:56:49,306[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating datamodule <src.data.fsd_datamodule.FSDDataModule>[0m
[[36m2024-08-15 12:56:50,668[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating model <src.models.tagging_module_test.TaggingModule>[0m
/data/home/acw572/.conda/envs/hgann/lib/python3.12/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
norm is batch
bias is True
drop_path is 0.1
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
[[36m2024-08-15 12:57:16,364[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Loading img pretrained weights[0m
[[36m2024-08-15 12:57:16,364[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating callbacks...[0m
[[36m2024-08-15 12:57:16,364[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2024-08-15 12:57:16,368[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2024-08-15 12:57:16,369[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2024-08-15 12:57:16,369[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <pytorch_lightning.callbacks.TQDMProgressBar>[0m
[[36m2024-08-15 12:57:16,370[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating loggers...[0m
[[36m2024-08-15 12:57:16,370[0m][[34mutils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating logger <pytorch_lightning.loggers.WandbLogger>[0m
[[36m2024-08-15 12:57:16,590[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating trainer <pytorch_lightning.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-08-15 12:57:16,782[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Logging hyperparameters![0m
wandb: Currently logged in as: shubhr. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /data/scratch/acw572/LHGNN/logs/train/runs/2024-08-15_12-56-48/wandb/run-20240815_125719-tf8c0w59
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-durian-46
wandb: â­ï¸ View project at https://wandb.ai/shubhr/audioset-bal
wandb: ğŸš€ View run at https://wandb.ai/shubhr/audioset-bal/runs/tf8c0w59
[[36m2024-08-15 12:57:39,292[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting training![0m
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”³â”â”â”“
â”ƒ   â”ƒ Name                                                              â”ƒ â€¦ â”ƒ  â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â•‡â”â”â”©
â”‚ 0 â”‚ net                                                               â”‚ â€¦ â”‚  â”‚
â”‚ 1 â”‚ net.stem                                                          â”‚ â€¦ â”‚  â”‚
â”‚ 2 â”‚ net.stem.convs                                                    â”‚ â€¦ â”‚  â”‚
â”‚ 3 â”‚ net.stem.convs.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 4 â”‚ net.stem.convs.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 5 â”‚ net.stem.convs.2                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 6 â”‚ net.stem.convs.3                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 7 â”‚ net.stem.convs.4                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 8 â”‚ net.stem.convs.5                                                  â”‚ â€¦ â”‚  â”‚
â”‚ 9 â”‚ net.stem.convs.6                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.stem.convs.7                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone                                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.0.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.1.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.2                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.2.conv                                               â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.2.conv.0                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.2.conv.1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.3.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.4.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.5                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.5.conv                                               â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.5.conv.0                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.5.conv.1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.6.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.7.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.8.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.nn                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.nn.0                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.nn.1                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.nn.2                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.get_centroids                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.gconv.get_centroids.centers_proposal  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.dilated_knn_graph                     â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.graph_conv.dilated_knn_graph._dilated            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.0.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc1                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc1.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc1.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.act                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc2                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc2.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.fc2.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.conv                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.conv.conv                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.9.1.drop_path                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv                                â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.nn                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.nn.0                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.nn.1                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.nn.2                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.get_centroids                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.gconv.get_centroids.centers_proposal â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.dilated_knn_graph                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.graph_conv.dilated_knn_graph._dilated           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.0.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.act                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.conv                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.conv.conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.10.1.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv                                â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.nn                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.nn.0                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.nn.1                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.nn.2                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.get_centroids                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.gconv.get_centroids.centers_proposal â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.dilated_knn_graph                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.graph_conv.dilated_knn_graph._dilated           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.0.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.act                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.conv                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.conv.conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.11.1.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.12                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.12.conv                                              â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.12.conv.0                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.12.conv.1                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv                                â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.nn                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.nn.0                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.nn.1                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.nn.2                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.get_centroids                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.gconv.get_centroids.centers_proposal â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.dilated_knn_graph                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.graph_conv.dilated_knn_graph._dilated           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.0.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.act                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.conv                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.conv.conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.13.1.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14                                                   â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv                                      â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv                                â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.nn                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.nn.0                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.nn.1                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.nn.2                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.get_centroids                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.gconv.get_centroids.centers_proposal â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.dilated_knn_graph                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.graph_conv.dilated_knn_graph._dilated           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.0.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1                                                 â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc1                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc1.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc1.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.act                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc2                                             â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc2.0                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.fc2.1                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.conv                                            â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.conv.conv                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.backbone.14.1.drop_path                                       â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction                                                    â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.0                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.1                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.2                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.3                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ net.prediction.4                                                  â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ criterion                                                         â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ train_loss                                                        â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ val_loss                                                          â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ test_loss                                                         â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ val_mAP                                                           â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ test_mAP                                                          â”‚ â€¦ â”‚  â”‚
â”‚ â€¦ â”‚ val_mAP_best                                                      â”‚ â€¦ â”‚  â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”˜
Trainable params: 31.1 M                                                        
Non-trainable params: 12.1 M                                                    
Total params: 43.2 M                                                            
Total estimated model params size (MB): 172                                     
In main
norm is batch
bias is True
drop_path is 0.1
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
using relative_pos
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/459 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/459 [00:00<?, ?it/s] hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
Epoch 0:   0%|          | 1/459 [00:17<2:12:46,  0.06it/s]Epoch 0:   0%|          | 1/459 [00:17<2:12:47,  0.06it/s, v_num=0w59, train/loss_step=0.771]Epoch 0:   0%|          | 2/459 [00:19<1:12:29,  0.11it/s, v_num=0w59, train/loss_step=0.771]Epoch 0:   0%|          | 2/459 [00:19<1:12:29,  0.11it/s, v_num=0w59, train/loss_step=0.767]Epoch 0:   1%|          | 3/459 [00:20<52:13,  0.15it/s, v_num=0w59, train/loss_step=0.767]  Epoch 0:   1%|          | 3/459 [00:20<52:13,  0.15it/s, v_num=0w59, train/loss_step=0.769]Epoch 0:   1%|          | 4/459 [00:22<42:05,  0.18it/s, v_num=0w59, train/loss_step=0.769]Epoch 0:   1%|          | 4/459 [00:22<42:05,  0.18it/s, v_num=0w59, train/loss_step=0.761]Epoch 0:   1%|          | 5/459 [00:23<35:59,  0.21it/s, v_num=0w59, train/loss_step=0.761]Epoch 0:   1%|          | 5/459 [00:23<35:59,  0.21it/s, v_num=0w59, train/loss_step=0.771]Epoch 0:   1%|â–         | 6/459 [00:25<31:55,  0.24it/s, v_num=0w59, train/loss_step=0.771]Epoch 0:   1%|â–         | 6/459 [00:25<31:55,  0.24it/s, v_num=0w59, train/loss_step=0.764]Epoch 0:   2%|â–         | 7/459 [00:26<29:00,  0.26it/s, v_num=0w59, train/loss_step=0.764]Epoch 0:   2%|â–         | 7/459 [00:26<29:00,  0.26it/s, v_num=0w59, train/loss_step=0.773]Epoch 0:   2%|â–         | 8/459 [00:28<26:49,  0.28it/s, v_num=0w59, train/loss_step=0.773]Epoch 0:   2%|â–         | 8/459 [00:28<26:49,  0.28it/s, v_num=0w59, train/loss_step=0.770]Epoch 0:   2%|â–         | 9/459 [00:30<25:06,  0.30it/s, v_num=0w59, train/loss_step=0.770]Epoch 0:   2%|â–         | 9/459 [00:30<25:06,  0.30it/s, v_num=0w59, train/loss_step=0.757]Epoch 0:   2%|â–         | 10/459 [00:31<23:44,  0.32it/s, v_num=0w59, train/loss_step=0.757]Epoch 0:   2%|â–         | 10/459 [00:31<23:44,  0.32it/s, v_num=0w59, train/loss_step=0.770]Epoch 0:   2%|â–         | 11/459 [00:33<22:36,  0.33it/s, v_num=0w59, train/loss_step=0.770]Epoch 0:   2%|â–         | 11/459 [00:33<22:36,  0.33it/s, v_num=0w59, train/loss_step=0.772]Epoch 0:   3%|â–         | 12/459 [00:34<21:39,  0.34it/s, v_num=0w59, train/loss_step=0.772]Epoch 0:   3%|â–         | 12/459 [00:34<21:39,  0.34it/s, v_num=0w59, train/loss_step=0.757]Epoch 0:   3%|â–         | 13/459 [00:36<20:51,  0.36it/s, v_num=0w59, train/loss_step=0.757]Epoch 0:   3%|â–         | 13/459 [00:36<20:51,  0.36it/s, v_num=0w59, train/loss_step=0.754]Epoch 0:   3%|â–         | 14/459 [00:38<20:09,  0.37it/s, v_num=0w59, train/loss_step=0.754]Epoch 0:   3%|â–         | 14/459 [00:38<20:09,  0.37it/s, v_num=0w59, train/loss_step=0.771]Epoch 0:   3%|â–         | 15/459 [00:39<19:32,  0.38it/s, v_num=0w59, train/loss_step=0.771]Epoch 0:   3%|â–         | 15/459 [00:39<19:32,  0.38it/s, v_num=0w59, train/loss_step=0.765]Epoch 0:   3%|â–         | 16/459 [00:41<19:00,  0.39it/s, v_num=0w59, train/loss_step=0.765]Epoch 0:   3%|â–         | 16/459 [00:41<19:00,  0.39it/s, v_num=0w59, train/loss_step=0.758]Epoch 0:   4%|â–         | 17/459 [00:42<18:31,  0.40it/s, v_num=0w59, train/loss_step=0.758]Epoch 0:   4%|â–         | 17/459 [00:42<18:31,  0.40it/s, v_num=0w59, train/loss_step=0.753]Epoch 0:   4%|â–         | 18/459 [00:44<18:06,  0.41it/s, v_num=0w59, train/loss_step=0.753]Epoch 0:   4%|â–         | 18/459 [00:44<18:06,  0.41it/s, v_num=0w59, train/loss_step=0.751]Epoch 0:   4%|â–         | 19/459 [00:45<17:42,  0.41it/s, v_num=0w59, train/loss_step=0.751]Epoch 0:   4%|â–         | 19/459 [00:45<17:42,  0.41it/s, v_num=0w59, train/loss_step=0.754]Epoch 0:   4%|â–         | 20/459 [00:47<17:21,  0.42it/s, v_num=0w59, train/loss_step=0.754]Epoch 0:   4%|â–         | 20/459 [00:47<17:22,  0.42it/s, v_num=0w59, train/loss_step=0.742]Epoch 0:   5%|â–         | 21/459 [00:49<17:02,  0.43it/s, v_num=0w59, train/loss_step=0.742]Epoch 0:   5%|â–         | 21/459 [00:49<17:02,  0.43it/s, v_num=0w59, train/loss_step=0.749]Epoch 0:   5%|â–         | 22/459 [00:50<16:45,  0.43it/s, v_num=0w59, train/loss_step=0.749]Epoch 0:   5%|â–         | 22/459 [00:50<16:45,  0.43it/s, v_num=0w59, train/loss_step=0.739]Epoch 0:   5%|â–Œ         | 23/459 [00:52<16:29,  0.44it/s, v_num=0w59, train/loss_step=0.739]Epoch 0:   5%|â–Œ         | 23/459 [00:52<16:29,  0.44it/s, v_num=0w59, train/loss_step=0.731]Epoch 0:   5%|â–Œ         | 24/459 [00:53<16:14,  0.45it/s, v_num=0w59, train/loss_step=0.731]Epoch 0:   5%|â–Œ         | 24/459 [00:53<16:14,  0.45it/s, v_num=0w59, train/loss_step=0.738]Epoch 0:   5%|â–Œ         | 25/459 [00:55<16:00,  0.45it/s, v_num=0w59, train/loss_step=0.738]Epoch 0:   5%|â–Œ         | 25/459 [00:55<16:00,  0.45it/s, v_num=0w59, train/loss_step=0.731]Epoch 0:   6%|â–Œ         | 26/459 [00:56<15:47,  0.46it/s, v_num=0w59, train/loss_step=0.731]Epoch 0:   6%|â–Œ         | 26/459 [00:56<15:47,  0.46it/s, v_num=0w59, train/loss_step=0.743]Epoch 0:   6%|â–Œ         | 27/459 [00:58<15:35,  0.46it/s, v_num=0w59, train/loss_step=0.743]Epoch 0:   6%|â–Œ         | 27/459 [00:58<15:35,  0.46it/s, v_num=0w59, train/loss_step=0.734]Epoch 0:   6%|â–Œ         | 28/459 [01:00<15:24,  0.47it/s, v_num=0w59, train/loss_step=0.734]Epoch 0:   6%|â–Œ         | 28/459 [01:00<15:24,  0.47it/s, v_num=0w59, train/loss_step=0.721]Epoch 0:   6%|â–‹         | 29/459 [01:01<15:13,  0.47it/s, v_num=0w59, train/loss_step=0.721]Epoch 0:   6%|â–‹         | 29/459 [01:01<15:13,  0.47it/s, v_num=0w59, train/loss_step=0.725]Epoch 0:   7%|â–‹         | 30/459 [01:03<15:03,  0.47it/s, v_num=0w59, train/loss_step=0.725]Epoch 0:   7%|â–‹         | 30/459 [01:03<15:03,  0.47it/s, v_num=0w59, train/loss_step=0.713]Epoch 0:   7%|â–‹         | 31/459 [01:04<14:54,  0.48it/s, v_num=0w59, train/loss_step=0.713]Epoch 0:   7%|â–‹         | 31/459 [01:04<14:54,  0.48it/s, v_num=0w59, train/loss_step=0.714]Epoch 0:   7%|â–‹         | 32/459 [01:06<14:45,  0.48it/s, v_num=0w59, train/loss_step=0.714]Epoch 0:   7%|â–‹         | 32/459 [01:06<14:45,  0.48it/s, v_num=0w59, train/loss_step=0.721]Epoch 0:   7%|â–‹         | 33/459 [01:07<14:37,  0.49it/s, v_num=0w59, train/loss_step=0.721]Epoch 0:   7%|â–‹         | 33/459 [01:07<14:37,  0.49it/s, v_num=0w59, train/loss_step=0.717]Epoch 0:   7%|â–‹         | 34/459 [01:09<14:29,  0.49it/s, v_num=0w59, train/loss_step=0.717]Epoch 0:   7%|â–‹         | 34/459 [01:09<14:29,  0.49it/s, v_num=0w59, train/loss_step=0.700]Epoch 0:   8%|â–Š         | 35/459 [01:11<14:21,  0.49it/s, v_num=0w59, train/loss_step=0.700]Epoch 0:   8%|â–Š         | 35/459 [01:11<14:21,  0.49it/s, v_num=0w59, train/loss_step=0.713]Epoch 0:   8%|â–Š         | 36/459 [01:12<14:14,  0.50it/s, v_num=0w59, train/loss_step=0.713]Epoch 0:   8%|â–Š         | 36/459 [01:12<14:14,  0.50it/s, v_num=0w59, train/loss_step=0.689]Epoch 0:   8%|â–Š         | 37/459 [01:14<14:07,  0.50it/s, v_num=0w59, train/loss_step=0.689]Epoch 0:   8%|â–Š         | 37/459 [01:14<14:07,  0.50it/s, v_num=0w59, train/loss_step=0.694]Epoch 0:   8%|â–Š         | 38/459 [01:15<14:00,  0.50it/s, v_num=0w59, train/loss_step=0.694]Epoch 0:   8%|â–Š         | 38/459 [01:15<14:00,  0.50it/s, v_num=0w59, train/loss_step=0.688]Epoch 0:   8%|â–Š         | 39/459 [01:17<13:53,  0.50it/s, v_num=0w59, train/loss_step=0.688]Epoch 0:   8%|â–Š         | 39/459 [01:17<13:53,  0.50it/s, v_num=0w59, train/loss_step=0.683]Epoch 0:   9%|â–Š         | 40/459 [01:19<13:47,  0.51it/s, v_num=0w59, train/loss_step=0.683]Epoch 0:   9%|â–Š         | 40/459 [01:19<13:47,  0.51it/s, v_num=0w59, train/loss_step=0.672]Epoch 0:   9%|â–‰         | 41/459 [01:20<13:41,  0.51it/s, v_num=0w59, train/loss_step=0.672]Epoch 0:   9%|â–‰         | 41/459 [01:20<13:41,  0.51it/s, v_num=0w59, train/loss_step=0.680]Epoch 0:   9%|â–‰         | 42/459 [01:22<13:36,  0.51it/s, v_num=0w59, train/loss_step=0.680]Epoch 0:   9%|â–‰         | 42/459 [01:22<13:36,  0.51it/s, v_num=0w59, train/loss_step=0.666]Epoch 0:   9%|â–‰         | 43/459 [01:23<13:30,  0.51it/s, v_num=0w59, train/loss_step=0.666]Epoch 0:   9%|â–‰         | 43/459 [01:23<13:30,  0.51it/s, v_num=0w59, train/loss_step=0.681]Epoch 0:  10%|â–‰         | 44/459 [01:25<13:25,  0.52it/s, v_num=0w59, train/loss_step=0.681]Epoch 0:  10%|â–‰         | 44/459 [01:25<13:25,  0.52it/s, v_num=0w59, train/loss_step=0.659]Epoch 0:  10%|â–‰         | 45/459 [01:26<13:19,  0.52it/s, v_num=0w59, train/loss_step=0.659]Epoch 0:  10%|â–‰         | 45/459 [01:26<13:19,  0.52it/s, v_num=0w59, train/loss_step=0.663]Epoch 0:  10%|â–ˆ         | 46/459 [01:28<13:14,  0.52it/s, v_num=0w59, train/loss_step=0.663]Epoch 0:  10%|â–ˆ         | 46/459 [01:28<13:14,  0.52it/s, v_num=0w59, train/loss_step=0.648]Epoch 0:  10%|â–ˆ         | 47/459 [01:30<13:09,  0.52it/s, v_num=0w59, train/loss_step=0.648]Epoch 0:  10%|â–ˆ         | 47/459 [01:30<13:09,  0.52it/s, v_num=0w59, train/loss_step=0.655]Epoch 0:  10%|â–ˆ         | 48/459 [01:31<13:05,  0.52it/s, v_num=0w59, train/loss_step=0.655]Epoch 0:  10%|â–ˆ         | 48/459 [01:31<13:05,  0.52it/s, v_num=0w59, train/loss_step=0.645]Epoch 0:  11%|â–ˆ         | 49/459 [01:33<13:00,  0.53it/s, v_num=0w59, train/loss_step=0.645]Epoch 0:  11%|â–ˆ         | 49/459 [01:33<13:00,  0.53it/s, v_num=0w59, train/loss_step=0.645]Epoch 0:  11%|â–ˆ         | 50/459 [01:34<12:56,  0.53it/s, v_num=0w59, train/loss_step=0.645]Epoch 0:  11%|â–ˆ         | 50/459 [01:34<12:56,  0.53it/s, v_num=0w59, train/loss_step=0.635]Epoch 0:  11%|â–ˆ         | 51/459 [01:36<12:51,  0.53it/s, v_num=0w59, train/loss_step=0.635]Epoch 0:  11%|â–ˆ         | 51/459 [01:36<12:51,  0.53it/s, v_num=0w59, train/loss_step=0.647]Epoch 0:  11%|â–ˆâ–        | 52/459 [01:38<12:47,  0.53it/s, v_num=0w59, train/loss_step=0.647]Epoch 0:  11%|â–ˆâ–        | 52/459 [01:38<12:47,  0.53it/s, v_num=0w59, train/loss_step=0.625]Epoch 0:  12%|â–ˆâ–        | 53/459 [01:39<12:43,  0.53it/s, v_num=0w59, train/loss_step=0.625]Epoch 0:  12%|â–ˆâ–        | 53/459 [01:39<12:43,  0.53it/s, v_num=0w59, train/loss_step=0.623]Epoch 0:  12%|â–ˆâ–        | 54/459 [01:41<12:39,  0.53it/s, v_num=0w59, train/loss_step=0.623]Epoch 0:  12%|â–ˆâ–        | 54/459 [01:41<12:39,  0.53it/s, v_num=0w59, train/loss_step=0.624]Epoch 0:  12%|â–ˆâ–        | 55/459 [01:42<12:35,  0.53it/s, v_num=0w59, train/loss_step=0.624]Epoch 0:  12%|â–ˆâ–        | 55/459 [01:42<12:35,  0.53it/s, v_num=0w59, train/loss_step=0.645]Epoch 0:  12%|â–ˆâ–        | 56/459 [01:44<12:32,  0.54it/s, v_num=0w59, train/loss_step=0.645]Epoch 0:  12%|â–ˆâ–        | 56/459 [01:44<12:32,  0.54it/s, v_num=0w59, train/loss_step=0.601]Epoch 0:  12%|â–ˆâ–        | 57/459 [01:46<12:28,  0.54it/s, v_num=0w59, train/loss_step=0.601]Epoch 0:  12%|â–ˆâ–        | 57/459 [01:46<12:28,  0.54it/s, v_num=0w59, train/loss_step=0.600]Epoch 0:  13%|â–ˆâ–        | 58/459 [01:47<12:24,  0.54it/s, v_num=0w59, train/loss_step=0.600]Epoch 0:  13%|â–ˆâ–        | 58/459 [01:47<12:24,  0.54it/s, v_num=0w59, train/loss_step=0.603]Epoch 0:  13%|â–ˆâ–        | 59/459 [01:49<12:21,  0.54it/s, v_num=0w59, train/loss_step=0.603]Epoch 0:  13%|â–ˆâ–        | 59/459 [01:49<12:21,  0.54it/s, v_num=0w59, train/loss_step=0.595]Epoch 0:  13%|â–ˆâ–        | 60/459 [01:50<12:17,  0.54it/s, v_num=0w59, train/loss_step=0.595]Epoch 0:  13%|â–ˆâ–        | 60/459 [01:50<12:17,  0.54it/s, v_num=0w59, train/loss_step=0.591]Epoch 0:  13%|â–ˆâ–        | 61/459 [01:52<12:14,  0.54it/s, v_num=0w59, train/loss_step=0.591]Epoch 0:  13%|â–ˆâ–        | 61/459 [01:52<12:14,  0.54it/s, v_num=0w59, train/loss_step=0.575]Epoch 0:  14%|â–ˆâ–        | 62/459 [01:54<12:11,  0.54it/s, v_num=0w59, train/loss_step=0.575]Epoch 0:  14%|â–ˆâ–        | 62/459 [01:54<12:11,  0.54it/s, v_num=0w59, train/loss_step=0.573]Epoch 0:  14%|â–ˆâ–        | 63/459 [01:55<12:08,  0.54it/s, v_num=0w59, train/loss_step=0.573]Epoch 0:  14%|â–ˆâ–        | 63/459 [01:55<12:08,  0.54it/s, v_num=0w59, train/loss_step=0.569]Epoch 0:  14%|â–ˆâ–        | 64/459 [01:57<12:04,  0.54it/s, v_num=0w59, train/loss_step=0.569]Epoch 0:  14%|â–ˆâ–        | 64/459 [01:57<12:04,  0.54it/s, v_num=0w59, train/loss_step=0.567]Epoch 0:  14%|â–ˆâ–        | 65/459 [01:59<12:01,  0.55it/s, v_num=0w59, train/loss_step=0.567]Epoch 0:  14%|â–ˆâ–        | 65/459 [01:59<12:01,  0.55it/s, v_num=0w59, train/loss_step=0.555]Epoch 0:  14%|â–ˆâ–        | 66/459 [02:00<11:58,  0.55it/s, v_num=0w59, train/loss_step=0.555]Epoch 0:  14%|â–ˆâ–        | 66/459 [02:00<11:58,  0.55it/s, v_num=0w59, train/loss_step=0.558]Epoch 0:  15%|â–ˆâ–        | 67/459 [02:02<11:55,  0.55it/s, v_num=0w59, train/loss_step=0.558]Epoch 0:  15%|â–ˆâ–        | 67/459 [02:02<11:55,  0.55it/s, v_num=0w59, train/loss_step=0.556]Epoch 0:  15%|â–ˆâ–        | 68/459 [02:03<11:52,  0.55it/s, v_num=0w59, train/loss_step=0.556]Epoch 0:  15%|â–ˆâ–        | 68/459 [02:03<11:52,  0.55it/s, v_num=0w59, train/loss_step=0.542]Epoch 0:  15%|â–ˆâ–Œ        | 69/459 [02:05<11:49,  0.55it/s, v_num=0w59, train/loss_step=0.542]Epoch 0:  15%|â–ˆâ–Œ        | 69/459 [02:05<11:49,  0.55it/s, v_num=0w59, train/loss_step=0.542]Epoch 0:  15%|â–ˆâ–Œ        | 70/459 [02:07<11:46,  0.55it/s, v_num=0w59, train/loss_step=0.542]Epoch 0:  15%|â–ˆâ–Œ        | 70/459 [02:07<11:46,  0.55it/s, v_num=0w59, train/loss_step=0.533]Epoch 0:  15%|â–ˆâ–Œ        | 71/459 [02:08<11:43,  0.55it/s, v_num=0w59, train/loss_step=0.533]Epoch 0:  15%|â–ˆâ–Œ        | 71/459 [02:08<11:43,  0.55it/s, v_num=0w59, train/loss_step=0.529]Epoch 0:  16%|â–ˆâ–Œ        | 72/459 [02:10<11:40,  0.55it/s, v_num=0w59, train/loss_step=0.529]Epoch 0:  16%|â–ˆâ–Œ        | 72/459 [02:10<11:40,  0.55it/s, v_num=0w59, train/loss_step=0.520]Epoch 0:  16%|â–ˆâ–Œ        | 73/459 [02:11<11:37,  0.55it/s, v_num=0w59, train/loss_step=0.520]Epoch 0:  16%|â–ˆâ–Œ        | 73/459 [02:11<11:37,  0.55it/s, v_num=0w59, train/loss_step=0.534]Epoch 0:  16%|â–ˆâ–Œ        | 74/459 [02:13<11:34,  0.55it/s, v_num=0w59, train/loss_step=0.534]Epoch 0:  16%|â–ˆâ–Œ        | 74/459 [02:13<11:34,  0.55it/s, v_num=0w59, train/loss_step=0.521]Epoch 0:  16%|â–ˆâ–‹        | 75/459 [02:15<11:31,  0.55it/s, v_num=0w59, train/loss_step=0.521]Epoch 0:  16%|â–ˆâ–‹        | 75/459 [02:15<11:31,  0.55it/s, v_num=0w59, train/loss_step=0.509]Epoch 0:  17%|â–ˆâ–‹        | 76/459 [02:16<11:29,  0.56it/s, v_num=0w59, train/loss_step=0.509]Epoch 0:  17%|â–ˆâ–‹        | 76/459 [02:16<11:29,  0.56it/s, v_num=0w59, train/loss_step=0.510]Epoch 0:  17%|â–ˆâ–‹        | 77/459 [02:18<11:26,  0.56it/s, v_num=0w59, train/loss_step=0.510]Epoch 0:  17%|â–ˆâ–‹        | 77/459 [02:18<11:26,  0.56it/s, v_num=0w59, train/loss_step=0.514]Epoch 0:  17%|â–ˆâ–‹        | 78/459 [02:19<11:23,  0.56it/s, v_num=0w59, train/loss_step=0.514]Epoch 0:  17%|â–ˆâ–‹        | 78/459 [02:19<11:23,  0.56it/s, v_num=0w59, train/loss_step=0.511]Epoch 0:  17%|â–ˆâ–‹        | 79/459 [02:21<11:21,  0.56it/s, v_num=0w59, train/loss_step=0.511]Epoch 0:  17%|â–ˆâ–‹        | 79/459 [02:21<11:21,  0.56it/s, v_num=0w59, train/loss_step=0.516]Epoch 0:  17%|â–ˆâ–‹        | 80/459 [02:23<11:18,  0.56it/s, v_num=0w59, train/loss_step=0.516]Epoch 0:  17%|â–ˆâ–‹        | 80/459 [02:23<11:18,  0.56it/s, v_num=0w59, train/loss_step=0.492]Epoch 0:  18%|â–ˆâ–Š        | 81/459 [02:24<11:15,  0.56it/s, v_num=0w59, train/loss_step=0.492]Epoch 0:  18%|â–ˆâ–Š        | 81/459 [02:24<11:15,  0.56it/s, v_num=0w59, train/loss_step=0.491]Epoch 0:  18%|â–ˆâ–Š        | 82/459 [02:26<11:13,  0.56it/s, v_num=0w59, train/loss_step=0.491]Epoch 0:  18%|â–ˆâ–Š        | 82/459 [02:26<11:13,  0.56it/s, v_num=0w59, train/loss_step=0.488]Epoch 0:  18%|â–ˆâ–Š        | 83/459 [02:28<11:10,  0.56it/s, v_num=0w59, train/loss_step=0.488]Epoch 0:  18%|â–ˆâ–Š        | 83/459 [02:28<11:10,  0.56it/s, v_num=0w59, train/loss_step=0.486]Epoch 0:  18%|â–ˆâ–Š        | 84/459 [02:29<11:08,  0.56it/s, v_num=0w59, train/loss_step=0.486]Epoch 0:  18%|â–ˆâ–Š        | 84/459 [02:29<11:08,  0.56it/s, v_num=0w59, train/loss_step=0.465]Epoch 0:  19%|â–ˆâ–Š        | 85/459 [02:31<11:05,  0.56it/s, v_num=0w59, train/loss_step=0.465]Epoch 0:  19%|â–ˆâ–Š        | 85/459 [02:31<11:05,  0.56it/s, v_num=0w59, train/loss_step=0.470]Epoch 0:  19%|â–ˆâ–Š        | 86/459 [02:32<11:03,  0.56it/s, v_num=0w59, train/loss_step=0.470]Epoch 0:  19%|â–ˆâ–Š        | 86/459 [02:32<11:03,  0.56it/s, v_num=0w59, train/loss_step=0.465]Epoch 0:  19%|â–ˆâ–‰        | 87/459 [02:34<11:00,  0.56it/s, v_num=0w59, train/loss_step=0.465]Epoch 0:  19%|â–ˆâ–‰        | 87/459 [02:34<11:00,  0.56it/s, v_num=0w59, train/loss_step=0.463]Epoch 0:  19%|â–ˆâ–‰        | 88/459 [02:36<10:58,  0.56it/s, v_num=0w59, train/loss_step=0.463]Epoch 0:  19%|â–ˆâ–‰        | 88/459 [02:36<10:58,  0.56it/s, v_num=0w59, train/loss_step=0.459]Epoch 0:  19%|â–ˆâ–‰        | 89/459 [02:37<10:55,  0.56it/s, v_num=0w59, train/loss_step=0.459]Epoch 0:  19%|â–ˆâ–‰        | 89/459 [02:37<10:55,  0.56it/s, v_num=0w59, train/loss_step=0.455]Epoch 0:  20%|â–ˆâ–‰        | 90/459 [02:39<10:53,  0.56it/s, v_num=0w59, train/loss_step=0.455]Epoch 0:  20%|â–ˆâ–‰        | 90/459 [02:39<10:53,  0.56it/s, v_num=0w59, train/loss_step=0.446]Epoch 0:  20%|â–ˆâ–‰        | 91/459 [02:40<10:50,  0.57it/s, v_num=0w59, train/loss_step=0.446]Epoch 0:  20%|â–ˆâ–‰        | 91/459 [02:40<10:50,  0.57it/s, v_num=0w59, train/loss_step=0.440]Epoch 0:  20%|â–ˆâ–ˆ        | 92/459 [02:42<10:48,  0.57it/s, v_num=0w59, train/loss_step=0.440]Epoch 0:  20%|â–ˆâ–ˆ        | 92/459 [02:42<10:48,  0.57it/s, v_num=0w59, train/loss_step=0.443]Epoch 0:  20%|â–ˆâ–ˆ        | 93/459 [02:44<10:46,  0.57it/s, v_num=0w59, train/loss_step=0.443]Epoch 0:  20%|â–ˆâ–ˆ        | 93/459 [02:44<10:46,  0.57it/s, v_num=0w59, train/loss_step=0.432]Epoch 0:  20%|â–ˆâ–ˆ        | 94/459 [02:45<10:43,  0.57it/s, v_num=0w59, train/loss_step=0.432]Epoch 0:  20%|â–ˆâ–ˆ        | 94/459 [02:45<10:43,  0.57it/s, v_num=0w59, train/loss_step=0.433]Epoch 0:  21%|â–ˆâ–ˆ        | 95/459 [02:47<10:41,  0.57it/s, v_num=0w59, train/loss_step=0.433]Epoch 0:  21%|â–ˆâ–ˆ        | 95/459 [02:47<10:41,  0.57it/s, v_num=0w59, train/loss_step=0.431]Epoch 0:  21%|â–ˆâ–ˆ        | 96/459 [02:49<10:39,  0.57it/s, v_num=0w59, train/loss_step=0.431]Epoch 0:  21%|â–ˆâ–ˆ        | 96/459 [02:49<10:39,  0.57it/s, v_num=0w59, train/loss_step=0.423]Epoch 0:  21%|â–ˆâ–ˆ        | 97/459 [02:50<10:36,  0.57it/s, v_num=0w59, train/loss_step=0.423]Epoch 0:  21%|â–ˆâ–ˆ        | 97/459 [02:50<10:36,  0.57it/s, v_num=0w59, train/loss_step=0.412]Epoch 0:  21%|â–ˆâ–ˆâ–       | 98/459 [02:52<10:34,  0.57it/s, v_num=0w59, train/loss_step=0.412]Epoch 0:  21%|â–ˆâ–ˆâ–       | 98/459 [02:52<10:34,  0.57it/s, v_num=0w59, train/loss_step=0.416]Epoch 0:  22%|â–ˆâ–ˆâ–       | 99/459 [02:53<10:32,  0.57it/s, v_num=0w59, train/loss_step=0.416]Epoch 0:  22%|â–ˆâ–ˆâ–       | 99/459 [02:53<10:32,  0.57it/s, v_num=0w59, train/loss_step=0.409]Epoch 0:  22%|â–ˆâ–ˆâ–       | 100/459 [02:55<10:29,  0.57it/s, v_num=0w59, train/loss_step=0.409]Epoch 0:  22%|â–ˆâ–ˆâ–       | 100/459 [02:55<10:29,  0.57it/s, v_num=0w59, train/loss_step=0.412]Epoch 0:  22%|â–ˆâ–ˆâ–       | 101/459 [02:57<10:27,  0.57it/s, v_num=0w59, train/loss_step=0.412]Epoch 0:  22%|â–ˆâ–ˆâ–       | 101/459 [02:57<10:27,  0.57it/s, v_num=0w59, train/loss_step=0.399]Epoch 0:  22%|â–ˆâ–ˆâ–       | 102/459 [02:58<10:25,  0.57it/s, v_num=0w59, train/loss_step=0.399]Epoch 0:  22%|â–ˆâ–ˆâ–       | 102/459 [02:58<10:25,  0.57it/s, v_num=0w59, train/loss_step=0.385]Epoch 0:  22%|â–ˆâ–ˆâ–       | 103/459 [03:00<10:23,  0.57it/s, v_num=0w59, train/loss_step=0.385]Epoch 0:  22%|â–ˆâ–ˆâ–       | 103/459 [03:00<10:23,  0.57it/s, v_num=0w59, train/loss_step=0.394]Epoch 0:  23%|â–ˆâ–ˆâ–       | 104/459 [03:01<10:20,  0.57it/s, v_num=0w59, train/loss_step=0.394]Epoch 0:  23%|â–ˆâ–ˆâ–       | 104/459 [03:01<10:20,  0.57it/s, v_num=0w59, train/loss_step=0.387]Epoch 0:  23%|â–ˆâ–ˆâ–       | 105/459 [03:03<10:18,  0.57it/s, v_num=0w59, train/loss_step=0.387]Epoch 0:  23%|â–ˆâ–ˆâ–       | 105/459 [03:03<10:18,  0.57it/s, v_num=0w59, train/loss_step=0.374]Epoch 0:  23%|â–ˆâ–ˆâ–       | 106/459 [03:05<10:16,  0.57it/s, v_num=0w59, train/loss_step=0.374]Epoch 0:  23%|â–ˆâ–ˆâ–       | 106/459 [03:05<10:16,  0.57it/s, v_num=0w59, train/loss_step=0.375]Epoch 0:  23%|â–ˆâ–ˆâ–       | 107/459 [03:06<10:14,  0.57it/s, v_num=0w59, train/loss_step=0.375]Epoch 0:  23%|â–ˆâ–ˆâ–       | 107/459 [03:06<10:14,  0.57it/s, v_num=0w59, train/loss_step=0.376]Epoch 0:  24%|â–ˆâ–ˆâ–       | 108/459 [03:08<10:12,  0.57it/s, v_num=0w59, train/loss_step=0.376]Epoch 0:  24%|â–ˆâ–ˆâ–       | 108/459 [03:08<10:12,  0.57it/s, v_num=0w59, train/loss_step=0.360]Epoch 0:  24%|â–ˆâ–ˆâ–       | 109/459 [03:09<10:09,  0.57it/s, v_num=0w59, train/loss_step=0.360]Epoch 0:  24%|â–ˆâ–ˆâ–       | 109/459 [03:09<10:09,  0.57it/s, v_num=0w59, train/loss_step=0.356]Epoch 0:  24%|â–ˆâ–ˆâ–       | 110/459 [03:11<10:07,  0.57it/s, v_num=0w59, train/loss_step=0.356]Epoch 0:  24%|â–ˆâ–ˆâ–       | 110/459 [03:11<10:07,  0.57it/s, v_num=0w59, train/loss_step=0.345]Epoch 0:  24%|â–ˆâ–ˆâ–       | 111/459 [03:13<10:05,  0.57it/s, v_num=0w59, train/loss_step=0.345]Epoch 0:  24%|â–ˆâ–ˆâ–       | 111/459 [03:13<10:05,  0.57it/s, v_num=0w59, train/loss_step=0.352]Epoch 0:  24%|â–ˆâ–ˆâ–       | 112/459 [03:14<10:03,  0.57it/s, v_num=0w59, train/loss_step=0.352]Epoch 0:  24%|â–ˆâ–ˆâ–       | 112/459 [03:14<10:03,  0.57it/s, v_num=0w59, train/loss_step=0.393]Epoch 0:  25%|â–ˆâ–ˆâ–       | 113/459 [03:16<10:01,  0.58it/s, v_num=0w59, train/loss_step=0.393]Epoch 0:  25%|â–ˆâ–ˆâ–       | 113/459 [03:16<10:01,  0.58it/s, v_num=0w59, train/loss_step=0.336]Epoch 0:  25%|â–ˆâ–ˆâ–       | 114/459 [03:18<09:59,  0.58it/s, v_num=0w59, train/loss_step=0.336]Epoch 0:  25%|â–ˆâ–ˆâ–       | 114/459 [03:18<09:59,  0.58it/s, v_num=0w59, train/loss_step=0.331]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 115/459 [03:19<09:57,  0.58it/s, v_num=0w59, train/loss_step=0.331]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 115/459 [03:19<09:57,  0.58it/s, v_num=0w59, train/loss_step=0.328]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 116/459 [03:21<09:55,  0.58it/s, v_num=0w59, train/loss_step=0.328]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 116/459 [03:21<09:55,  0.58it/s, v_num=0w59, train/loss_step=0.335]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 117/459 [03:22<09:52,  0.58it/s, v_num=0w59, train/loss_step=0.335]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 117/459 [03:22<09:52,  0.58it/s, v_num=0w59, train/loss_step=0.321]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 118/459 [03:24<09:50,  0.58it/s, v_num=0w59, train/loss_step=0.321]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 118/459 [03:24<09:50,  0.58it/s, v_num=0w59, train/loss_step=0.323]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 119/459 [03:26<09:48,  0.58it/s, v_num=0w59, train/loss_step=0.323]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 119/459 [03:26<09:48,  0.58it/s, v_num=0w59, train/loss_step=0.326]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 120/459 [03:27<09:46,  0.58it/s, v_num=0w59, train/loss_step=0.326]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 120/459 [03:27<09:46,  0.58it/s, v_num=0w59, train/loss_step=0.312]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 121/459 [03:29<09:44,  0.58it/s, v_num=0w59, train/loss_step=0.312]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 121/459 [03:29<09:44,  0.58it/s, v_num=0w59, train/loss_step=0.313]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 122/459 [03:30<09:42,  0.58it/s, v_num=0w59, train/loss_step=0.313]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 122/459 [03:30<09:42,  0.58it/s, v_num=0w59, train/loss_step=0.303]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 123/459 [03:32<09:40,  0.58it/s, v_num=0w59, train/loss_step=0.303]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 123/459 [03:32<09:40,  0.58it/s, v_num=0w59, train/loss_step=0.301]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 124/459 [03:34<09:38,  0.58it/s, v_num=0w59, train/loss_step=0.301]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 124/459 [03:34<09:38,  0.58it/s, v_num=0w59, train/loss_step=0.294]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 125/459 [03:35<09:36,  0.58it/s, v_num=0w59, train/loss_step=0.294]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 125/459 [03:35<09:36,  0.58it/s, v_num=0w59, train/loss_step=0.292]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 126/459 [03:37<09:34,  0.58it/s, v_num=0w59, train/loss_step=0.292]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 126/459 [03:37<09:34,  0.58it/s, v_num=0w59, train/loss_step=0.290]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 127/459 [03:38<09:32,  0.58it/s, v_num=0w59, train/loss_step=0.290]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 127/459 [03:38<09:32,  0.58it/s, v_num=0w59, train/loss_step=0.284]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 128/459 [03:40<09:30,  0.58it/s, v_num=0w59, train/loss_step=0.284]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 128/459 [03:40<09:30,  0.58it/s, v_num=0w59, train/loss_step=0.268]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 129/459 [03:42<09:28,  0.58it/s, v_num=0w59, train/loss_step=0.268]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 129/459 [03:42<09:28,  0.58it/s, v_num=0w59, train/loss_step=0.264]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 130/459 [03:43<09:26,  0.58it/s, v_num=0w59, train/loss_step=0.264]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 130/459 [03:43<09:26,  0.58it/s, v_num=0w59, train/loss_step=0.257]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 131/459 [03:45<09:24,  0.58it/s, v_num=0w59, train/loss_step=0.257]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 131/459 [03:45<09:24,  0.58it/s, v_num=0w59, train/loss_step=0.252]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 132/459 [03:46<09:22,  0.58it/s, v_num=0w59, train/loss_step=0.252]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 132/459 [03:46<09:22,  0.58it/s, v_num=0w59, train/loss_step=0.246]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 133/459 [03:48<09:20,  0.58it/s, v_num=0w59, train/loss_step=0.246]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 133/459 [03:48<09:20,  0.58it/s, v_num=0w59, train/loss_step=0.244]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 134/459 [03:50<09:18,  0.58it/s, v_num=0w59, train/loss_step=0.244]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 134/459 [03:50<09:18,  0.58it/s, v_num=0w59, train/loss_step=0.232]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 135/459 [03:51<09:16,  0.58it/s, v_num=0w59, train/loss_step=0.232]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 135/459 [03:51<09:16,  0.58it/s, v_num=0w59, train/loss_step=0.231]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 136/459 [03:53<09:14,  0.58it/s, v_num=0w59, train/loss_step=0.231]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 136/459 [03:53<09:14,  0.58it/s, v_num=0w59, train/loss_step=0.226]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 137/459 [03:54<09:12,  0.58it/s, v_num=0w59, train/loss_step=0.226]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 137/459 [03:54<09:12,  0.58it/s, v_num=0w59, train/loss_step=0.218]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 138/459 [03:56<09:10,  0.58it/s, v_num=0w59, train/loss_step=0.218]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 138/459 [03:56<09:10,  0.58it/s, v_num=0w59, train/loss_step=0.211]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 139/459 [03:58<09:08,  0.58it/s, v_num=0w59, train/loss_step=0.211]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 139/459 [03:58<09:08,  0.58it/s, v_num=0w59, train/loss_step=0.210]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 140/459 [03:59<09:06,  0.58it/s, v_num=0w59, train/loss_step=0.210]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 140/459 [03:59<09:06,  0.58it/s, v_num=0w59, train/loss_step=0.203]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 141/459 [04:01<09:04,  0.58it/s, v_num=0w59, train/loss_step=0.203]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 141/459 [04:01<09:04,  0.58it/s, v_num=0w59, train/loss_step=0.195]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 142/459 [04:02<09:02,  0.58it/s, v_num=0w59, train/loss_step=0.195]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 142/459 [04:02<09:02,  0.58it/s, v_num=0w59, train/loss_step=0.195]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 143/459 [04:04<09:00,  0.58it/s, v_num=0w59, train/loss_step=0.195]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 143/459 [04:04<09:00,  0.58it/s, v_num=0w59, train/loss_step=0.194]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 144/459 [04:06<08:58,  0.58it/s, v_num=0w59, train/loss_step=0.194]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 144/459 [04:06<08:58,  0.58it/s, v_num=0w59, train/loss_step=0.183]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 145/459 [04:07<08:56,  0.59it/s, v_num=0w59, train/loss_step=0.183]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 145/459 [04:07<08:56,  0.59it/s, v_num=0w59, train/loss_step=0.178]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 146/459 [04:09<08:54,  0.59it/s, v_num=0w59, train/loss_step=0.178]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 146/459 [04:09<08:54,  0.59it/s, v_num=0w59, train/loss_step=0.174]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 147/459 [04:10<08:52,  0.59it/s, v_num=0w59, train/loss_step=0.174]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 147/459 [04:10<08:52,  0.59it/s, v_num=0w59, train/loss_step=0.166]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 148/459 [04:12<08:50,  0.59it/s, v_num=0w59, train/loss_step=0.166]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 148/459 [04:12<08:50,  0.59it/s, v_num=0w59, train/loss_step=0.165]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 149/459 [04:14<08:48,  0.59it/s, v_num=0w59, train/loss_step=0.165]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 149/459 [04:14<08:48,  0.59it/s, v_num=0w59, train/loss_step=0.164]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 150/459 [04:15<08:46,  0.59it/s, v_num=0w59, train/loss_step=0.164]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 150/459 [04:15<08:46,  0.59it/s, v_num=0w59, train/loss_step=0.158]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 151/459 [04:17<08:45,  0.59it/s, v_num=0w59, train/loss_step=0.158]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 151/459 [04:17<08:45,  0.59it/s, v_num=0w59, train/loss_step=0.153]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 152/459 [04:19<08:43,  0.59it/s, v_num=0w59, train/loss_step=0.153]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 152/459 [04:19<08:43,  0.59it/s, v_num=0w59, train/loss_step=0.144]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 153/459 [04:20<08:41,  0.59it/s, v_num=0w59, train/loss_step=0.144]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 153/459 [04:20<08:41,  0.59it/s, v_num=0w59, train/loss_step=0.143]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 154/459 [04:22<08:39,  0.59it/s, v_num=0w59, train/loss_step=0.143]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 154/459 [04:22<08:39,  0.59it/s, v_num=0w59, train/loss_step=0.141]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 155/459 [04:23<08:37,  0.59it/s, v_num=0w59, train/loss_step=0.141]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 155/459 [04:23<08:37,  0.59it/s, v_num=0w59, train/loss_step=0.135]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 156/459 [04:25<08:35,  0.59it/s, v_num=0w59, train/loss_step=0.135]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 156/459 [04:25<08:35,  0.59it/s, v_num=0w59, train/loss_step=0.133]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 157/459 [04:26<08:33,  0.59it/s, v_num=0w59, train/loss_step=0.133]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 157/459 [04:26<08:33,  0.59it/s, v_num=0w59, train/loss_step=0.135]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 158/459 [04:28<08:31,  0.59it/s, v_num=0w59, train/loss_step=0.135]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 158/459 [04:28<08:31,  0.59it/s, v_num=0w59, train/loss_step=0.130]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 159/459 [04:30<08:29,  0.59it/s, v_num=0w59, train/loss_step=0.130]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 159/459 [04:30<08:29,  0.59it/s, v_num=0w59, train/loss_step=0.121]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 160/459 [04:31<08:27,  0.59it/s, v_num=0w59, train/loss_step=0.121]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 160/459 [04:31<08:27,  0.59it/s, v_num=0w59, train/loss_step=0.121]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 161/459 [04:33<08:25,  0.59it/s, v_num=0w59, train/loss_step=0.121]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 161/459 [04:33<08:25,  0.59it/s, v_num=0w59, train/loss_step=0.115]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 162/459 [04:34<08:23,  0.59it/s, v_num=0w59, train/loss_step=0.115]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 162/459 [04:34<08:23,  0.59it/s, v_num=0w59, train/loss_step=0.116]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 163/459 [04:36<08:22,  0.59it/s, v_num=0w59, train/loss_step=0.116]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 163/459 [04:36<08:22,  0.59it/s, v_num=0w59, train/loss_step=0.117]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 164/459 [04:38<08:20,  0.59it/s, v_num=0w59, train/loss_step=0.117]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 164/459 [04:38<08:20,  0.59it/s, v_num=0w59, train/loss_step=0.112]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 165/459 [04:39<08:18,  0.59it/s, v_num=0w59, train/loss_step=0.112]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 165/459 [04:39<08:18,  0.59it/s, v_num=0w59, train/loss_step=0.111]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 166/459 [04:41<08:16,  0.59it/s, v_num=0w59, train/loss_step=0.111]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 166/459 [04:41<08:16,  0.59it/s, v_num=0w59, train/loss_step=0.113]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 167/459 [04:42<08:14,  0.59it/s, v_num=0w59, train/loss_step=0.113]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 167/459 [04:42<08:14,  0.59it/s, v_num=0w59, train/loss_step=0.113]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 168/459 [04:44<08:12,  0.59it/s, v_num=0w59, train/loss_step=0.113]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 168/459 [04:44<08:12,  0.59it/s, v_num=0w59, train/loss_step=0.101]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 169/459 [04:46<08:10,  0.59it/s, v_num=0w59, train/loss_step=0.101]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 169/459 [04:46<08:10,  0.59it/s, v_num=0w59, train/loss_step=0.0985]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 170/459 [04:47<08:08,  0.59it/s, v_num=0w59, train/loss_step=0.0985]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 170/459 [04:47<08:08,  0.59it/s, v_num=0w59, train/loss_step=0.0979]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 171/459 [04:49<08:07,  0.59it/s, v_num=0w59, train/loss_step=0.0979]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 171/459 [04:49<08:07,  0.59it/s, v_num=0w59, train/loss_step=0.0916]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 172/459 [04:50<08:05,  0.59it/s, v_num=0w59, train/loss_step=0.0916]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 172/459 [04:50<08:05,  0.59it/s, v_num=0w59, train/loss_step=0.0963]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 173/459 [04:52<08:03,  0.59it/s, v_num=0w59, train/loss_step=0.0963]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 173/459 [04:52<08:03,  0.59it/s, v_num=0w59, train/loss_step=0.0899]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 174/459 [04:53<08:01,  0.59it/s, v_num=0w59, train/loss_step=0.0899]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 174/459 [04:53<08:01,  0.59it/s, v_num=0w59, train/loss_step=0.0845]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 175/459 [04:55<07:59,  0.59it/s, v_num=0w59, train/loss_step=0.0845]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 175/459 [04:55<07:59,  0.59it/s, v_num=0w59, train/loss_step=0.0912]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 176/459 [04:57<07:57,  0.59it/s, v_num=0w59, train/loss_step=0.0912]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 176/459 [04:57<07:57,  0.59it/s, v_num=0w59, train/loss_step=0.0812]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 177/459 [04:58<07:55,  0.59it/s, v_num=0w59, train/loss_step=0.0812]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 177/459 [04:58<07:55,  0.59it/s, v_num=0w59, train/loss_step=0.081] Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 178/459 [05:00<07:54,  0.59it/s, v_num=0w59, train/loss_step=0.081]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 178/459 [05:00<07:54,  0.59it/s, v_num=0w59, train/loss_step=0.0823]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 179/459 [05:01<07:52,  0.59it/s, v_num=0w59, train/loss_step=0.0823]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 179/459 [05:01<07:52,  0.59it/s, v_num=0w59, train/loss_step=0.0787]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 180/459 [05:03<07:50,  0.59it/s, v_num=0w59, train/loss_step=0.0787]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 180/459 [05:03<07:50,  0.59it/s, v_num=0w59, train/loss_step=0.0774]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 181/459 [05:05<07:48,  0.59it/s, v_num=0w59, train/loss_step=0.0774]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 181/459 [05:05<07:48,  0.59it/s, v_num=0w59, train/loss_step=0.0765]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 182/459 [05:06<07:46,  0.59it/s, v_num=0w59, train/loss_step=0.0765]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 182/459 [05:06<07:46,  0.59it/s, v_num=0w59, train/loss_step=0.0752]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 183/459 [05:08<07:44,  0.59it/s, v_num=0w59, train/loss_step=0.0752]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 183/459 [05:08<07:44,  0.59it/s, v_num=0w59, train/loss_step=0.0782]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 184/459 [05:09<07:43,  0.59it/s, v_num=0w59, train/loss_step=0.0782]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 184/459 [05:09<07:43,  0.59it/s, v_num=0w59, train/loss_step=0.0711]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 185/459 [05:11<07:41,  0.59it/s, v_num=0w59, train/loss_step=0.0711]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 185/459 [05:11<07:41,  0.59it/s, v_num=0w59, train/loss_step=0.0695]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 186/459 [05:12<07:39,  0.59it/s, v_num=0w59, train/loss_step=0.0695]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 186/459 [05:12<07:39,  0.59it/s, v_num=0w59, train/loss_step=0.0661]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 187/459 [05:14<07:37,  0.59it/s, v_num=0w59, train/loss_step=0.0661]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 187/459 [05:14<07:37,  0.59it/s, v_num=0w59, train/loss_step=0.0697]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 188/459 [05:16<07:35,  0.59it/s, v_num=0w59, train/loss_step=0.0697]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 188/459 [05:16<07:35,  0.59it/s, v_num=0w59, train/loss_step=0.0707]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 189/459 [05:17<07:33,  0.59it/s, v_num=0w59, train/loss_step=0.0707]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 189/459 [05:17<07:33,  0.59it/s, v_num=0w59, train/loss_step=0.0679]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 190/459 [05:19<07:31,  0.60it/s, v_num=0w59, train/loss_step=0.0679]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 190/459 [05:19<07:31,  0.60it/s, v_num=0w59, train/loss_step=0.0665]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 191/459 [05:20<07:30,  0.60it/s, v_num=0w59, train/loss_step=0.0665]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 191/459 [05:20<07:30,  0.60it/s, v_num=0w59, train/loss_step=0.0678]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 192/459 [05:22<07:28,  0.60it/s, v_num=0w59, train/loss_step=0.0678]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 192/459 [05:22<07:28,  0.60it/s, v_num=0w59, train/loss_step=0.0679]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/459 [05:23<07:26,  0.60it/s, v_num=0w59, train/loss_step=0.0679]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/459 [05:23<07:26,  0.60it/s, v_num=0w59, train/loss_step=0.0664]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 194/459 [05:25<07:24,  0.60it/s, v_num=0w59, train/loss_step=0.0664]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 194/459 [05:25<07:24,  0.60it/s, v_num=0w59, train/loss_step=0.0623]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 195/459 [05:27<07:22,  0.60it/s, v_num=0w59, train/loss_step=0.0623]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 195/459 [05:27<07:22,  0.60it/s, v_num=0w59, train/loss_step=0.0642]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 196/459 [05:28<07:21,  0.60it/s, v_num=0w59, train/loss_step=0.0642]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 196/459 [05:28<07:21,  0.60it/s, v_num=0w59, train/loss_step=0.0716]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 197/459 [05:30<07:19,  0.60it/s, v_num=0w59, train/loss_step=0.0716]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 197/459 [05:30<07:19,  0.60it/s, v_num=0w59, train/loss_step=0.064] Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 198/459 [05:32<07:17,  0.60it/s, v_num=0w59, train/loss_step=0.064]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 198/459 [05:32<07:17,  0.60it/s, v_num=0w59, train/loss_step=0.0641]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 199/459 [05:33<07:15,  0.60it/s, v_num=0w59, train/loss_step=0.0641]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 199/459 [05:33<07:15,  0.60it/s, v_num=0w59, train/loss_step=0.0611]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200/459 [05:35<07:14,  0.60it/s, v_num=0w59, train/loss_step=0.0611]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200/459 [05:35<07:14,  0.60it/s, v_num=0w59, train/loss_step=0.0659]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 201/459 [05:36<07:12,  0.60it/s, v_num=0w59, train/loss_step=0.0659]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 201/459 [05:36<07:12,  0.60it/s, v_num=0w59, train/loss_step=0.0628]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 202/459 [05:38<07:10,  0.60it/s, v_num=0w59, train/loss_step=0.0628]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 202/459 [05:38<07:10,  0.60it/s, v_num=0w59, train/loss_step=0.0648]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 203/459 [05:39<07:08,  0.60it/s, v_num=0w59, train/loss_step=0.0648]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 203/459 [05:39<07:08,  0.60it/s, v_num=0w59, train/loss_step=0.0631]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 204/459 [05:41<07:06,  0.60it/s, v_num=0w59, train/loss_step=0.0631]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 204/459 [05:41<07:06,  0.60it/s, v_num=0w59, train/loss_step=0.0613]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 205/459 [05:43<07:05,  0.60it/s, v_num=0w59, train/loss_step=0.0613]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 205/459 [05:43<07:05,  0.60it/s, v_num=0w59, train/loss_step=0.0639]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 206/459 [05:44<07:03,  0.60it/s, v_num=0w59, train/loss_step=0.0639]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 206/459 [05:44<07:03,  0.60it/s, v_num=0w59, train/loss_step=0.0575]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 207/459 [05:46<07:01,  0.60it/s, v_num=0w59, train/loss_step=0.0575]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 207/459 [05:46<07:01,  0.60it/s, v_num=0w59, train/loss_step=0.0594]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 208/459 [05:47<06:59,  0.60it/s, v_num=0w59, train/loss_step=0.0594]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 208/459 [05:47<06:59,  0.60it/s, v_num=0w59, train/loss_step=0.063] Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 209/459 [05:49<06:58,  0.60it/s, v_num=0w59, train/loss_step=0.063]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 209/459 [05:49<06:58,  0.60it/s, v_num=0w59, train/loss_step=0.0575]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 210/459 [05:51<06:56,  0.60it/s, v_num=0w59, train/loss_step=0.0575]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 210/459 [05:51<06:56,  0.60it/s, v_num=0w59, train/loss_step=0.0555]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 211/459 [05:52<06:54,  0.60it/s, v_num=0w59, train/loss_step=0.0555]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 211/459 [05:52<06:54,  0.60it/s, v_num=0w59, train/loss_step=0.0607]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 212/459 [05:54<06:52,  0.60it/s, v_num=0w59, train/loss_step=0.0607]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 212/459 [05:54<06:52,  0.60it/s, v_num=0w59, train/loss_step=0.0619]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 213/459 [05:55<06:51,  0.60it/s, v_num=0w59, train/loss_step=0.0619]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 213/459 [05:55<06:51,  0.60it/s, v_num=0w59, train/loss_step=0.063] Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 214/459 [05:57<06:49,  0.60it/s, v_num=0w59, train/loss_step=0.063]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 214/459 [05:57<06:49,  0.60it/s, v_num=0w59, train/loss_step=0.0623]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 215/459 [05:59<06:47,  0.60it/s, v_num=0w59, train/loss_step=0.0623]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 215/459 [05:59<06:47,  0.60it/s, v_num=0w59, train/loss_step=0.0586]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 216/459 [06:00<06:45,  0.60it/s, v_num=0w59, train/loss_step=0.0586]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 216/459 [06:00<06:45,  0.60it/s, v_num=0w59, train/loss_step=0.0597]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 217/459 [06:02<06:43,  0.60it/s, v_num=0w59, train/loss_step=0.0597]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 217/459 [06:02<06:43,  0.60it/s, v_num=0w59, train/loss_step=0.0578]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 218/459 [06:03<06:42,  0.60it/s, v_num=0w59, train/loss_step=0.0578]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 218/459 [06:03<06:42,  0.60it/s, v_num=0w59, train/loss_step=0.0566]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 219/459 [06:05<06:40,  0.60it/s, v_num=0w59, train/loss_step=0.0566]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 219/459 [06:05<06:40,  0.60it/s, v_num=0w59, train/loss_step=0.0561]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 220/459 [06:06<06:38,  0.60it/s, v_num=0w59, train/loss_step=0.0561]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 220/459 [06:06<06:38,  0.60it/s, v_num=0w59, train/loss_step=0.0586]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 221/459 [06:08<06:36,  0.60it/s, v_num=0w59, train/loss_step=0.0586]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 221/459 [06:08<06:36,  0.60it/s, v_num=0w59, train/loss_step=0.0581]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 222/459 [06:10<06:35,  0.60it/s, v_num=0w59, train/loss_step=0.0581]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 222/459 [06:10<06:35,  0.60it/s, v_num=0w59, train/loss_step=0.0538]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 223/459 [06:11<06:33,  0.60it/s, v_num=0w59, train/loss_step=0.0538]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 223/459 [06:11<06:33,  0.60it/s, v_num=0w59, train/loss_step=0.0541]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 224/459 [06:13<06:31,  0.60it/s, v_num=0w59, train/loss_step=0.0541]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 224/459 [06:13<06:31,  0.60it/s, v_num=0w59, train/loss_step=0.0555]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 225/459 [06:14<06:29,  0.60it/s, v_num=0w59, train/loss_step=0.0555]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 225/459 [06:14<06:29,  0.60it/s, v_num=0w59, train/loss_step=0.0607]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 226/459 [06:16<06:28,  0.60it/s, v_num=0w59, train/loss_step=0.0607]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 226/459 [06:16<06:28,  0.60it/s, v_num=0w59, train/loss_step=0.0581]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 227/459 [06:17<06:26,  0.60it/s, v_num=0w59, train/loss_step=0.0581]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 227/459 [06:17<06:26,  0.60it/s, v_num=0w59, train/loss_step=0.0557]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 228/459 [06:19<06:24,  0.60it/s, v_num=0w59, train/loss_step=0.0557]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 228/459 [06:19<06:24,  0.60it/s, v_num=0w59, train/loss_step=0.0553]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 229/459 [06:21<06:22,  0.60it/s, v_num=0w59, train/loss_step=0.0553]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 229/459 [06:21<06:22,  0.60it/s, v_num=0w59, train/loss_step=0.055] Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 230/459 [06:22<06:21,  0.60it/s, v_num=0w59, train/loss_step=0.055]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 230/459 [06:22<06:21,  0.60it/s, v_num=0w59, train/loss_step=0.0614]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 231/459 [06:24<06:19,  0.60it/s, v_num=0w59, train/loss_step=0.0614]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 231/459 [06:24<06:19,  0.60it/s, v_num=0w59, train/loss_step=0.0526]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 232/459 [06:25<06:17,  0.60it/s, v_num=0w59, train/loss_step=0.0526]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 232/459 [06:25<06:17,  0.60it/s, v_num=0w59, train/loss_step=0.0575]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 233/459 [06:27<06:15,  0.60it/s, v_num=0w59, train/loss_step=0.0575]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 233/459 [06:27<06:15,  0.60it/s, v_num=0w59, train/loss_step=0.0544]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 234/459 [06:29<06:14,  0.60it/s, v_num=0w59, train/loss_step=0.0544]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 234/459 [06:29<06:14,  0.60it/s, v_num=0w59, train/loss_step=0.0562]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 235/459 [06:30<06:12,  0.60it/s, v_num=0w59, train/loss_step=0.0562]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 235/459 [06:30<06:12,  0.60it/s, v_num=0w59, train/loss_step=0.056] Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 236/459 [06:32<06:10,  0.60it/s, v_num=0w59, train/loss_step=0.056]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 236/459 [06:32<06:10,  0.60it/s, v_num=0w59, train/loss_step=0.0565]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 237/459 [06:33<06:08,  0.60it/s, v_num=0w59, train/loss_step=0.0565]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 237/459 [06:33<06:08,  0.60it/s, v_num=0w59, train/loss_step=0.0549]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/459 [06:35<06:07,  0.60it/s, v_num=0w59, train/loss_step=0.0549]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/459 [06:35<06:07,  0.60it/s, v_num=0w59, train/loss_step=0.052] Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 239/459 [06:36<06:05,  0.60it/s, v_num=0w59, train/loss_step=0.052]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 239/459 [06:36<06:05,  0.60it/s, v_num=0w59, train/loss_step=0.0523]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 240/459 [06:38<06:03,  0.60it/s, v_num=0w59, train/loss_step=0.0523]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 240/459 [06:38<06:03,  0.60it/s, v_num=0w59, train/loss_step=0.0536]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 241/459 [06:40<06:01,  0.60it/s, v_num=0w59, train/loss_step=0.0536]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 241/459 [06:40<06:01,  0.60it/s, v_num=0w59, train/loss_step=0.0587]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 242/459 [06:41<06:00,  0.60it/s, v_num=0w59, train/loss_step=0.0587]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 242/459 [06:41<06:00,  0.60it/s, v_num=0w59, train/loss_step=0.059] Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 243/459 [06:43<05:58,  0.60it/s, v_num=0w59, train/loss_step=0.059]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 243/459 [06:43<05:58,  0.60it/s, v_num=0w59, train/loss_step=0.0517]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 244/459 [06:44<05:56,  0.60it/s, v_num=0w59, train/loss_step=0.0517]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 244/459 [06:44<05:56,  0.60it/s, v_num=0w59, train/loss_step=0.0527]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 245/459 [06:46<05:54,  0.60it/s, v_num=0w59, train/loss_step=0.0527]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 245/459 [06:46<05:54,  0.60it/s, v_num=0w59, train/loss_step=0.0584]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 246/459 [06:47<05:53,  0.60it/s, v_num=0w59, train/loss_step=0.0584]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 246/459 [06:47<05:53,  0.60it/s, v_num=0w59, train/loss_step=0.0538]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 247/459 [06:49<05:51,  0.60it/s, v_num=0w59, train/loss_step=0.0538]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 247/459 [06:49<05:51,  0.60it/s, v_num=0w59, train/loss_step=0.0503]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248/459 [06:51<05:49,  0.60it/s, v_num=0w59, train/loss_step=0.0503]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248/459 [06:51<05:49,  0.60it/s, v_num=0w59, train/loss_step=0.0528]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 249/459 [06:52<05:48,  0.60it/s, v_num=0w59, train/loss_step=0.0528]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 249/459 [06:52<05:48,  0.60it/s, v_num=0w59, train/loss_step=0.0551]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 250/459 [06:54<05:46,  0.60it/s, v_num=0w59, train/loss_step=0.0551]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 250/459 [06:54<05:46,  0.60it/s, v_num=0w59, train/loss_step=0.058] Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 251/459 [06:55<05:44,  0.60it/s, v_num=0w59, train/loss_step=0.058]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 251/459 [06:55<05:44,  0.60it/s, v_num=0w59, train/loss_step=0.0561]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 252/459 [06:57<05:42,  0.60it/s, v_num=0w59, train/loss_step=0.0561]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 252/459 [06:57<05:42,  0.60it/s, v_num=0w59, train/loss_step=0.0546]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 253/459 [06:58<05:41,  0.60it/s, v_num=0w59, train/loss_step=0.0546]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 253/459 [06:58<05:41,  0.60it/s, v_num=0w59, train/loss_step=0.0523]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 254/459 [07:00<05:39,  0.60it/s, v_num=0w59, train/loss_step=0.0523]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 254/459 [07:00<05:39,  0.60it/s, v_num=0w59, train/loss_step=0.0546]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 255/459 [07:02<05:37,  0.60it/s, v_num=0w59, train/loss_step=0.0546]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 255/459 [07:02<05:37,  0.60it/s, v_num=0w59, train/loss_step=0.053] Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 256/459 [07:03<05:36,  0.60it/s, v_num=0w59, train/loss_step=0.053]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 256/459 [07:03<05:36,  0.60it/s, v_num=0w59, train/loss_step=0.0529]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 257/459 [07:05<05:34,  0.60it/s, v_num=0w59, train/loss_step=0.0529]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 257/459 [07:05<05:34,  0.60it/s, v_num=0w59, train/loss_step=0.0528]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 258/459 [07:06<05:32,  0.60it/s, v_num=0w59, train/loss_step=0.0528]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 258/459 [07:06<05:32,  0.60it/s, v_num=0w59, train/loss_step=0.0539]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 259/459 [07:08<05:30,  0.60it/s, v_num=0w59, train/loss_step=0.0539]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 259/459 [07:08<05:30,  0.60it/s, v_num=0w59, train/loss_step=0.057] Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 260/459 [07:10<05:29,  0.60it/s, v_num=0w59, train/loss_step=0.057]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 260/459 [07:10<05:29,  0.60it/s, v_num=0w59, train/loss_step=0.0579]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 261/459 [07:11<05:27,  0.60it/s, v_num=0w59, train/loss_step=0.0579]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 261/459 [07:11<05:27,  0.60it/s, v_num=0w59, train/loss_step=0.0514]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 262/459 [07:13<05:25,  0.60it/s, v_num=0w59, train/loss_step=0.0514]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 262/459 [07:13<05:25,  0.60it/s, v_num=0w59, train/loss_step=0.0502]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 263/459 [07:14<05:24,  0.60it/s, v_num=0w59, train/loss_step=0.0502]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 263/459 [07:14<05:24,  0.60it/s, v_num=0w59, train/loss_step=0.0502]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 264/459 [07:16<05:22,  0.60it/s, v_num=0w59, train/loss_step=0.0502]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 264/459 [07:16<05:22,  0.60it/s, v_num=0w59, train/loss_step=0.053] Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 265/459 [07:18<05:20,  0.60it/s, v_num=0w59, train/loss_step=0.053]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 265/459 [07:18<05:20,  0.60it/s, v_num=0w59, train/loss_step=0.052]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 266/459 [07:19<05:18,  0.61it/s, v_num=0w59, train/loss_step=0.052]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 266/459 [07:19<05:18,  0.61it/s, v_num=0w59, train/loss_step=0.0472]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 267/459 [07:21<05:17,  0.61it/s, v_num=0w59, train/loss_step=0.0472]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 267/459 [07:21<05:17,  0.61it/s, v_num=0w59, train/loss_step=0.0506]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 268/459 [07:22<05:15,  0.61it/s, v_num=0w59, train/loss_step=0.0506]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 268/459 [07:22<05:15,  0.61it/s, v_num=0w59, train/loss_step=0.048] Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 269/459 [07:24<05:13,  0.61it/s, v_num=0w59, train/loss_step=0.048]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 269/459 [07:24<05:13,  0.61it/s, v_num=0w59, train/loss_step=0.0477]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 270/459 [07:25<05:12,  0.61it/s, v_num=0w59, train/loss_step=0.0477]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 270/459 [07:25<05:12,  0.61it/s, v_num=0w59, train/loss_step=0.0525]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 271/459 [07:27<05:10,  0.61it/s, v_num=0w59, train/loss_step=0.0525]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 271/459 [07:27<05:10,  0.61it/s, v_num=0w59, train/loss_step=0.0526]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 272/459 [07:29<05:08,  0.61it/s, v_num=0w59, train/loss_step=0.0526]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 272/459 [07:29<05:08,  0.61it/s, v_num=0w59, train/loss_step=0.0532]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 273/459 [07:30<05:07,  0.61it/s, v_num=0w59, train/loss_step=0.0532]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 273/459 [07:30<05:07,  0.61it/s, v_num=0w59, train/loss_step=0.054] Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 274/459 [07:32<05:05,  0.61it/s, v_num=0w59, train/loss_step=0.054]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 274/459 [07:32<05:05,  0.61it/s, v_num=0w59, train/loss_step=0.0496]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 275/459 [07:33<05:03,  0.61it/s, v_num=0w59, train/loss_step=0.0496]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 275/459 [07:33<05:03,  0.61it/s, v_num=0w59, train/loss_step=0.0518]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 276/459 [07:35<05:01,  0.61it/s, v_num=0w59, train/loss_step=0.0518]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 276/459 [07:35<05:01,  0.61it/s, v_num=0w59, train/loss_step=0.0499]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 277/459 [07:37<05:00,  0.61it/s, v_num=0w59, train/loss_step=0.0499]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 277/459 [07:37<05:00,  0.61it/s, v_num=0w59, train/loss_step=0.0536]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 278/459 [07:38<04:58,  0.61it/s, v_num=0w59, train/loss_step=0.0536]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 278/459 [07:38<04:58,  0.61it/s, v_num=0w59, train/loss_step=0.0509]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 279/459 [07:40<04:56,  0.61it/s, v_num=0w59, train/loss_step=0.0509]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 279/459 [07:40<04:56,  0.61it/s, v_num=0w59, train/loss_step=0.0502]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 280/459 [07:41<04:55,  0.61it/s, v_num=0w59, train/loss_step=0.0502]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 280/459 [07:41<04:55,  0.61it/s, v_num=0w59, train/loss_step=0.0499]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 281/459 [07:43<04:53,  0.61it/s, v_num=0w59, train/loss_step=0.0499]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 281/459 [07:43<04:53,  0.61it/s, v_num=0w59, train/loss_step=0.051] Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 282/459 [07:45<04:51,  0.61it/s, v_num=0w59, train/loss_step=0.051]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 282/459 [07:45<04:51,  0.61it/s, v_num=0w59, train/loss_step=0.0514]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 283/459 [07:46<04:50,  0.61it/s, v_num=0w59, train/loss_step=0.0514]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 283/459 [07:46<04:50,  0.61it/s, v_num=0w59, train/loss_step=0.0479]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 284/459 [07:48<04:48,  0.61it/s, v_num=0w59, train/loss_step=0.0479]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 284/459 [07:48<04:48,  0.61it/s, v_num=0w59, train/loss_step=0.052] Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 285/459 [07:49<04:46,  0.61it/s, v_num=0w59, train/loss_step=0.052]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 285/459 [07:49<04:46,  0.61it/s, v_num=0w59, train/loss_step=0.0507]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 286/459 [07:51<04:45,  0.61it/s, v_num=0w59, train/loss_step=0.0507]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 286/459 [07:51<04:45,  0.61it/s, v_num=0w59, train/loss_step=0.0518]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 287/459 [07:53<04:43,  0.61it/s, v_num=0w59, train/loss_step=0.0518]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 287/459 [07:53<04:43,  0.61it/s, v_num=0w59, train/loss_step=0.0556]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 288/459 [07:54<04:41,  0.61it/s, v_num=0w59, train/loss_step=0.0556]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 288/459 [07:54<04:41,  0.61it/s, v_num=0w59, train/loss_step=0.0495]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 289/459 [07:56<04:40,  0.61it/s, v_num=0w59, train/loss_step=0.0495]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 289/459 [07:56<04:40,  0.61it/s, v_num=0w59, train/loss_step=0.0479]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 290/459 [07:57<04:38,  0.61it/s, v_num=0w59, train/loss_step=0.0479]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 290/459 [07:57<04:38,  0.61it/s, v_num=0w59, train/loss_step=0.0535]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 291/459 [07:59<04:36,  0.61it/s, v_num=0w59, train/loss_step=0.0535]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 291/459 [07:59<04:36,  0.61it/s, v_num=0w59, train/loss_step=0.0496]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 292/459 [08:01<04:35,  0.61it/s, v_num=0w59, train/loss_step=0.0496]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 292/459 [08:01<04:35,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 293/459 [08:02<04:33,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 293/459 [08:02<04:33,  0.61it/s, v_num=0w59, train/loss_step=0.0521]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 294/459 [08:04<04:31,  0.61it/s, v_num=0w59, train/loss_step=0.0521]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 294/459 [08:04<04:31,  0.61it/s, v_num=0w59, train/loss_step=0.0508]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 295/459 [08:05<04:30,  0.61it/s, v_num=0w59, train/loss_step=0.0508]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 295/459 [08:05<04:30,  0.61it/s, v_num=0w59, train/loss_step=0.0522]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 296/459 [08:07<04:28,  0.61it/s, v_num=0w59, train/loss_step=0.0522]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 296/459 [08:07<04:28,  0.61it/s, v_num=0w59, train/loss_step=0.0495]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 297/459 [08:08<04:26,  0.61it/s, v_num=0w59, train/loss_step=0.0495]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 297/459 [08:08<04:26,  0.61it/s, v_num=0w59, train/loss_step=0.0472]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 298/459 [08:10<04:25,  0.61it/s, v_num=0w59, train/loss_step=0.0472]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 298/459 [08:10<04:25,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 299/459 [08:12<04:23,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 299/459 [08:12<04:23,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 300/459 [08:13<04:21,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 300/459 [08:13<04:21,  0.61it/s, v_num=0w59, train/loss_step=0.0517]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 301/459 [08:15<04:20,  0.61it/s, v_num=0w59, train/loss_step=0.0517]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 301/459 [08:15<04:20,  0.61it/s, v_num=0w59, train/loss_step=0.0496]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 302/459 [08:16<04:18,  0.61it/s, v_num=0w59, train/loss_step=0.0496]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 302/459 [08:16<04:18,  0.61it/s, v_num=0w59, train/loss_step=0.0525]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 303/459 [08:18<04:16,  0.61it/s, v_num=0w59, train/loss_step=0.0525]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 303/459 [08:18<04:16,  0.61it/s, v_num=0w59, train/loss_step=0.0493]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 304/459 [08:20<04:15,  0.61it/s, v_num=0w59, train/loss_step=0.0493]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 304/459 [08:20<04:15,  0.61it/s, v_num=0w59, train/loss_step=0.0517]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 305/459 [08:21<04:13,  0.61it/s, v_num=0w59, train/loss_step=0.0517]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 305/459 [08:21<04:13,  0.61it/s, v_num=0w59, train/loss_step=0.0459]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 306/459 [08:23<04:11,  0.61it/s, v_num=0w59, train/loss_step=0.0459]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 306/459 [08:23<04:11,  0.61it/s, v_num=0w59, train/loss_step=0.0504]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 307/459 [08:24<04:10,  0.61it/s, v_num=0w59, train/loss_step=0.0504]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 307/459 [08:24<04:10,  0.61it/s, v_num=0w59, train/loss_step=0.0493]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 308/459 [08:26<04:08,  0.61it/s, v_num=0w59, train/loss_step=0.0493]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 308/459 [08:26<04:08,  0.61it/s, v_num=0w59, train/loss_step=0.051] Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 309/459 [08:28<04:06,  0.61it/s, v_num=0w59, train/loss_step=0.051]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 309/459 [08:28<04:06,  0.61it/s, v_num=0w59, train/loss_step=0.0508]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 310/459 [08:29<04:05,  0.61it/s, v_num=0w59, train/loss_step=0.0508]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 310/459 [08:29<04:05,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 311/459 [08:31<04:03,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 311/459 [08:31<04:03,  0.61it/s, v_num=0w59, train/loss_step=0.0455]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 312/459 [08:32<04:01,  0.61it/s, v_num=0w59, train/loss_step=0.0455]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 312/459 [08:32<04:01,  0.61it/s, v_num=0w59, train/loss_step=0.046] Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 313/459 [08:34<04:00,  0.61it/s, v_num=0w59, train/loss_step=0.046]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 313/459 [08:34<04:00,  0.61it/s, v_num=0w59, train/loss_step=0.0491]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 314/459 [08:36<03:58,  0.61it/s, v_num=0w59, train/loss_step=0.0491]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 314/459 [08:36<03:58,  0.61it/s, v_num=0w59, train/loss_step=0.0478]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 315/459 [08:37<03:56,  0.61it/s, v_num=0w59, train/loss_step=0.0478]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 315/459 [08:37<03:56,  0.61it/s, v_num=0w59, train/loss_step=0.054] Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 316/459 [08:39<03:55,  0.61it/s, v_num=0w59, train/loss_step=0.054]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 316/459 [08:39<03:55,  0.61it/s, v_num=0w59, train/loss_step=0.0484]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 317/459 [08:40<03:53,  0.61it/s, v_num=0w59, train/loss_step=0.0484]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 317/459 [08:40<03:53,  0.61it/s, v_num=0w59, train/loss_step=0.0514]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 318/459 [08:42<03:51,  0.61it/s, v_num=0w59, train/loss_step=0.0514]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 318/459 [08:42<03:51,  0.61it/s, v_num=0w59, train/loss_step=0.0459]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 319/459 [08:44<03:50,  0.61it/s, v_num=0w59, train/loss_step=0.0459]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 319/459 [08:44<03:50,  0.61it/s, v_num=0w59, train/loss_step=0.0466]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 320/459 [08:45<03:48,  0.61it/s, v_num=0w59, train/loss_step=0.0466]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 320/459 [08:45<03:48,  0.61it/s, v_num=0w59, train/loss_step=0.0455]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 321/459 [08:47<03:46,  0.61it/s, v_num=0w59, train/loss_step=0.0455]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 321/459 [08:47<03:46,  0.61it/s, v_num=0w59, train/loss_step=0.0422]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 322/459 [08:48<03:45,  0.61it/s, v_num=0w59, train/loss_step=0.0422]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 322/459 [08:48<03:45,  0.61it/s, v_num=0w59, train/loss_step=0.0479]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 323/459 [08:50<03:43,  0.61it/s, v_num=0w59, train/loss_step=0.0479]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 323/459 [08:50<03:43,  0.61it/s, v_num=0w59, train/loss_step=0.0451]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 324/459 [08:52<03:41,  0.61it/s, v_num=0w59, train/loss_step=0.0451]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 324/459 [08:52<03:41,  0.61it/s, v_num=0w59, train/loss_step=0.0452]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 325/459 [08:53<03:40,  0.61it/s, v_num=0w59, train/loss_step=0.0452]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 325/459 [08:53<03:40,  0.61it/s, v_num=0w59, train/loss_step=0.0442]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 326/459 [08:55<03:38,  0.61it/s, v_num=0w59, train/loss_step=0.0442]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 326/459 [08:55<03:38,  0.61it/s, v_num=0w59, train/loss_step=0.0475]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 327/459 [08:56<03:36,  0.61it/s, v_num=0w59, train/loss_step=0.0475]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 327/459 [08:56<03:36,  0.61it/s, v_num=0w59, train/loss_step=0.0474]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 328/459 [08:58<03:35,  0.61it/s, v_num=0w59, train/loss_step=0.0474]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 328/459 [08:58<03:35,  0.61it/s, v_num=0w59, train/loss_step=0.0469]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 329/459 [09:00<03:33,  0.61it/s, v_num=0w59, train/loss_step=0.0469]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 329/459 [09:00<03:33,  0.61it/s, v_num=0w59, train/loss_step=0.0528]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 330/459 [09:01<03:31,  0.61it/s, v_num=0w59, train/loss_step=0.0528]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 330/459 [09:01<03:31,  0.61it/s, v_num=0w59, train/loss_step=0.0494]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 331/459 [09:03<03:30,  0.61it/s, v_num=0w59, train/loss_step=0.0494]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 331/459 [09:03<03:30,  0.61it/s, v_num=0w59, train/loss_step=0.0459]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 332/459 [09:04<03:28,  0.61it/s, v_num=0w59, train/loss_step=0.0459]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 332/459 [09:04<03:28,  0.61it/s, v_num=0w59, train/loss_step=0.0495]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 333/459 [09:06<03:26,  0.61it/s, v_num=0w59, train/loss_step=0.0495]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 333/459 [09:06<03:26,  0.61it/s, v_num=0w59, train/loss_step=0.0444]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 334/459 [09:08<03:25,  0.61it/s, v_num=0w59, train/loss_step=0.0444]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 334/459 [09:08<03:25,  0.61it/s, v_num=0w59, train/loss_step=0.0475]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 335/459 [09:09<03:23,  0.61it/s, v_num=0w59, train/loss_step=0.0475]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 335/459 [09:09<03:23,  0.61it/s, v_num=0w59, train/loss_step=0.0485]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 336/459 [09:11<03:21,  0.61it/s, v_num=0w59, train/loss_step=0.0485]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 336/459 [09:11<03:21,  0.61it/s, v_num=0w59, train/loss_step=0.050] Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 337/459 [09:12<03:20,  0.61it/s, v_num=0w59, train/loss_step=0.050]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 337/459 [09:12<03:20,  0.61it/s, v_num=0w59, train/loss_step=0.0457]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 338/459 [09:14<03:18,  0.61it/s, v_num=0w59, train/loss_step=0.0457]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 338/459 [09:14<03:18,  0.61it/s, v_num=0w59, train/loss_step=0.0488]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 339/459 [09:16<03:16,  0.61it/s, v_num=0w59, train/loss_step=0.0488]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 339/459 [09:16<03:16,  0.61it/s, v_num=0w59, train/loss_step=0.0444]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 340/459 [09:17<03:15,  0.61it/s, v_num=0w59, train/loss_step=0.0444]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 340/459 [09:17<03:15,  0.61it/s, v_num=0w59, train/loss_step=0.0437]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 341/459 [09:19<03:13,  0.61it/s, v_num=0w59, train/loss_step=0.0437]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 341/459 [09:19<03:13,  0.61it/s, v_num=0w59, train/loss_step=0.0452]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 342/459 [09:20<03:11,  0.61it/s, v_num=0w59, train/loss_step=0.0452]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 342/459 [09:20<03:11,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 343/459 [09:22<03:10,  0.61it/s, v_num=0w59, train/loss_step=0.0487]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 343/459 [09:22<03:10,  0.61it/s, v_num=0w59, train/loss_step=0.0447]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 344/459 [09:24<03:08,  0.61it/s, v_num=0w59, train/loss_step=0.0447]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 344/459 [09:24<03:08,  0.61it/s, v_num=0w59, train/loss_step=0.047] Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 345/459 [09:25<03:06,  0.61it/s, v_num=0w59, train/loss_step=0.047]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 345/459 [09:25<03:06,  0.61it/s, v_num=0w59, train/loss_step=0.0431]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 346/459 [09:27<03:05,  0.61it/s, v_num=0w59, train/loss_step=0.0431]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 346/459 [09:27<03:05,  0.61it/s, v_num=0w59, train/loss_step=0.0449]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 347/459 [09:28<03:03,  0.61it/s, v_num=0w59, train/loss_step=0.0449]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 347/459 [09:28<03:03,  0.61it/s, v_num=0w59, train/loss_step=0.0408]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 348/459 [09:30<03:01,  0.61it/s, v_num=0w59, train/loss_step=0.0408]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 348/459 [09:30<03:01,  0.61it/s, v_num=0w59, train/loss_step=0.0451]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 349/459 [09:32<03:00,  0.61it/s, v_num=0w59, train/loss_step=0.0451]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 349/459 [09:32<03:00,  0.61it/s, v_num=0w59, train/loss_step=0.0448]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 350/459 [09:33<02:58,  0.61it/s, v_num=0w59, train/loss_step=0.0448]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 350/459 [09:33<02:58,  0.61it/s, v_num=0w59, train/loss_step=0.0485]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 351/459 [09:35<02:56,  0.61it/s, v_num=0w59, train/loss_step=0.0485]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 351/459 [09:35<02:56,  0.61it/s, v_num=0w59, train/loss_step=0.0467]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 352/459 [09:36<02:55,  0.61it/s, v_num=0w59, train/loss_step=0.0467]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 352/459 [09:36<02:55,  0.61it/s, v_num=0w59, train/loss_step=0.0448]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 353/459 [09:38<02:53,  0.61it/s, v_num=0w59, train/loss_step=0.0448]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 353/459 [09:38<02:53,  0.61it/s, v_num=0w59, train/loss_step=0.0436]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 354/459 [09:39<02:52,  0.61it/s, v_num=0w59, train/loss_step=0.0436]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 354/459 [09:40<02:52,  0.61it/s, v_num=0w59, train/loss_step=0.0452]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 355/459 [09:41<02:50,  0.61it/s, v_num=0w59, train/loss_step=0.0452]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 355/459 [09:41<02:50,  0.61it/s, v_num=0w59, train/loss_step=0.0458]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 356/459 [09:43<02:48,  0.61it/s, v_num=0w59, train/loss_step=0.0458]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 356/459 [09:43<02:48,  0.61it/s, v_num=0w59, train/loss_step=0.045] Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 357/459 [09:44<02:47,  0.61it/s, v_num=0w59, train/loss_step=0.045]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 357/459 [09:44<02:47,  0.61it/s, v_num=0w59, train/loss_step=0.043]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 358/459 [09:46<02:45,  0.61it/s, v_num=0w59, train/loss_step=0.043]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 358/459 [09:46<02:45,  0.61it/s, v_num=0w59, train/loss_step=0.049]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 359/459 [09:48<02:43,  0.61it/s, v_num=0w59, train/loss_step=0.049]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 359/459 [09:48<02:43,  0.61it/s, v_num=0w59, train/loss_step=0.0415]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 360/459 [09:49<02:42,  0.61it/s, v_num=0w59, train/loss_step=0.0415]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 360/459 [09:49<02:42,  0.61it/s, v_num=0w59, train/loss_step=0.0431]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 361/459 [09:51<02:40,  0.61it/s, v_num=0w59, train/loss_step=0.0431]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 361/459 [09:51<02:40,  0.61it/s, v_num=0w59, train/loss_step=0.0451]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 362/459 [09:52<02:38,  0.61it/s, v_num=0w59, train/loss_step=0.0451]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 362/459 [09:52<02:38,  0.61it/s, v_num=0w59, train/loss_step=0.0465]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 363/459 [09:54<02:37,  0.61it/s, v_num=0w59, train/loss_step=0.0465]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 363/459 [09:54<02:37,  0.61it/s, v_num=0w59, train/loss_step=0.0461]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 364/459 [09:55<02:35,  0.61it/s, v_num=0w59, train/loss_step=0.0461]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 364/459 [09:55<02:35,  0.61it/s, v_num=0w59, train/loss_step=0.0453]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 365/459 [09:57<02:33,  0.61it/s, v_num=0w59, train/loss_step=0.0453]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 365/459 [09:57<02:33,  0.61it/s, v_num=0w59, train/loss_step=0.0449]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 366/459 [09:59<02:32,  0.61it/s, v_num=0w59, train/loss_step=0.0449]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 366/459 [09:59<02:32,  0.61it/s, v_num=0w59, train/loss_step=0.048] Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 367/459 [10:00<02:30,  0.61it/s, v_num=0w59, train/loss_step=0.048]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 367/459 [10:00<02:30,  0.61it/s, v_num=0w59, train/loss_step=0.049]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 368/459 [10:02<02:28,  0.61it/s, v_num=0w59, train/loss_step=0.049]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 368/459 [10:02<02:28,  0.61it/s, v_num=0w59, train/loss_step=0.0426]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 369/459 [10:03<02:27,  0.61it/s, v_num=0w59, train/loss_step=0.0426]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 369/459 [10:03<02:27,  0.61it/s, v_num=0w59, train/loss_step=0.0455]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 370/459 [10:05<02:25,  0.61it/s, v_num=0w59, train/loss_step=0.0455]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 370/459 [10:05<02:25,  0.61it/s, v_num=0w59, train/loss_step=0.0431]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 371/459 [10:06<02:23,  0.61it/s, v_num=0w59, train/loss_step=0.0431]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 371/459 [10:06<02:23,  0.61it/s, v_num=0w59, train/loss_step=0.0423]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 372/459 [10:08<02:22,  0.61it/s, v_num=0w59, train/loss_step=0.0423]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 372/459 [10:08<02:22,  0.61it/s, v_num=0w59, train/loss_step=0.0436]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 373/459 [10:10<02:20,  0.61it/s, v_num=0w59, train/loss_step=0.0436]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 373/459 [10:10<02:20,  0.61it/s, v_num=0w59, train/loss_step=0.0427]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 374/459 [10:11<02:19,  0.61it/s, v_num=0w59, train/loss_step=0.0427]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 374/459 [10:11<02:19,  0.61it/s, v_num=0w59, train/loss_step=0.0432]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 375/459 [10:13<02:17,  0.61it/s, v_num=0w59, train/loss_step=0.0432]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 375/459 [10:13<02:17,  0.61it/s, v_num=0w59, train/loss_step=0.0435]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 376/459 [10:14<02:15,  0.61it/s, v_num=0w59, train/loss_step=0.0435]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 376/459 [10:14<02:15,  0.61it/s, v_num=0w59, train/loss_step=0.0448]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 377/459 [10:16<02:14,  0.61it/s, v_num=0w59, train/loss_step=0.0448]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 377/459 [10:16<02:14,  0.61it/s, v_num=0w59, train/loss_step=0.0482]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 378/459 [10:18<02:12,  0.61it/s, v_num=0w59, train/loss_step=0.0482]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 378/459 [10:18<02:12,  0.61it/s, v_num=0w59, train/loss_step=0.0459]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 379/459 [10:19<02:10,  0.61it/s, v_num=0w59, train/loss_step=0.0459]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 379/459 [10:19<02:10,  0.61it/s, v_num=0w59, train/loss_step=0.0447]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 380/459 [10:21<02:09,  0.61it/s, v_num=0w59, train/loss_step=0.0447]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 380/459 [10:21<02:09,  0.61it/s, v_num=0w59, train/loss_step=0.0419]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 381/459 [10:22<02:07,  0.61it/s, v_num=0w59, train/loss_step=0.0419]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 381/459 [10:22<02:07,  0.61it/s, v_num=0w59, train/loss_step=0.0432]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 382/459 [10:24<02:05,  0.61it/s, v_num=0w59, train/loss_step=0.0432]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 382/459 [10:24<02:05,  0.61it/s, v_num=0w59, train/loss_step=0.0416]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 383/459 [10:26<02:04,  0.61it/s, v_num=0w59, train/loss_step=0.0416]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 383/459 [10:26<02:04,  0.61it/s, v_num=0w59, train/loss_step=0.0403]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 384/459 [10:27<02:02,  0.61it/s, v_num=0w59, train/loss_step=0.0403]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 384/459 [10:27<02:02,  0.61it/s, v_num=0w59, train/loss_step=0.0394]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 385/459 [10:29<02:00,  0.61it/s, v_num=0w59, train/loss_step=0.0394]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 385/459 [10:29<02:00,  0.61it/s, v_num=0w59, train/loss_step=0.0445]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 386/459 [10:30<01:59,  0.61it/s, v_num=0w59, train/loss_step=0.0445]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 386/459 [10:30<01:59,  0.61it/s, v_num=0w59, train/loss_step=0.0435]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 387/459 [10:32<01:57,  0.61it/s, v_num=0w59, train/loss_step=0.0435]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 387/459 [10:32<01:57,  0.61it/s, v_num=0w59, train/loss_step=0.0422]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 388/459 [10:34<01:56,  0.61it/s, v_num=0w59, train/loss_step=0.0422]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 388/459 [10:34<01:56,  0.61it/s, v_num=0w59, train/loss_step=0.0446]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 389/459 [10:35<01:54,  0.61it/s, v_num=0w59, train/loss_step=0.0446]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 389/459 [10:35<01:54,  0.61it/s, v_num=0w59, train/loss_step=0.0413]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 390/459 [10:37<01:52,  0.61it/s, v_num=0w59, train/loss_step=0.0413]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 390/459 [10:37<01:52,  0.61it/s, v_num=0w59, train/loss_step=0.0403]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 391/459 [10:38<01:51,  0.61it/s, v_num=0w59, train/loss_step=0.0403]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 391/459 [10:38<01:51,  0.61it/s, v_num=0w59, train/loss_step=0.0413]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 392/459 [10:40<01:49,  0.61it/s, v_num=0w59, train/loss_step=0.0413]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 392/459 [10:40<01:49,  0.61it/s, v_num=0w59, train/loss_step=0.0429]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 393/459 [10:42<01:47,  0.61it/s, v_num=0w59, train/loss_step=0.0429]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 393/459 [10:42<01:47,  0.61it/s, v_num=0w59, train/loss_step=0.0426]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 394/459 [10:43<01:46,  0.61it/s, v_num=0w59, train/loss_step=0.0426]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 394/459 [10:43<01:46,  0.61it/s, v_num=0w59, train/loss_step=0.0455]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 395/459 [10:45<01:44,  0.61it/s, v_num=0w59, train/loss_step=0.0455]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 395/459 [10:45<01:44,  0.61it/s, v_num=0w59, train/loss_step=0.0432]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 396/459 [10:46<01:42,  0.61it/s, v_num=0w59, train/loss_step=0.0432]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 396/459 [10:46<01:42,  0.61it/s, v_num=0w59, train/loss_step=0.0462]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 397/459 [10:48<01:41,  0.61it/s, v_num=0w59, train/loss_step=0.0462]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 397/459 [10:48<01:41,  0.61it/s, v_num=0w59, train/loss_step=0.0431]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 398/459 [10:50<01:39,  0.61it/s, v_num=0w59, train/loss_step=0.0431]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 398/459 [10:50<01:39,  0.61it/s, v_num=0w59, train/loss_step=0.0452]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 399/459 [10:51<01:37,  0.61it/s, v_num=0w59, train/loss_step=0.0452]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 399/459 [10:51<01:37,  0.61it/s, v_num=0w59, train/loss_step=0.045] Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 400/459 [10:53<01:36,  0.61it/s, v_num=0w59, train/loss_step=0.045]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 400/459 [10:53<01:36,  0.61it/s, v_num=0w59, train/loss_step=0.0418]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 401/459 [10:54<01:34,  0.61it/s, v_num=0w59, train/loss_step=0.0418]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 401/459 [10:54<01:34,  0.61it/s, v_num=0w59, train/loss_step=0.0397]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 402/459 [10:56<01:33,  0.61it/s, v_num=0w59, train/loss_step=0.0397]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 402/459 [10:56<01:33,  0.61it/s, v_num=0w59, train/loss_step=0.0421]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 403/459 [10:58<01:31,  0.61it/s, v_num=0w59, train/loss_step=0.0421]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 403/459 [10:58<01:31,  0.61it/s, v_num=0w59, train/loss_step=0.0401]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 404/459 [10:59<01:29,  0.61it/s, v_num=0w59, train/loss_step=0.0401]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 404/459 [10:59<01:29,  0.61it/s, v_num=0w59, train/loss_step=0.0407]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 405/459 [11:01<01:28,  0.61it/s, v_num=0w59, train/loss_step=0.0407]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 405/459 [11:01<01:28,  0.61it/s, v_num=0w59, train/loss_step=0.0443]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 406/459 [11:02<01:26,  0.61it/s, v_num=0w59, train/loss_step=0.0443]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 406/459 [11:02<01:26,  0.61it/s, v_num=0w59, train/loss_step=0.0379]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 407/459 [11:04<01:24,  0.61it/s, v_num=0w59, train/loss_step=0.0379]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 407/459 [11:04<01:24,  0.61it/s, v_num=0w59, train/loss_step=0.044] Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 408/459 [11:06<01:23,  0.61it/s, v_num=0w59, train/loss_step=0.044]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 408/459 [11:06<01:23,  0.61it/s, v_num=0w59, train/loss_step=0.0414]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 409/459 [11:07<01:21,  0.61it/s, v_num=0w59, train/loss_step=0.0414]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 409/459 [11:07<01:21,  0.61it/s, v_num=0w59, train/loss_step=0.0424]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 410/459 [11:09<01:19,  0.61it/s, v_num=0w59, train/loss_step=0.0424]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 410/459 [11:09<01:19,  0.61it/s, v_num=0w59, train/loss_step=0.0404]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 411/459 [11:10<01:18,  0.61it/s, v_num=0w59, train/loss_step=0.0404]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 411/459 [11:10<01:18,  0.61it/s, v_num=0w59, train/loss_step=0.0408]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 412/459 [11:12<01:16,  0.61it/s, v_num=0w59, train/loss_step=0.0408]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 412/459 [11:12<01:16,  0.61it/s, v_num=0w59, train/loss_step=0.0433]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 413/459 [11:14<01:15,  0.61it/s, v_num=0w59, train/loss_step=0.0433]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 413/459 [11:14<01:15,  0.61it/s, v_num=0w59, train/loss_step=0.0427]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 414/459 [11:15<01:13,  0.61it/s, v_num=0w59, train/loss_step=0.0427]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 414/459 [11:15<01:13,  0.61it/s, v_num=0w59, train/loss_step=0.0364]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 415/459 [11:17<01:11,  0.61it/s, v_num=0w59, train/loss_step=0.0364]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 415/459 [11:17<01:11,  0.61it/s, v_num=0w59, train/loss_step=0.0444]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 416/459 [11:18<01:10,  0.61it/s, v_num=0w59, train/loss_step=0.0444]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 416/459 [11:18<01:10,  0.61it/s, v_num=0w59, train/loss_step=0.0382]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 417/459 [11:20<01:08,  0.61it/s, v_num=0w59, train/loss_step=0.0382]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 417/459 [11:20<01:08,  0.61it/s, v_num=0w59, train/loss_step=0.0414]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 418/459 [11:22<01:06,  0.61it/s, v_num=0w59, train/loss_step=0.0414]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 418/459 [11:22<01:06,  0.61it/s, v_num=0w59, train/loss_step=0.0435]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 419/459 [11:23<01:05,  0.61it/s, v_num=0w59, train/loss_step=0.0435]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 419/459 [11:23<01:05,  0.61it/s, v_num=0w59, train/loss_step=0.044] Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 420/459 [11:25<01:03,  0.61it/s, v_num=0w59, train/loss_step=0.044]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 420/459 [11:25<01:03,  0.61it/s, v_num=0w59, train/loss_step=0.0418]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 421/459 [11:26<01:01,  0.61it/s, v_num=0w59, train/loss_step=0.0418]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 421/459 [11:26<01:02,  0.61it/s, v_num=0w59, train/loss_step=0.0409]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 422/459 [11:28<01:00,  0.61it/s, v_num=0w59, train/loss_step=0.0409]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 422/459 [11:28<01:00,  0.61it/s, v_num=0w59, train/loss_step=0.0438]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 423/459 [11:30<00:58,  0.61it/s, v_num=0w59, train/loss_step=0.0438]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 423/459 [11:30<00:58,  0.61it/s, v_num=0w59, train/loss_step=0.042] Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 424/459 [11:31<00:57,  0.61it/s, v_num=0w59, train/loss_step=0.042]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 424/459 [11:31<00:57,  0.61it/s, v_num=0w59, train/loss_step=0.036]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 425/459 [11:33<00:55,  0.61it/s, v_num=0w59, train/loss_step=0.036]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 425/459 [11:33<00:55,  0.61it/s, v_num=0w59, train/loss_step=0.0387]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 426/459 [11:34<00:53,  0.61it/s, v_num=0w59, train/loss_step=0.0387]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 426/459 [11:34<00:53,  0.61it/s, v_num=0w59, train/loss_step=0.0407]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 427/459 [11:36<00:52,  0.61it/s, v_num=0w59, train/loss_step=0.0407]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 427/459 [11:36<00:52,  0.61it/s, v_num=0w59, train/loss_step=0.0403]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 428/459 [11:38<00:50,  0.61it/s, v_num=0w59, train/loss_step=0.0403]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 428/459 [11:38<00:50,  0.61it/s, v_num=0w59, train/loss_step=0.0418]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 429/459 [11:39<00:48,  0.61it/s, v_num=0w59, train/loss_step=0.0418]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 429/459 [11:39<00:48,  0.61it/s, v_num=0w59, train/loss_step=0.0416]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 430/459 [11:41<00:47,  0.61it/s, v_num=0w59, train/loss_step=0.0416]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 430/459 [11:41<00:47,  0.61it/s, v_num=0w59, train/loss_step=0.0396]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 431/459 [11:42<00:45,  0.61it/s, v_num=0w59, train/loss_step=0.0396]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 431/459 [11:42<00:45,  0.61it/s, v_num=0w59, train/loss_step=0.0416]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 432/459 [11:44<00:44,  0.61it/s, v_num=0w59, train/loss_step=0.0416]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 432/459 [11:44<00:44,  0.61it/s, v_num=0w59, train/loss_step=0.0395]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 433/459 [11:46<00:42,  0.61it/s, v_num=0w59, train/loss_step=0.0395]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 433/459 [11:46<00:42,  0.61it/s, v_num=0w59, train/loss_step=0.0394]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 434/459 [11:47<00:40,  0.61it/s, v_num=0w59, train/loss_step=0.0394]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 434/459 [11:47<00:40,  0.61it/s, v_num=0w59, train/loss_step=0.0422]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 435/459 [11:49<00:39,  0.61it/s, v_num=0w59, train/loss_step=0.0422]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 435/459 [11:49<00:39,  0.61it/s, v_num=0w59, train/loss_step=0.0386]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 436/459 [11:50<00:37,  0.61it/s, v_num=0w59, train/loss_step=0.0386]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 436/459 [11:50<00:37,  0.61it/s, v_num=0w59, train/loss_step=0.0416]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 437/459 [11:52<00:35,  0.61it/s, v_num=0w59, train/loss_step=0.0416]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 437/459 [11:52<00:35,  0.61it/s, v_num=0w59, train/loss_step=0.0417]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 438/459 [11:54<00:34,  0.61it/s, v_num=0w59, train/loss_step=0.0417]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 438/459 [11:54<00:34,  0.61it/s, v_num=0w59, train/loss_step=0.0389]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 439/459 [11:55<00:32,  0.61it/s, v_num=0w59, train/loss_step=0.0389]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 439/459 [11:55<00:32,  0.61it/s, v_num=0w59, train/loss_step=0.0365]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 440/459 [11:57<00:30,  0.61it/s, v_num=0w59, train/loss_step=0.0365]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 440/459 [11:57<00:30,  0.61it/s, v_num=0w59, train/loss_step=0.0362]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 441/459 [11:58<00:29,  0.61it/s, v_num=0w59, train/loss_step=0.0362]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 441/459 [11:58<00:29,  0.61it/s, v_num=0w59, train/loss_step=0.0409]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 442/459 [12:00<00:27,  0.61it/s, v_num=0w59, train/loss_step=0.0409]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 442/459 [12:00<00:27,  0.61it/s, v_num=0w59, train/loss_step=0.0385]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 443/459 [12:02<00:26,  0.61it/s, v_num=0w59, train/loss_step=0.0385]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 443/459 [12:02<00:26,  0.61it/s, v_num=0w59, train/loss_step=0.0396]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 444/459 [12:03<00:24,  0.61it/s, v_num=0w59, train/loss_step=0.0396]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 444/459 [12:03<00:24,  0.61it/s, v_num=0w59, train/loss_step=0.0402]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 445/459 [12:05<00:22,  0.61it/s, v_num=0w59, train/loss_step=0.0402]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 445/459 [12:05<00:22,  0.61it/s, v_num=0w59, train/loss_step=0.044] Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 446/459 [12:06<00:21,  0.61it/s, v_num=0w59, train/loss_step=0.044]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 446/459 [12:06<00:21,  0.61it/s, v_num=0w59, train/loss_step=0.0385]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 447/459 [12:08<00:19,  0.61it/s, v_num=0w59, train/loss_step=0.0385]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 447/459 [12:08<00:19,  0.61it/s, v_num=0w59, train/loss_step=0.043] Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 448/459 [12:09<00:17,  0.61it/s, v_num=0w59, train/loss_step=0.043]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 448/459 [12:09<00:17,  0.61it/s, v_num=0w59, train/loss_step=0.0391]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 449/459 [12:11<00:16,  0.61it/s, v_num=0w59, train/loss_step=0.0391]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 449/459 [12:11<00:16,  0.61it/s, v_num=0w59, train/loss_step=0.0379]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 450/459 [12:13<00:14,  0.61it/s, v_num=0w59, train/loss_step=0.0379]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 450/459 [12:13<00:14,  0.61it/s, v_num=0w59, train/loss_step=0.0394]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 451/459 [12:14<00:13,  0.61it/s, v_num=0w59, train/loss_step=0.0394]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 451/459 [12:14<00:13,  0.61it/s, v_num=0w59, train/loss_step=0.038] Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 452/459 [12:16<00:11,  0.61it/s, v_num=0w59, train/loss_step=0.038]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 452/459 [12:16<00:11,  0.61it/s, v_num=0w59, train/loss_step=0.041]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 453/459 [12:17<00:09,  0.61it/s, v_num=0w59, train/loss_step=0.041]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 453/459 [12:17<00:09,  0.61it/s, v_num=0w59, train/loss_step=0.0418]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 454/459 [12:19<00:08,  0.61it/s, v_num=0w59, train/loss_step=0.0418]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 454/459 [12:19<00:08,  0.61it/s, v_num=0w59, train/loss_step=0.044] Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 455/459 [12:20<00:06,  0.61it/s, v_num=0w59, train/loss_step=0.044]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 455/459 [12:20<00:06,  0.61it/s, v_num=0w59, train/loss_step=0.0441]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 456/459 [12:22<00:04,  0.61it/s, v_num=0w59, train/loss_step=0.0441]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 456/459 [12:22<00:04,  0.61it/s, v_num=0w59, train/loss_step=0.0386]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 457/459 [12:23<00:03,  0.61it/s, v_num=0w59, train/loss_step=0.0386]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 457/459 [12:23<00:03,  0.61it/s, v_num=0w59, train/loss_step=0.0387]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 458/459 [12:25<00:01,  0.61it/s, v_num=0w59, train/loss_step=0.0387]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 458/459 [12:25<00:01,  0.61it/s, v_num=0w59, train/loss_step=0.0414]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [12:27<00:00,  0.61it/s, v_num=0w59, train/loss_step=0.0414]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [12:27<00:00,  0.61it/s, v_num=0w59, train/loss_step=0.0416]hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/52 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/52 [00:00<?, ?it/s][A
Validation DataLoader 0:   2%|â–         | 1/52 [00:01<00:52,  0.97it/s][A
Validation DataLoader 0:   4%|â–         | 2/52 [00:01<00:47,  1.06it/s][A
Validation DataLoader 0:   6%|â–Œ         | 3/52 [00:02<00:44,  1.09it/s][A
Validation DataLoader 0:   8%|â–Š         | 4/52 [00:03<00:43,  1.11it/s][A
Validation DataLoader 0:  10%|â–‰         | 5/52 [00:04<00:42,  1.12it/s][A
Validation DataLoader 0:  12%|â–ˆâ–        | 6/52 [00:05<00:40,  1.13it/s][A
Validation DataLoader 0:  13%|â–ˆâ–        | 7/52 [00:06<00:39,  1.13it/s][A
Validation DataLoader 0:  15%|â–ˆâ–Œ        | 8/52 [00:07<00:38,  1.14it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 9/52 [00:07<00:37,  1.14it/s][A
Validation DataLoader 0:  19%|â–ˆâ–‰        | 10/52 [00:08<00:36,  1.14it/s][A
Validation DataLoader 0:  21%|â–ˆâ–ˆ        | 11/52 [00:09<00:35,  1.14it/s][A
Validation DataLoader 0:  23%|â–ˆâ–ˆâ–       | 12/52 [00:10<00:34,  1.15it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 13/52 [00:11<00:33,  1.15it/s][A
Validation DataLoader 0:  27%|â–ˆâ–ˆâ–‹       | 14/52 [00:12<00:33,  1.15it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–‰       | 15/52 [00:13<00:32,  1.15it/s][A
Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆ       | 16/52 [00:13<00:31,  1.15it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 17/52 [00:14<00:30,  1.15it/s][A
Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 18/52 [00:15<00:29,  1.15it/s][A
Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/52 [00:16<00:28,  1.16it/s][A
Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/52 [00:17<00:27,  1.16it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/52 [00:18<00:26,  1.16it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/52 [00:19<00:25,  1.16it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/52 [00:19<00:25,  1.16it/s][A
Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/52 [00:20<00:24,  1.16it/s][A
Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 25/52 [00:21<00:23,  1.16it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/52 [00:22<00:22,  1.16it/s][A
Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/52 [00:23<00:21,  1.16it/s][A
Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/52 [00:24<00:20,  1.16it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 29/52 [00:25<00:19,  1.16it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 30/52 [00:25<00:18,  1.16it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 31/52 [00:26<00:18,  1.16it/s][A
Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/52 [00:27<00:17,  1.16it/s][A
Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/52 [00:28<00:16,  1.16it/s][A
Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 34/52 [00:29<00:15,  1.16it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 35/52 [00:30<00:14,  1.16it/s][A
Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 36/52 [00:31<00:13,  1.16it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 37/52 [00:31<00:12,  1.16it/s][A
Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/52 [00:32<00:12,  1.16it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 39/52 [00:33<00:11,  1.16it/s][A
Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 40/52 [00:34<00:10,  1.16it/s][A
Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 41/52 [00:35<00:09,  1.16it/s][A
Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 42/52 [00:36<00:08,  1.16it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/52 [00:36<00:07,  1.16it/s][A
Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/52 [00:37<00:06,  1.16it/s][A
Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 45/52 [00:38<00:06,  1.16it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 46/52 [00:39<00:05,  1.16it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 47/52 [00:40<00:04,  1.16it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/52 [00:41<00:03,  1.16it/s][A
Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/52 [00:42<00:02,  1.16it/s][A
Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 50/52 [00:42<00:01,  1.16it/s][A
Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 51/52 [00:43<00:00,  1.16it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:44<00:00,  1.16it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [13:16<00:00,  0.58it/s, v_num=0w59, train/loss_step=0.0416, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [13:16<00:00,  0.58it/s, v_num=0w59, train/loss_step=0.0416, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 0:   0%|          | 0/459 [00:00<?, ?it/s, v_num=0w59, train/loss_step=0.0416, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]          Epoch 1:   0%|          | 0/459 [00:00<?, ?it/s, v_num=0w59, train/loss_step=0.0416, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]hyperparameters: "compile":            False
"learning_rate":      0.0005
"loss":               bce
"lr_rate":            [0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002]
"lr_scheduler_epoch": [10, 15, 20, 25, 30, 35, 50, 45]
"net":                HGCN(
  (stem): Stem_conv(
    (convs): Sequential(
      (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (backbone): Sequential(
    (0): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): Identity()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)
        )
        (drop_path): DropPath()
      )
    )
    (2): DownSample(
      (conv): Sequential(
        (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (4): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(480, 480, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
        )
        (drop_path): DropPath()
      )
    )
    (5): DownSample(
      (conv): Sequential(
        (0): Conv2d(160, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (7): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (8): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (9): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (10): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (11): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1200, 1200, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1200, 400, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(400, 1600, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(1600, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(1600, 1600, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1600)
        )
        (drop_path): DropPath()
      )
    )
    (12): DownSample(
      (conv): Sequential(
        (0): Conv2d(400, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
    (14): Sequential(
      (0): Grapher(
        (fc1): Sequential(
          (0): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (graph_conv): DyGraphConv2d(
          (gconv): MRConv2d(
            (nn): BasicConv(
              (0): Conv2d(1920, 1920, kernel_size=(1, 1), stride=(1, 1), groups=4)
              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): GELU(approximate='none')
            )
            (get_centroids): HyperedgeConstruction(
              (centers_proposal): AdaptiveAvgPool2d(output_size=(5, 10))
            )
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
        (fc2): Sequential(
          (0): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop_path): DropPath()
      )
      (1): ConvFFN(
        (fc1): Sequential(
          (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (act): GELU(approximate='none')
        (fc2): Sequential(
          (0): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv): ResDWC(
          (conv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
        )
        (drop_path): DropPath()
      )
    )
  )
  (prediction): Sequential(
    (0): Conv2d(640, 1024, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.0, inplace=False)
    (4): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))
  )
)
"opt_warmup":         True
"optimizer":          functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0005, weight_decay=5e-07, eps=1e-08, betas=[0.95, 0.999])
"scheduler":          functools.partial(<class 'torch.optim.lr_scheduler.MultiStepLR'>, milestones=[10, 15, 20, 25, 30, 35, 40], gamma=0.5)
Epoch 1:   0%|          | 1/459 [00:04<35:54,  0.21it/s, v_num=0w59, train/loss_step=0.0416, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   0%|          | 1/459 [00:04<35:54,  0.21it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   0%|          | 2/459 [00:06<23:57,  0.32it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   0%|          | 2/459 [00:06<23:57,  0.32it/s, v_num=0w59, train/loss_step=0.0393, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   1%|          | 3/459 [00:08<20:39,  0.37it/s, v_num=0w59, train/loss_step=0.0393, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   1%|          | 3/459 [00:08<20:39,  0.37it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:   1%|          | 4/459 [00:09<18:28,  0.41it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   1%|          | 4/459 [00:09<18:28,  0.41it/s, v_num=0w59, train/loss_step=0.0394, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   1%|          | 5/459 [00:11<17:09,  0.44it/s, v_num=0w59, train/loss_step=0.0394, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   1%|          | 5/459 [00:11<17:09,  0.44it/s, v_num=0w59, train/loss_step=0.0413, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   1%|â–         | 6/459 [00:12<16:15,  0.46it/s, v_num=0w59, train/loss_step=0.0413, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   1%|â–         | 6/459 [00:12<16:15,  0.46it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 7/459 [00:14<15:37,  0.48it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 7/459 [00:14<15:37,  0.48it/s, v_num=0w59, train/loss_step=0.0403, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 8/459 [00:16<15:08,  0.50it/s, v_num=0w59, train/loss_step=0.0403, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 8/459 [00:16<15:08,  0.50it/s, v_num=0w59, train/loss_step=0.041, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:   2%|â–         | 9/459 [00:17<14:46,  0.51it/s, v_num=0w59, train/loss_step=0.041, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 9/459 [00:17<14:46,  0.51it/s, v_num=0w59, train/loss_step=0.0421, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 10/459 [00:19<14:27,  0.52it/s, v_num=0w59, train/loss_step=0.0421, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 10/459 [00:19<14:27,  0.52it/s, v_num=0w59, train/loss_step=0.0386, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 11/459 [00:20<14:12,  0.53it/s, v_num=0w59, train/loss_step=0.0386, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   2%|â–         | 11/459 [00:20<14:12,  0.53it/s, v_num=0w59, train/loss_step=0.0434, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 12/459 [00:22<13:58,  0.53it/s, v_num=0w59, train/loss_step=0.0434, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 12/459 [00:22<13:58,  0.53it/s, v_num=0w59, train/loss_step=0.0412, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 13/459 [00:24<13:46,  0.54it/s, v_num=0w59, train/loss_step=0.0412, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 13/459 [00:24<13:46,  0.54it/s, v_num=0w59, train/loss_step=0.0357, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 14/459 [00:25<13:36,  0.54it/s, v_num=0w59, train/loss_step=0.0357, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 14/459 [00:25<13:36,  0.54it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 15/459 [00:27<13:27,  0.55it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 15/459 [00:27<13:27,  0.55it/s, v_num=0w59, train/loss_step=0.0399, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 16/459 [00:28<13:19,  0.55it/s, v_num=0w59, train/loss_step=0.0399, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   3%|â–         | 16/459 [00:28<13:19,  0.55it/s, v_num=0w59, train/loss_step=0.0384, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   4%|â–         | 17/459 [00:30<13:12,  0.56it/s, v_num=0w59, train/loss_step=0.0384, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   4%|â–         | 17/459 [00:30<13:12,  0.56it/s, v_num=0w59, train/loss_step=0.0396, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   4%|â–         | 18/459 [00:32<13:05,  0.56it/s, v_num=0w59, train/loss_step=0.0396, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   4%|â–         | 18/459 [00:32<13:05,  0.56it/s, v_num=0w59, train/loss_step=0.0393, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   4%|â–         | 19/459 [00:33<12:59,  0.56it/s, v_num=0w59, train/loss_step=0.0393, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   4%|â–         | 19/459 [00:33<12:59,  0.56it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:   4%|â–         | 20/459 [00:35<12:53,  0.57it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   4%|â–         | 20/459 [00:35<12:53,  0.57it/s, v_num=0w59, train/loss_step=0.0357, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–         | 21/459 [00:36<12:48,  0.57it/s, v_num=0w59, train/loss_step=0.0357, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–         | 21/459 [00:36<12:48,  0.57it/s, v_num=0w59, train/loss_step=0.0396, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–         | 22/459 [00:38<12:43,  0.57it/s, v_num=0w59, train/loss_step=0.0396, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–         | 22/459 [00:38<12:43,  0.57it/s, v_num=0w59, train/loss_step=0.0403, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–Œ         | 23/459 [00:40<12:38,  0.57it/s, v_num=0w59, train/loss_step=0.0403, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–Œ         | 23/459 [00:40<12:38,  0.57it/s, v_num=0w59, train/loss_step=0.0397, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–Œ         | 24/459 [00:41<12:34,  0.58it/s, v_num=0w59, train/loss_step=0.0397, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–Œ         | 24/459 [00:41<12:34,  0.58it/s, v_num=0w59, train/loss_step=0.0392, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–Œ         | 25/459 [00:43<12:29,  0.58it/s, v_num=0w59, train/loss_step=0.0392, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   5%|â–Œ         | 25/459 [00:43<12:29,  0.58it/s, v_num=0w59, train/loss_step=0.0391, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   6%|â–Œ         | 26/459 [00:44<12:25,  0.58it/s, v_num=0w59, train/loss_step=0.0391, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   6%|â–Œ         | 26/459 [00:44<12:25,  0.58it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   6%|â–Œ         | 27/459 [00:46<12:22,  0.58it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   6%|â–Œ         | 27/459 [00:46<12:22,  0.58it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   6%|â–Œ         | 28/459 [00:47<12:18,  0.58it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   6%|â–Œ         | 28/459 [00:47<12:18,  0.58it/s, v_num=0w59, train/loss_step=0.0398, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   6%|â–‹         | 29/459 [00:49<12:14,  0.59it/s, v_num=0w59, train/loss_step=0.0398, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   6%|â–‹         | 29/459 [00:49<12:14,  0.59it/s, v_num=0w59, train/loss_step=0.0414, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 30/459 [00:51<12:11,  0.59it/s, v_num=0w59, train/loss_step=0.0414, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 30/459 [00:51<12:11,  0.59it/s, v_num=0w59, train/loss_step=0.0398, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 31/459 [00:52<12:08,  0.59it/s, v_num=0w59, train/loss_step=0.0398, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 31/459 [00:52<12:08,  0.59it/s, v_num=0w59, train/loss_step=0.0407, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 32/459 [00:54<12:05,  0.59it/s, v_num=0w59, train/loss_step=0.0407, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 32/459 [00:54<12:05,  0.59it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 33/459 [00:55<12:02,  0.59it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 33/459 [00:55<12:02,  0.59it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 34/459 [00:57<11:59,  0.59it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   7%|â–‹         | 34/459 [00:57<11:59,  0.59it/s, v_num=0w59, train/loss_step=0.0389, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 35/459 [00:59<11:56,  0.59it/s, v_num=0w59, train/loss_step=0.0389, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 35/459 [00:59<11:56,  0.59it/s, v_num=0w59, train/loss_step=0.0406, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 36/459 [01:00<11:53,  0.59it/s, v_num=0w59, train/loss_step=0.0406, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 36/459 [01:00<11:53,  0.59it/s, v_num=0w59, train/loss_step=0.0397, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 37/459 [01:02<11:50,  0.59it/s, v_num=0w59, train/loss_step=0.0397, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 37/459 [01:02<11:50,  0.59it/s, v_num=0w59, train/loss_step=0.0466, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 38/459 [01:03<11:48,  0.59it/s, v_num=0w59, train/loss_step=0.0466, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 38/459 [01:03<11:48,  0.59it/s, v_num=0w59, train/loss_step=0.0386, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 39/459 [01:05<11:45,  0.60it/s, v_num=0w59, train/loss_step=0.0386, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   8%|â–Š         | 39/459 [01:05<11:45,  0.60it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   9%|â–Š         | 40/459 [01:07<11:43,  0.60it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   9%|â–Š         | 40/459 [01:07<11:43,  0.60it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   9%|â–‰         | 41/459 [01:08<11:40,  0.60it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   9%|â–‰         | 41/459 [01:08<11:40,  0.60it/s, v_num=0w59, train/loss_step=0.0421, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   9%|â–‰         | 42/459 [01:10<11:38,  0.60it/s, v_num=0w59, train/loss_step=0.0421, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   9%|â–‰         | 42/459 [01:10<11:38,  0.60it/s, v_num=0w59, train/loss_step=0.0383, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   9%|â–‰         | 43/459 [01:11<11:35,  0.60it/s, v_num=0w59, train/loss_step=0.0383, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:   9%|â–‰         | 43/459 [01:11<11:35,  0.60it/s, v_num=0w59, train/loss_step=0.0392, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–‰         | 44/459 [01:13<11:33,  0.60it/s, v_num=0w59, train/loss_step=0.0392, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–‰         | 44/459 [01:13<11:33,  0.60it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–‰         | 45/459 [01:15<11:30,  0.60it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–‰         | 45/459 [01:15<11:30,  0.60it/s, v_num=0w59, train/loss_step=0.0394, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–ˆ         | 46/459 [01:16<11:28,  0.60it/s, v_num=0w59, train/loss_step=0.0394, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–ˆ         | 46/459 [01:16<11:28,  0.60it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–ˆ         | 47/459 [01:18<11:26,  0.60it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–ˆ         | 47/459 [01:18<11:26,  0.60it/s, v_num=0w59, train/loss_step=0.0391, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–ˆ         | 48/459 [01:19<11:24,  0.60it/s, v_num=0w59, train/loss_step=0.0391, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  10%|â–ˆ         | 48/459 [01:19<11:24,  0.60it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  11%|â–ˆ         | 49/459 [01:21<11:21,  0.60it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  11%|â–ˆ         | 49/459 [01:21<11:21,  0.60it/s, v_num=0w59, train/loss_step=0.0397, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  11%|â–ˆ         | 50/459 [01:23<11:19,  0.60it/s, v_num=0w59, train/loss_step=0.0397, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  11%|â–ˆ         | 50/459 [01:23<11:19,  0.60it/s, v_num=0w59, train/loss_step=0.0356, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  11%|â–ˆ         | 51/459 [01:24<11:17,  0.60it/s, v_num=0w59, train/loss_step=0.0356, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  11%|â–ˆ         | 51/459 [01:24<11:17,  0.60it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  11%|â–ˆâ–        | 52/459 [01:26<11:15,  0.60it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  11%|â–ˆâ–        | 52/459 [01:26<11:15,  0.60it/s, v_num=0w59, train/loss_step=0.0394, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 53/459 [01:27<11:12,  0.60it/s, v_num=0w59, train/loss_step=0.0394, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 53/459 [01:27<11:12,  0.60it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 54/459 [01:29<11:10,  0.60it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 54/459 [01:29<11:10,  0.60it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 55/459 [01:30<11:08,  0.60it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 55/459 [01:30<11:08,  0.60it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  12%|â–ˆâ–        | 56/459 [01:32<11:06,  0.60it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 56/459 [01:32<11:06,  0.60it/s, v_num=0w59, train/loss_step=0.0366, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 57/459 [01:34<11:04,  0.61it/s, v_num=0w59, train/loss_step=0.0366, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  12%|â–ˆâ–        | 57/459 [01:34<11:04,  0.61it/s, v_num=0w59, train/loss_step=0.0388, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  13%|â–ˆâ–        | 58/459 [01:35<11:01,  0.61it/s, v_num=0w59, train/loss_step=0.0388, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  13%|â–ˆâ–        | 58/459 [01:35<11:01,  0.61it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  13%|â–ˆâ–        | 59/459 [01:37<10:59,  0.61it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  13%|â–ˆâ–        | 59/459 [01:37<10:59,  0.61it/s, v_num=0w59, train/loss_step=0.0384, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  13%|â–ˆâ–        | 60/459 [01:38<10:57,  0.61it/s, v_num=0w59, train/loss_step=0.0384, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  13%|â–ˆâ–        | 60/459 [01:38<10:57,  0.61it/s, v_num=0w59, train/loss_step=0.0399, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  13%|â–ˆâ–        | 61/459 [01:40<10:55,  0.61it/s, v_num=0w59, train/loss_step=0.0399, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  13%|â–ˆâ–        | 61/459 [01:40<10:55,  0.61it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 62/459 [01:42<10:53,  0.61it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 62/459 [01:42<10:53,  0.61it/s, v_num=0w59, train/loss_step=0.0389, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 63/459 [01:43<10:51,  0.61it/s, v_num=0w59, train/loss_step=0.0389, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 63/459 [01:43<10:51,  0.61it/s, v_num=0w59, train/loss_step=0.0353, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 64/459 [01:45<10:49,  0.61it/s, v_num=0w59, train/loss_step=0.0353, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 64/459 [01:45<10:49,  0.61it/s, v_num=0w59, train/loss_step=0.0412, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 65/459 [01:46<10:47,  0.61it/s, v_num=0w59, train/loss_step=0.0412, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 65/459 [01:46<10:47,  0.61it/s, v_num=0w59, train/loss_step=0.0379, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 66/459 [01:48<10:45,  0.61it/s, v_num=0w59, train/loss_step=0.0379, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  14%|â–ˆâ–        | 66/459 [01:48<10:45,  0.61it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–        | 67/459 [01:49<10:43,  0.61it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–        | 67/459 [01:49<10:43,  0.61it/s, v_num=0w59, train/loss_step=0.0383, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–        | 68/459 [01:51<10:41,  0.61it/s, v_num=0w59, train/loss_step=0.0383, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–        | 68/459 [01:51<10:41,  0.61it/s, v_num=0w59, train/loss_step=0.0406, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–Œ        | 69/459 [01:53<10:39,  0.61it/s, v_num=0w59, train/loss_step=0.0406, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–Œ        | 69/459 [01:53<10:39,  0.61it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–Œ        | 70/459 [01:54<10:37,  0.61it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–Œ        | 70/459 [01:54<10:37,  0.61it/s, v_num=0w59, train/loss_step=0.0315, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–Œ        | 71/459 [01:56<10:35,  0.61it/s, v_num=0w59, train/loss_step=0.0315, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  15%|â–ˆâ–Œ        | 71/459 [01:56<10:35,  0.61it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  16%|â–ˆâ–Œ        | 72/459 [01:57<10:33,  0.61it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  16%|â–ˆâ–Œ        | 72/459 [01:57<10:33,  0.61it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  16%|â–ˆâ–Œ        | 73/459 [01:59<10:31,  0.61it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  16%|â–ˆâ–Œ        | 73/459 [01:59<10:31,  0.61it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  16%|â–ˆâ–Œ        | 74/459 [02:01<10:29,  0.61it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  16%|â–ˆâ–Œ        | 74/459 [02:01<10:29,  0.61it/s, v_num=0w59, train/loss_step=0.0397, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  16%|â–ˆâ–‹        | 75/459 [02:02<10:27,  0.61it/s, v_num=0w59, train/loss_step=0.0397, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  16%|â–ˆâ–‹        | 75/459 [02:02<10:27,  0.61it/s, v_num=0w59, train/loss_step=0.0401, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 76/459 [02:04<10:26,  0.61it/s, v_num=0w59, train/loss_step=0.0401, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 76/459 [02:04<10:26,  0.61it/s, v_num=0w59, train/loss_step=0.0386, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 77/459 [02:05<10:24,  0.61it/s, v_num=0w59, train/loss_step=0.0386, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 77/459 [02:05<10:24,  0.61it/s, v_num=0w59, train/loss_step=0.0377, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 78/459 [02:07<10:22,  0.61it/s, v_num=0w59, train/loss_step=0.0377, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 78/459 [02:07<10:22,  0.61it/s, v_num=0w59, train/loss_step=0.0336, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 79/459 [02:09<10:20,  0.61it/s, v_num=0w59, train/loss_step=0.0336, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 79/459 [02:09<10:20,  0.61it/s, v_num=0w59, train/loss_step=0.0438, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 80/459 [02:10<10:18,  0.61it/s, v_num=0w59, train/loss_step=0.0438, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  17%|â–ˆâ–‹        | 80/459 [02:10<10:18,  0.61it/s, v_num=0w59, train/loss_step=0.0376, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  18%|â–ˆâ–Š        | 81/459 [02:12<10:16,  0.61it/s, v_num=0w59, train/loss_step=0.0376, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  18%|â–ˆâ–Š        | 81/459 [02:12<10:17,  0.61it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  18%|â–ˆâ–Š        | 82/459 [02:13<10:15,  0.61it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  18%|â–ˆâ–Š        | 82/459 [02:13<10:15,  0.61it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  18%|â–ˆâ–Š        | 83/459 [02:15<10:13,  0.61it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  18%|â–ˆâ–Š        | 83/459 [02:15<10:13,  0.61it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  18%|â–ˆâ–Š        | 84/459 [02:16<10:11,  0.61it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  18%|â–ˆâ–Š        | 84/459 [02:16<10:11,  0.61it/s, v_num=0w59, train/loss_step=0.0392, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–Š        | 85/459 [02:18<10:09,  0.61it/s, v_num=0w59, train/loss_step=0.0392, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–Š        | 85/459 [02:18<10:09,  0.61it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–Š        | 86/459 [02:20<10:08,  0.61it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–Š        | 86/459 [02:20<10:08,  0.61it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–‰        | 87/459 [02:21<10:06,  0.61it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–‰        | 87/459 [02:21<10:06,  0.61it/s, v_num=0w59, train/loss_step=0.0344, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–‰        | 88/459 [02:23<10:04,  0.61it/s, v_num=0w59, train/loss_step=0.0344, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–‰        | 88/459 [02:23<10:04,  0.61it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–‰        | 89/459 [02:24<10:02,  0.61it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  19%|â–ˆâ–‰        | 89/459 [02:24<10:02,  0.61it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  20%|â–ˆâ–‰        | 90/459 [02:26<10:00,  0.61it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  20%|â–ˆâ–‰        | 90/459 [02:26<10:00,  0.61it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  20%|â–ˆâ–‰        | 91/459 [02:28<09:59,  0.61it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  20%|â–ˆâ–‰        | 91/459 [02:28<09:59,  0.61it/s, v_num=0w59, train/loss_step=0.0399, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  20%|â–ˆâ–ˆ        | 92/459 [02:29<09:57,  0.61it/s, v_num=0w59, train/loss_step=0.0399, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  20%|â–ˆâ–ˆ        | 92/459 [02:29<09:57,  0.61it/s, v_num=0w59, train/loss_step=0.036, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  20%|â–ˆâ–ˆ        | 93/459 [02:31<09:55,  0.61it/s, v_num=0w59, train/loss_step=0.036, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  20%|â–ˆâ–ˆ        | 93/459 [02:31<09:55,  0.61it/s, v_num=0w59, train/loss_step=0.0344, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  20%|â–ˆâ–ˆ        | 94/459 [02:32<09:53,  0.61it/s, v_num=0w59, train/loss_step=0.0344, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  20%|â–ˆâ–ˆ        | 94/459 [02:32<09:53,  0.61it/s, v_num=0w59, train/loss_step=0.0358, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  21%|â–ˆâ–ˆ        | 95/459 [02:34<09:52,  0.61it/s, v_num=0w59, train/loss_step=0.0358, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  21%|â–ˆâ–ˆ        | 95/459 [02:34<09:52,  0.61it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  21%|â–ˆâ–ˆ        | 96/459 [02:36<09:50,  0.61it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  21%|â–ˆâ–ˆ        | 96/459 [02:36<09:50,  0.61it/s, v_num=0w59, train/loss_step=0.0375, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  21%|â–ˆâ–ˆ        | 97/459 [02:37<09:48,  0.61it/s, v_num=0w59, train/loss_step=0.0375, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  21%|â–ˆâ–ˆ        | 97/459 [02:37<09:48,  0.61it/s, v_num=0w59, train/loss_step=0.0426, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  21%|â–ˆâ–ˆâ–       | 98/459 [02:39<09:47,  0.61it/s, v_num=0w59, train/loss_step=0.0426, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  21%|â–ˆâ–ˆâ–       | 98/459 [02:39<09:47,  0.61it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  22%|â–ˆâ–ˆâ–       | 99/459 [02:40<09:45,  0.61it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  22%|â–ˆâ–ˆâ–       | 99/459 [02:40<09:45,  0.61it/s, v_num=0w59, train/loss_step=0.0425, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  22%|â–ˆâ–ˆâ–       | 100/459 [02:42<09:43,  0.62it/s, v_num=0w59, train/loss_step=0.0425, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  22%|â–ˆâ–ˆâ–       | 100/459 [02:42<09:43,  0.62it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  22%|â–ˆâ–ˆâ–       | 101/459 [02:44<09:41,  0.62it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  22%|â–ˆâ–ˆâ–       | 101/459 [02:44<09:41,  0.62it/s, v_num=0w59, train/loss_step=0.0381, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  22%|â–ˆâ–ˆâ–       | 102/459 [02:45<09:40,  0.62it/s, v_num=0w59, train/loss_step=0.0381, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  22%|â–ˆâ–ˆâ–       | 102/459 [02:45<09:40,  0.62it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  22%|â–ˆâ–ˆâ–       | 103/459 [02:47<09:38,  0.62it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  22%|â–ˆâ–ˆâ–       | 103/459 [02:47<09:38,  0.62it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  23%|â–ˆâ–ˆâ–       | 104/459 [02:48<09:36,  0.62it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  23%|â–ˆâ–ˆâ–       | 104/459 [02:48<09:36,  0.62it/s, v_num=0w59, train/loss_step=0.0392, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  23%|â–ˆâ–ˆâ–       | 105/459 [02:50<09:35,  0.62it/s, v_num=0w59, train/loss_step=0.0392, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  23%|â–ˆâ–ˆâ–       | 105/459 [02:50<09:35,  0.62it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  23%|â–ˆâ–ˆâ–       | 106/459 [02:52<09:33,  0.62it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  23%|â–ˆâ–ˆâ–       | 106/459 [02:52<09:33,  0.62it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  23%|â–ˆâ–ˆâ–       | 107/459 [02:53<09:31,  0.62it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  23%|â–ˆâ–ˆâ–       | 107/459 [02:53<09:31,  0.62it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 108/459 [02:55<09:30,  0.62it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 108/459 [02:55<09:30,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 109/459 [02:56<09:28,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 109/459 [02:56<09:28,  0.62it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 110/459 [02:58<09:26,  0.62it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 110/459 [02:58<09:26,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 111/459 [03:00<09:24,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 111/459 [03:00<09:24,  0.62it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 112/459 [03:01<09:23,  0.62it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  24%|â–ˆâ–ˆâ–       | 112/459 [03:01<09:23,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–       | 113/459 [03:03<09:21,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–       | 113/459 [03:03<09:21,  0.62it/s, v_num=0w59, train/loss_step=0.0365, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–       | 114/459 [03:04<09:19,  0.62it/s, v_num=0w59, train/loss_step=0.0365, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–       | 114/459 [03:04<09:19,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 115/459 [03:06<09:17,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 115/459 [03:06<09:17,  0.62it/s, v_num=0w59, train/loss_step=0.0369, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 116/459 [03:08<09:16,  0.62it/s, v_num=0w59, train/loss_step=0.0369, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 116/459 [03:08<09:16,  0.62it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 117/459 [03:09<09:14,  0.62it/s, v_num=0w59, train/loss_step=0.038, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  25%|â–ˆâ–ˆâ–Œ       | 117/459 [03:09<09:14,  0.62it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 118/459 [03:11<09:12,  0.62it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 118/459 [03:11<09:12,  0.62it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 119/459 [03:12<09:10,  0.62it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 119/459 [03:12<09:10,  0.62it/s, v_num=0w59, train/loss_step=0.0369, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 120/459 [03:14<09:09,  0.62it/s, v_num=0w59, train/loss_step=0.0369, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  26%|â–ˆâ–ˆâ–Œ       | 120/459 [03:14<09:09,  0.62it/s, v_num=0w59, train/loss_step=0.036, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  26%|â–ˆâ–ˆâ–‹       | 121/459 [03:15<09:07,  0.62it/s, v_num=0w59, train/loss_step=0.036, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  26%|â–ˆâ–ˆâ–‹       | 121/459 [03:15<09:07,  0.62it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 122/459 [03:17<09:05,  0.62it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 122/459 [03:17<09:05,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 123/459 [03:19<09:03,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 123/459 [03:19<09:03,  0.62it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 124/459 [03:20<09:02,  0.62it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 124/459 [03:20<09:02,  0.62it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 125/459 [03:22<09:00,  0.62it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 125/459 [03:22<09:00,  0.62it/s, v_num=0w59, train/loss_step=0.0369, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 126/459 [03:23<08:58,  0.62it/s, v_num=0w59, train/loss_step=0.0369, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  27%|â–ˆâ–ˆâ–‹       | 126/459 [03:23<08:58,  0.62it/s, v_num=0w59, train/loss_step=0.0395, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 127/459 [03:25<08:56,  0.62it/s, v_num=0w59, train/loss_step=0.0395, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 127/459 [03:25<08:56,  0.62it/s, v_num=0w59, train/loss_step=0.039, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 128/459 [03:26<08:55,  0.62it/s, v_num=0w59, train/loss_step=0.039, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 128/459 [03:26<08:55,  0.62it/s, v_num=0w59, train/loss_step=0.0401, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 129/459 [03:28<08:53,  0.62it/s, v_num=0w59, train/loss_step=0.0401, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 129/459 [03:28<08:53,  0.62it/s, v_num=0w59, train/loss_step=0.0347, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 130/459 [03:30<08:51,  0.62it/s, v_num=0w59, train/loss_step=0.0347, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  28%|â–ˆâ–ˆâ–Š       | 130/459 [03:30<08:51,  0.62it/s, v_num=0w59, train/loss_step=0.0357, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–Š       | 131/459 [03:31<08:50,  0.62it/s, v_num=0w59, train/loss_step=0.0357, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–Š       | 131/459 [03:31<08:50,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 132/459 [03:33<08:48,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 132/459 [03:33<08:48,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 133/459 [03:34<08:46,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 133/459 [03:34<08:46,  0.62it/s, v_num=0w59, train/loss_step=0.0352, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 134/459 [03:36<08:44,  0.62it/s, v_num=0w59, train/loss_step=0.0352, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 134/459 [03:36<08:44,  0.62it/s, v_num=0w59, train/loss_step=0.0395, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 135/459 [03:37<08:43,  0.62it/s, v_num=0w59, train/loss_step=0.0395, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 135/459 [03:37<08:43,  0.62it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  30%|â–ˆâ–ˆâ–‰       | 136/459 [03:39<08:41,  0.62it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  30%|â–ˆâ–ˆâ–‰       | 136/459 [03:39<08:41,  0.62it/s, v_num=0w59, train/loss_step=0.0318, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  30%|â–ˆâ–ˆâ–‰       | 137/459 [03:41<08:39,  0.62it/s, v_num=0w59, train/loss_step=0.0318, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  30%|â–ˆâ–ˆâ–‰       | 137/459 [03:41<08:39,  0.62it/s, v_num=0w59, train/loss_step=0.0375, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  30%|â–ˆâ–ˆâ–ˆ       | 138/459 [03:42<08:37,  0.62it/s, v_num=0w59, train/loss_step=0.0375, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  30%|â–ˆâ–ˆâ–ˆ       | 138/459 [03:42<08:37,  0.62it/s, v_num=0w59, train/loss_step=0.0398, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  30%|â–ˆâ–ˆâ–ˆ       | 139/459 [03:44<08:36,  0.62it/s, v_num=0w59, train/loss_step=0.0398, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  30%|â–ˆâ–ˆâ–ˆ       | 139/459 [03:44<08:36,  0.62it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 140/459 [03:45<08:34,  0.62it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 140/459 [03:45<08:34,  0.62it/s, v_num=0w59, train/loss_step=0.0401, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 141/459 [03:47<08:32,  0.62it/s, v_num=0w59, train/loss_step=0.0401, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 141/459 [03:47<08:32,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 142/459 [03:48<08:31,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 142/459 [03:48<08:31,  0.62it/s, v_num=0w59, train/loss_step=0.0398, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 143/459 [03:50<08:29,  0.62it/s, v_num=0w59, train/loss_step=0.0398, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆ       | 143/459 [03:50<08:29,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆâ–      | 144/459 [03:52<08:27,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  31%|â–ˆâ–ˆâ–ˆâ–      | 144/459 [03:52<08:27,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 145/459 [03:53<08:26,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 145/459 [03:53<08:26,  0.62it/s, v_num=0w59, train/loss_step=0.0331, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 146/459 [03:55<08:24,  0.62it/s, v_num=0w59, train/loss_step=0.0331, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 146/459 [03:55<08:24,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 147/459 [03:56<08:22,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 147/459 [03:56<08:22,  0.62it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 148/459 [03:58<08:20,  0.62it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 148/459 [03:58<08:21,  0.62it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 149/459 [03:59<08:19,  0.62it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 149/459 [03:59<08:19,  0.62it/s, v_num=0w59, train/loss_step=0.0369, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 150/459 [04:01<08:17,  0.62it/s, v_num=0w59, train/loss_step=0.0369, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 150/459 [04:01<08:17,  0.62it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 151/459 [04:03<08:15,  0.62it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 151/459 [04:03<08:15,  0.62it/s, v_num=0w59, train/loss_step=0.0352, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 152/459 [04:04<08:14,  0.62it/s, v_num=0w59, train/loss_step=0.0352, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 152/459 [04:04<08:14,  0.62it/s, v_num=0w59, train/loss_step=0.0313, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 153/459 [04:06<08:12,  0.62it/s, v_num=0w59, train/loss_step=0.0313, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 153/459 [04:06<08:12,  0.62it/s, v_num=0w59, train/loss_step=0.0347, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 154/459 [04:07<08:10,  0.62it/s, v_num=0w59, train/loss_step=0.0347, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 154/459 [04:07<08:10,  0.62it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 155/459 [04:09<08:09,  0.62it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 155/459 [04:09<08:09,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 156/459 [04:10<08:07,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 156/459 [04:10<08:07,  0.62it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 157/459 [04:12<08:05,  0.62it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 157/459 [04:12<08:05,  0.62it/s, v_num=0w59, train/loss_step=0.0316, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 158/459 [04:14<08:04,  0.62it/s, v_num=0w59, train/loss_step=0.0316, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 158/459 [04:14<08:04,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 159/459 [04:15<08:02,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 159/459 [04:15<08:02,  0.62it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 160/459 [04:17<08:00,  0.62it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–      | 160/459 [04:17<08:00,  0.62it/s, v_num=0w59, train/loss_step=0.0377, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 161/459 [04:18<07:59,  0.62it/s, v_num=0w59, train/loss_step=0.0377, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 161/459 [04:18<07:59,  0.62it/s, v_num=0w59, train/loss_step=0.0339, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 162/459 [04:20<07:57,  0.62it/s, v_num=0w59, train/loss_step=0.0339, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 162/459 [04:20<07:57,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 163/459 [04:21<07:55,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 163/459 [04:21<07:55,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 164/459 [04:23<07:54,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 164/459 [04:23<07:54,  0.62it/s, v_num=0w59, train/loss_step=0.0379, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 165/459 [04:25<07:52,  0.62it/s, v_num=0w59, train/loss_step=0.0379, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 165/459 [04:25<07:52,  0.62it/s, v_num=0w59, train/loss_step=0.0395, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 166/459 [04:26<07:50,  0.62it/s, v_num=0w59, train/loss_step=0.0395, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 166/459 [04:26<07:50,  0.62it/s, v_num=0w59, train/loss_step=0.0313, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 167/459 [04:28<07:49,  0.62it/s, v_num=0w59, train/loss_step=0.0313, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 167/459 [04:28<07:49,  0.62it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 168/459 [04:29<07:47,  0.62it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 168/459 [04:29<07:47,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 169/459 [04:31<07:45,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 169/459 [04:31<07:45,  0.62it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 170/459 [04:32<07:44,  0.62it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 170/459 [04:32<07:44,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 171/459 [04:34<07:42,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 171/459 [04:34<07:42,  0.62it/s, v_num=0w59, train/loss_step=0.0365, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 172/459 [04:36<07:40,  0.62it/s, v_num=0w59, train/loss_step=0.0365, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 172/459 [04:36<07:40,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 173/459 [04:37<07:39,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 173/459 [04:37<07:39,  0.62it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 174/459 [04:39<07:37,  0.62it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 174/459 [04:39<07:37,  0.62it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 175/459 [04:40<07:35,  0.62it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 175/459 [04:40<07:35,  0.62it/s, v_num=0w59, train/loss_step=0.0323, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 176/459 [04:42<07:34,  0.62it/s, v_num=0w59, train/loss_step=0.0323, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 176/459 [04:42<07:34,  0.62it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 177/459 [04:43<07:32,  0.62it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 177/459 [04:43<07:32,  0.62it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 178/459 [04:45<07:30,  0.62it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 178/459 [04:45<07:30,  0.62it/s, v_num=0w59, train/loss_step=0.0366, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 179/459 [04:47<07:29,  0.62it/s, v_num=0w59, train/loss_step=0.0366, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 179/459 [04:47<07:29,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 180/459 [04:48<07:27,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 180/459 [04:48<07:27,  0.62it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 181/459 [04:50<07:25,  0.62it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 181/459 [04:50<07:25,  0.62it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 182/459 [04:51<07:24,  0.62it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 182/459 [04:51<07:24,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 183/459 [04:53<07:22,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 183/459 [04:53<07:22,  0.62it/s, v_num=0w59, train/loss_step=0.0393, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 184/459 [04:55<07:20,  0.62it/s, v_num=0w59, train/loss_step=0.0393, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 184/459 [04:55<07:20,  0.62it/s, v_num=0w59, train/loss_step=0.0327, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 185/459 [04:56<07:19,  0.62it/s, v_num=0w59, train/loss_step=0.0327, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 185/459 [04:56<07:19,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 186/459 [04:58<07:17,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 186/459 [04:58<07:17,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 187/459 [04:59<07:15,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 187/459 [04:59<07:15,  0.62it/s, v_num=0w59, train/loss_step=0.0324, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 188/459 [05:01<07:14,  0.62it/s, v_num=0w59, train/loss_step=0.0324, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 188/459 [05:01<07:14,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 189/459 [05:02<07:12,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 189/459 [05:02<07:12,  0.62it/s, v_num=0w59, train/loss_step=0.0331, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 190/459 [05:04<07:11,  0.62it/s, v_num=0w59, train/loss_step=0.0331, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 190/459 [05:04<07:11,  0.62it/s, v_num=0w59, train/loss_step=0.0336, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 191/459 [05:06<07:09,  0.62it/s, v_num=0w59, train/loss_step=0.0336, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 191/459 [05:06<07:09,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 192/459 [05:07<07:07,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 192/459 [05:07<07:07,  0.62it/s, v_num=0w59, train/loss_step=0.0358, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/459 [05:09<07:06,  0.62it/s, v_num=0w59, train/loss_step=0.0358, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/459 [05:09<07:06,  0.62it/s, v_num=0w59, train/loss_step=0.0396, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 194/459 [05:10<07:04,  0.62it/s, v_num=0w59, train/loss_step=0.0396, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 194/459 [05:10<07:04,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 195/459 [05:12<07:02,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 195/459 [05:12<07:02,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 196/459 [05:14<07:01,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 196/459 [05:14<07:01,  0.62it/s, v_num=0w59, train/loss_step=0.0358, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 197/459 [05:15<06:59,  0.62it/s, v_num=0w59, train/loss_step=0.0358, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 197/459 [05:15<06:59,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 198/459 [05:17<06:58,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 198/459 [05:17<06:58,  0.62it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 199/459 [05:18<06:56,  0.62it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 199/459 [05:18<06:56,  0.62it/s, v_num=0w59, train/loss_step=0.032, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200/459 [05:20<06:54,  0.62it/s, v_num=0w59, train/loss_step=0.032, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200/459 [05:20<06:54,  0.62it/s, v_num=0w59, train/loss_step=0.0331, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 201/459 [05:21<06:53,  0.62it/s, v_num=0w59, train/loss_step=0.0331, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 201/459 [05:21<06:53,  0.62it/s, v_num=0w59, train/loss_step=0.0383, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 202/459 [05:23<06:51,  0.62it/s, v_num=0w59, train/loss_step=0.0383, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 202/459 [05:23<06:51,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 203/459 [05:25<06:50,  0.62it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 203/459 [05:25<06:50,  0.62it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 204/459 [05:26<06:48,  0.62it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 204/459 [05:26<06:48,  0.62it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 205/459 [05:28<06:46,  0.62it/s, v_num=0w59, train/loss_step=0.0378, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 205/459 [05:28<06:46,  0.62it/s, v_num=0w59, train/loss_step=0.0371, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 206/459 [05:29<06:45,  0.62it/s, v_num=0w59, train/loss_step=0.0371, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 206/459 [05:29<06:45,  0.62it/s, v_num=0w59, train/loss_step=0.036, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 207/459 [05:31<06:43,  0.62it/s, v_num=0w59, train/loss_step=0.036, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 207/459 [05:31<06:43,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 208/459 [05:33<06:41,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 208/459 [05:33<06:41,  0.62it/s, v_num=0w59, train/loss_step=0.0366, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 209/459 [05:34<06:40,  0.62it/s, v_num=0w59, train/loss_step=0.0366, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 209/459 [05:34<06:40,  0.62it/s, v_num=0w59, train/loss_step=0.0401, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 210/459 [05:36<06:38,  0.62it/s, v_num=0w59, train/loss_step=0.0401, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 210/459 [05:36<06:38,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 211/459 [05:37<06:37,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 211/459 [05:37<06:37,  0.62it/s, v_num=0w59, train/loss_step=0.0324, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 212/459 [05:39<06:35,  0.62it/s, v_num=0w59, train/loss_step=0.0324, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 212/459 [05:39<06:35,  0.62it/s, v_num=0w59, train/loss_step=0.0377, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 213/459 [05:41<06:33,  0.62it/s, v_num=0w59, train/loss_step=0.0377, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 213/459 [05:41<06:33,  0.62it/s, v_num=0w59, train/loss_step=0.0383, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 214/459 [05:42<06:32,  0.62it/s, v_num=0w59, train/loss_step=0.0383, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 214/459 [05:42<06:32,  0.62it/s, v_num=0w59, train/loss_step=0.0296, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 215/459 [05:44<06:30,  0.62it/s, v_num=0w59, train/loss_step=0.0296, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 215/459 [05:44<06:30,  0.62it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 216/459 [05:45<06:29,  0.62it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 216/459 [05:45<06:29,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 217/459 [05:47<06:27,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 217/459 [05:47<06:27,  0.62it/s, v_num=0w59, train/loss_step=0.0389, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 218/459 [05:49<06:25,  0.62it/s, v_num=0w59, train/loss_step=0.0389, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 218/459 [05:49<06:25,  0.62it/s, v_num=0w59, train/loss_step=0.0296, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 219/459 [05:50<06:24,  0.62it/s, v_num=0w59, train/loss_step=0.0296, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 219/459 [05:50<06:24,  0.62it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 220/459 [05:52<06:22,  0.62it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 220/459 [05:52<06:22,  0.62it/s, v_num=0w59, train/loss_step=0.0314, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 221/459 [05:53<06:21,  0.62it/s, v_num=0w59, train/loss_step=0.0314, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 221/459 [05:53<06:21,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 222/459 [05:55<06:19,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 222/459 [05:55<06:19,  0.62it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 223/459 [05:57<06:17,  0.62it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 223/459 [05:57<06:17,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 224/459 [05:58<06:16,  0.62it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 224/459 [05:58<06:16,  0.62it/s, v_num=0w59, train/loss_step=0.0356, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 225/459 [06:00<06:14,  0.62it/s, v_num=0w59, train/loss_step=0.0356, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 225/459 [06:00<06:14,  0.62it/s, v_num=0w59, train/loss_step=0.0332, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 226/459 [06:01<06:13,  0.62it/s, v_num=0w59, train/loss_step=0.0332, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 226/459 [06:01<06:13,  0.62it/s, v_num=0w59, train/loss_step=0.0327, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 227/459 [06:03<06:11,  0.62it/s, v_num=0w59, train/loss_step=0.0327, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 227/459 [06:03<06:11,  0.62it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 228/459 [06:05<06:09,  0.62it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 228/459 [06:05<06:09,  0.62it/s, v_num=0w59, train/loss_step=0.0366, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 229/459 [06:06<06:08,  0.62it/s, v_num=0w59, train/loss_step=0.0366, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 229/459 [06:06<06:08,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 230/459 [06:08<06:06,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 230/459 [06:08<06:06,  0.62it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 231/459 [06:09<06:05,  0.62it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 231/459 [06:09<06:05,  0.62it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 232/459 [06:11<06:03,  0.62it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 232/459 [06:11<06:03,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 233/459 [06:13<06:01,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 233/459 [06:13<06:01,  0.62it/s, v_num=0w59, train/loss_step=0.0318, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 234/459 [06:14<06:00,  0.62it/s, v_num=0w59, train/loss_step=0.0318, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 234/459 [06:14<06:00,  0.62it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 235/459 [06:16<05:58,  0.62it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 235/459 [06:16<05:58,  0.62it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 236/459 [06:17<05:57,  0.62it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 236/459 [06:17<05:57,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 237/459 [06:19<05:55,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 237/459 [06:19<05:55,  0.62it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/459 [06:21<05:53,  0.62it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/459 [06:21<05:53,  0.62it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 239/459 [06:22<05:52,  0.62it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 239/459 [06:22<05:52,  0.62it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 240/459 [06:24<05:50,  0.62it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 240/459 [06:24<05:50,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 241/459 [06:25<05:49,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 241/459 [06:25<05:49,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 242/459 [06:27<05:47,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 242/459 [06:27<05:47,  0.62it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 243/459 [06:29<05:45,  0.62it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 243/459 [06:29<05:45,  0.62it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 244/459 [06:30<05:44,  0.62it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 244/459 [06:30<05:44,  0.62it/s, v_num=0w59, train/loss_step=0.0344, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 245/459 [06:32<05:42,  0.62it/s, v_num=0w59, train/loss_step=0.0344, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 245/459 [06:32<05:42,  0.62it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 246/459 [06:33<05:41,  0.62it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 246/459 [06:33<05:41,  0.62it/s, v_num=0w59, train/loss_step=0.0393, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 247/459 [06:35<05:39,  0.62it/s, v_num=0w59, train/loss_step=0.0393, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 247/459 [06:35<05:39,  0.62it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248/459 [06:37<05:37,  0.62it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248/459 [06:37<05:37,  0.62it/s, v_num=0w59, train/loss_step=0.0337, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 249/459 [06:38<05:36,  0.62it/s, v_num=0w59, train/loss_step=0.0337, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 249/459 [06:38<05:36,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 250/459 [06:40<05:34,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 250/459 [06:40<05:34,  0.62it/s, v_num=0w59, train/loss_step=0.0332, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 251/459 [06:42<05:33,  0.62it/s, v_num=0w59, train/loss_step=0.0332, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 251/459 [06:42<05:33,  0.62it/s, v_num=0w59, train/loss_step=0.0314, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 252/459 [06:43<05:31,  0.62it/s, v_num=0w59, train/loss_step=0.0314, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 252/459 [06:43<05:31,  0.62it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 253/459 [06:45<05:29,  0.62it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 253/459 [06:45<05:29,  0.62it/s, v_num=0w59, train/loss_step=0.0337, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 254/459 [06:46<05:28,  0.62it/s, v_num=0w59, train/loss_step=0.0337, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 254/459 [06:46<05:28,  0.62it/s, v_num=0w59, train/loss_step=0.0335, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 255/459 [06:48<05:26,  0.62it/s, v_num=0w59, train/loss_step=0.0335, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 255/459 [06:48<05:26,  0.62it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 256/459 [06:49<05:25,  0.62it/s, v_num=0w59, train/loss_step=0.0359, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 256/459 [06:49<05:25,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 257/459 [06:51<05:23,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 257/459 [06:51<05:23,  0.62it/s, v_num=0w59, train/loss_step=0.0326, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 258/459 [06:53<05:21,  0.62it/s, v_num=0w59, train/loss_step=0.0326, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 258/459 [06:53<05:21,  0.62it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 259/459 [06:54<05:20,  0.62it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 259/459 [06:54<05:20,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 260/459 [06:56<05:18,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 260/459 [06:56<05:18,  0.62it/s, v_num=0w59, train/loss_step=0.032, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 261/459 [06:57<05:17,  0.62it/s, v_num=0w59, train/loss_step=0.032, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 261/459 [06:57<05:17,  0.62it/s, v_num=0w59, train/loss_step=0.0406, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 262/459 [06:59<05:15,  0.62it/s, v_num=0w59, train/loss_step=0.0406, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 262/459 [06:59<05:15,  0.62it/s, v_num=0w59, train/loss_step=0.0411, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 263/459 [07:01<05:13,  0.62it/s, v_num=0w59, train/loss_step=0.0411, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 263/459 [07:01<05:13,  0.62it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 264/459 [07:02<05:12,  0.62it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 264/459 [07:02<05:12,  0.62it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 265/459 [07:04<05:10,  0.62it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 265/459 [07:04<05:10,  0.62it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 266/459 [07:05<05:08,  0.62it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 266/459 [07:05<05:08,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 267/459 [07:07<05:07,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 267/459 [07:07<05:07,  0.62it/s, v_num=0w59, train/loss_step=0.0316, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 268/459 [07:08<05:05,  0.62it/s, v_num=0w59, train/loss_step=0.0316, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 268/459 [07:08<05:05,  0.62it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 269/459 [07:10<05:04,  0.62it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 269/459 [07:10<05:04,  0.62it/s, v_num=0w59, train/loss_step=0.0303, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 270/459 [07:12<05:02,  0.62it/s, v_num=0w59, train/loss_step=0.0303, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 270/459 [07:12<05:02,  0.62it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 271/459 [07:13<05:00,  0.62it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 271/459 [07:13<05:00,  0.62it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 272/459 [07:15<04:59,  0.62it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 272/459 [07:15<04:59,  0.62it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 273/459 [07:16<04:57,  0.62it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 273/459 [07:16<04:57,  0.62it/s, v_num=0w59, train/loss_step=0.0365, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 274/459 [07:18<04:56,  0.62it/s, v_num=0w59, train/loss_step=0.0365, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 274/459 [07:18<04:56,  0.62it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 275/459 [07:20<04:54,  0.62it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 275/459 [07:20<04:54,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 276/459 [07:21<04:52,  0.62it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 276/459 [07:21<04:52,  0.62it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 277/459 [07:23<04:51,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 277/459 [07:23<04:51,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 278/459 [07:24<04:49,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 278/459 [07:24<04:49,  0.63it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 279/459 [07:26<04:47,  0.63it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 279/459 [07:26<04:47,  0.63it/s, v_num=0w59, train/loss_step=0.0309, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 280/459 [07:27<04:46,  0.63it/s, v_num=0w59, train/loss_step=0.0309, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 280/459 [07:27<04:46,  0.63it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 281/459 [07:29<04:44,  0.63it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 281/459 [07:29<04:44,  0.63it/s, v_num=0w59, train/loss_step=0.0324, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 282/459 [07:31<04:43,  0.63it/s, v_num=0w59, train/loss_step=0.0324, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 282/459 [07:31<04:43,  0.63it/s, v_num=0w59, train/loss_step=0.0335, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 283/459 [07:32<04:41,  0.63it/s, v_num=0w59, train/loss_step=0.0335, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 283/459 [07:32<04:41,  0.63it/s, v_num=0w59, train/loss_step=0.0329, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 284/459 [07:34<04:39,  0.63it/s, v_num=0w59, train/loss_step=0.0329, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 284/459 [07:34<04:39,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 285/459 [07:35<04:38,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 285/459 [07:35<04:38,  0.63it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 286/459 [07:37<04:36,  0.63it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 286/459 [07:37<04:36,  0.63it/s, v_num=0w59, train/loss_step=0.0329, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 287/459 [07:38<04:35,  0.63it/s, v_num=0w59, train/loss_step=0.0329, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 287/459 [07:38<04:35,  0.63it/s, v_num=0w59, train/loss_step=0.0305, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 288/459 [07:40<04:33,  0.63it/s, v_num=0w59, train/loss_step=0.0305, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 288/459 [07:40<04:33,  0.63it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 289/459 [07:42<04:31,  0.63it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 289/459 [07:42<04:31,  0.63it/s, v_num=0w59, train/loss_step=0.0353, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 290/459 [07:43<04:30,  0.63it/s, v_num=0w59, train/loss_step=0.0353, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 290/459 [07:43<04:30,  0.63it/s, v_num=0w59, train/loss_step=0.0319, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 291/459 [07:45<04:28,  0.63it/s, v_num=0w59, train/loss_step=0.0319, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 291/459 [07:45<04:28,  0.63it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 292/459 [07:46<04:26,  0.63it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 292/459 [07:46<04:26,  0.63it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 293/459 [07:48<04:25,  0.63it/s, v_num=0w59, train/loss_step=0.0349, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 293/459 [07:48<04:25,  0.63it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 294/459 [07:49<04:23,  0.63it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 294/459 [07:49<04:23,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 295/459 [07:51<04:22,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 295/459 [07:51<04:22,  0.63it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 296/459 [07:53<04:20,  0.63it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 296/459 [07:53<04:20,  0.63it/s, v_num=0w59, train/loss_step=0.0379, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 297/459 [07:54<04:18,  0.63it/s, v_num=0w59, train/loss_step=0.0379, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 297/459 [07:54<04:18,  0.63it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 298/459 [07:56<04:17,  0.63it/s, v_num=0w59, train/loss_step=0.0382, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 298/459 [07:56<04:17,  0.63it/s, v_num=0w59, train/loss_step=0.0389, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 299/459 [07:57<04:15,  0.63it/s, v_num=0w59, train/loss_step=0.0389, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 299/459 [07:57<04:15,  0.63it/s, v_num=0w59, train/loss_step=0.0365, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 300/459 [07:59<04:14,  0.63it/s, v_num=0w59, train/loss_step=0.0365, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 300/459 [07:59<04:14,  0.63it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 301/459 [08:00<04:12,  0.63it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 301/459 [08:00<04:12,  0.63it/s, v_num=0w59, train/loss_step=0.0278, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 302/459 [08:02<04:10,  0.63it/s, v_num=0w59, train/loss_step=0.0278, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 302/459 [08:02<04:10,  0.63it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 303/459 [08:04<04:09,  0.63it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 303/459 [08:04<04:09,  0.63it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 304/459 [08:05<04:07,  0.63it/s, v_num=0w59, train/loss_step=0.0367, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 304/459 [08:05<04:07,  0.63it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 305/459 [08:07<04:06,  0.63it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 305/459 [08:07<04:06,  0.63it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 306/459 [08:08<04:04,  0.63it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 306/459 [08:08<04:04,  0.63it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 307/459 [08:10<04:02,  0.63it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 307/459 [08:10<04:02,  0.63it/s, v_num=0w59, train/loss_step=0.0326, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 308/459 [08:11<04:01,  0.63it/s, v_num=0w59, train/loss_step=0.0326, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 308/459 [08:11<04:01,  0.63it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 309/459 [08:13<03:59,  0.63it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 309/459 [08:13<03:59,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 310/459 [08:15<03:57,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 310/459 [08:15<03:57,  0.63it/s, v_num=0w59, train/loss_step=0.0337, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 311/459 [08:16<03:56,  0.63it/s, v_num=0w59, train/loss_step=0.0337, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 311/459 [08:16<03:56,  0.63it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 312/459 [08:18<03:54,  0.63it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 312/459 [08:18<03:54,  0.63it/s, v_num=0w59, train/loss_step=0.0329, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 313/459 [08:19<03:53,  0.63it/s, v_num=0w59, train/loss_step=0.0329, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 313/459 [08:19<03:53,  0.63it/s, v_num=0w59, train/loss_step=0.030, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 314/459 [08:21<03:51,  0.63it/s, v_num=0w59, train/loss_step=0.030, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 314/459 [08:21<03:51,  0.63it/s, v_num=0w59, train/loss_step=0.0347, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 315/459 [08:23<03:49,  0.63it/s, v_num=0w59, train/loss_step=0.0347, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 315/459 [08:23<03:49,  0.63it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 316/459 [08:24<03:48,  0.63it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 316/459 [08:24<03:48,  0.63it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 317/459 [08:26<03:46,  0.63it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 317/459 [08:26<03:46,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 318/459 [08:27<03:45,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 318/459 [08:27<03:45,  0.63it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 319/459 [08:29<03:43,  0.63it/s, v_num=0w59, train/loss_step=0.034, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 319/459 [08:29<03:43,  0.63it/s, v_num=0w59, train/loss_step=0.0352, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 320/459 [08:30<03:41,  0.63it/s, v_num=0w59, train/loss_step=0.0352, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 320/459 [08:30<03:41,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 321/459 [08:32<03:40,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 321/459 [08:32<03:40,  0.63it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 322/459 [08:34<03:38,  0.63it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 322/459 [08:34<03:38,  0.63it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 323/459 [08:35<03:37,  0.63it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 323/459 [08:35<03:37,  0.63it/s, v_num=0w59, train/loss_step=0.0292, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 324/459 [08:37<03:35,  0.63it/s, v_num=0w59, train/loss_step=0.0292, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 324/459 [08:37<03:35,  0.63it/s, v_num=0w59, train/loss_step=0.0339, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 325/459 [08:38<03:33,  0.63it/s, v_num=0w59, train/loss_step=0.0339, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 325/459 [08:38<03:33,  0.63it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 326/459 [08:40<03:32,  0.63it/s, v_num=0w59, train/loss_step=0.0374, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 326/459 [08:40<03:32,  0.63it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 327/459 [08:41<03:30,  0.63it/s, v_num=0w59, train/loss_step=0.0385, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 327/459 [08:41<03:30,  0.63it/s, v_num=0w59, train/loss_step=0.0329, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 328/459 [08:43<03:29,  0.63it/s, v_num=0w59, train/loss_step=0.0329, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 328/459 [08:43<03:29,  0.63it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 329/459 [08:45<03:27,  0.63it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 329/459 [08:45<03:27,  0.63it/s, v_num=0w59, train/loss_step=0.0323, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 330/459 [08:46<03:25,  0.63it/s, v_num=0w59, train/loss_step=0.0323, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 330/459 [08:46<03:25,  0.63it/s, v_num=0w59, train/loss_step=0.0339, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 331/459 [08:48<03:24,  0.63it/s, v_num=0w59, train/loss_step=0.0339, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 331/459 [08:48<03:24,  0.63it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 332/459 [08:49<03:22,  0.63it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 332/459 [08:49<03:22,  0.63it/s, v_num=0w59, train/loss_step=0.035, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 333/459 [08:51<03:21,  0.63it/s, v_num=0w59, train/loss_step=0.035, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 333/459 [08:51<03:21,  0.63it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 334/459 [08:52<03:19,  0.63it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 334/459 [08:52<03:19,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 335/459 [08:54<03:17,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 335/459 [08:54<03:17,  0.63it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 336/459 [08:56<03:16,  0.63it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 336/459 [08:56<03:16,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 337/459 [08:57<03:14,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 337/459 [08:57<03:14,  0.63it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 338/459 [08:59<03:13,  0.63it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 338/459 [08:59<03:13,  0.63it/s, v_num=0w59, train/loss_step=0.0352, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 339/459 [09:00<03:11,  0.63it/s, v_num=0w59, train/loss_step=0.0352, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 339/459 [09:00<03:11,  0.63it/s, v_num=0w59, train/loss_step=0.0358, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 340/459 [09:02<03:09,  0.63it/s, v_num=0w59, train/loss_step=0.0358, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 340/459 [09:02<03:09,  0.63it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 341/459 [09:03<03:08,  0.63it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 341/459 [09:03<03:08,  0.63it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 342/459 [09:05<03:06,  0.63it/s, v_num=0w59, train/loss_step=0.0372, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 342/459 [09:05<03:06,  0.63it/s, v_num=0w59, train/loss_step=0.0305, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 343/459 [09:07<03:05,  0.63it/s, v_num=0w59, train/loss_step=0.0305, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 343/459 [09:07<03:05,  0.63it/s, v_num=0w59, train/loss_step=0.0318, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 344/459 [09:08<03:03,  0.63it/s, v_num=0w59, train/loss_step=0.0318, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 344/459 [09:08<03:03,  0.63it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 345/459 [09:10<03:01,  0.63it/s, v_num=0w59, train/loss_step=0.0368, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 345/459 [09:10<03:01,  0.63it/s, v_num=0w59, train/loss_step=0.0319, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 346/459 [09:11<03:00,  0.63it/s, v_num=0w59, train/loss_step=0.0319, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 346/459 [09:11<03:00,  0.63it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 347/459 [09:13<02:58,  0.63it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 347/459 [09:13<02:58,  0.63it/s, v_num=0w59, train/loss_step=0.0335, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 348/459 [09:15<02:57,  0.63it/s, v_num=0w59, train/loss_step=0.0335, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 348/459 [09:15<02:57,  0.63it/s, v_num=0w59, train/loss_step=0.0339, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 349/459 [09:16<02:55,  0.63it/s, v_num=0w59, train/loss_step=0.0339, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 349/459 [09:16<02:55,  0.63it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 350/459 [09:18<02:53,  0.63it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 350/459 [09:18<02:53,  0.63it/s, v_num=0w59, train/loss_step=0.0319, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 351/459 [09:19<02:52,  0.63it/s, v_num=0w59, train/loss_step=0.0319, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 351/459 [09:19<02:52,  0.63it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 352/459 [09:21<02:50,  0.63it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 352/459 [09:21<02:50,  0.63it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 353/459 [09:22<02:49,  0.63it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 353/459 [09:22<02:49,  0.63it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 354/459 [09:24<02:47,  0.63it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 354/459 [09:24<02:47,  0.63it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 355/459 [09:26<02:45,  0.63it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 355/459 [09:26<02:45,  0.63it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 356/459 [09:27<02:44,  0.63it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 356/459 [09:27<02:44,  0.63it/s, v_num=0w59, train/loss_step=0.0305, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 357/459 [09:29<02:42,  0.63it/s, v_num=0w59, train/loss_step=0.0305, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 357/459 [09:29<02:42,  0.63it/s, v_num=0w59, train/loss_step=0.0302, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 358/459 [09:31<02:41,  0.63it/s, v_num=0w59, train/loss_step=0.0302, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 358/459 [09:31<02:41,  0.63it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 359/459 [09:33<02:39,  0.63it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 359/459 [09:33<02:39,  0.63it/s, v_num=0w59, train/loss_step=0.0331, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 360/459 [09:35<02:38,  0.63it/s, v_num=0w59, train/loss_step=0.0331, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 360/459 [09:35<02:38,  0.63it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 361/459 [09:37<02:36,  0.63it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 361/459 [09:37<02:36,  0.63it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 362/459 [09:38<02:35,  0.63it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 362/459 [09:38<02:35,  0.63it/s, v_num=0w59, train/loss_step=0.039, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 363/459 [09:40<02:33,  0.63it/s, v_num=0w59, train/loss_step=0.039, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 363/459 [09:40<02:33,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 364/459 [09:42<02:31,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 364/459 [09:42<02:31,  0.63it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 365/459 [09:43<02:30,  0.63it/s, v_num=0w59, train/loss_step=0.0362, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 365/459 [09:43<02:30,  0.63it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 366/459 [09:45<02:28,  0.63it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 366/459 [09:45<02:28,  0.63it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 367/459 [09:47<02:27,  0.63it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 367/459 [09:47<02:27,  0.63it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 368/459 [09:48<02:25,  0.63it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 368/459 [09:48<02:25,  0.63it/s, v_num=0w59, train/loss_step=0.0387, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 369/459 [09:50<02:23,  0.63it/s, v_num=0w59, train/loss_step=0.0387, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 369/459 [09:50<02:23,  0.63it/s, v_num=0w59, train/loss_step=0.0336, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 370/459 [09:51<02:22,  0.63it/s, v_num=0w59, train/loss_step=0.0336, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 370/459 [09:51<02:22,  0.63it/s, v_num=0w59, train/loss_step=0.037, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 371/459 [09:53<02:20,  0.63it/s, v_num=0w59, train/loss_step=0.037, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 371/459 [09:53<02:20,  0.63it/s, v_num=0w59, train/loss_step=0.0313, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 372/459 [09:54<02:19,  0.63it/s, v_num=0w59, train/loss_step=0.0313, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 372/459 [09:54<02:19,  0.63it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 373/459 [09:56<02:17,  0.63it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 373/459 [09:56<02:17,  0.63it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 374/459 [09:57<02:15,  0.63it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 374/459 [09:57<02:15,  0.63it/s, v_num=0w59, train/loss_step=0.0289, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 375/459 [09:59<02:14,  0.63it/s, v_num=0w59, train/loss_step=0.0289, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 375/459 [09:59<02:14,  0.63it/s, v_num=0w59, train/loss_step=0.0312, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 376/459 [10:01<02:12,  0.63it/s, v_num=0w59, train/loss_step=0.0312, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 376/459 [10:01<02:12,  0.63it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 377/459 [10:02<02:11,  0.63it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 377/459 [10:02<02:11,  0.63it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 378/459 [10:04<02:09,  0.63it/s, v_num=0w59, train/loss_step=0.0334, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 378/459 [10:04<02:09,  0.63it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 379/459 [10:05<02:07,  0.63it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 379/459 [10:05<02:07,  0.63it/s, v_num=0w59, train/loss_step=0.0336, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 380/459 [10:07<02:06,  0.63it/s, v_num=0w59, train/loss_step=0.0336, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 380/459 [10:07<02:06,  0.63it/s, v_num=0w59, train/loss_step=0.031, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 381/459 [10:09<02:04,  0.63it/s, v_num=0w59, train/loss_step=0.031, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 381/459 [10:09<02:04,  0.63it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 382/459 [10:10<02:03,  0.63it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 382/459 [10:10<02:03,  0.63it/s, v_num=0w59, train/loss_step=0.0371, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 383/459 [10:12<02:01,  0.63it/s, v_num=0w59, train/loss_step=0.0371, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 383/459 [10:12<02:01,  0.63it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 384/459 [10:13<01:59,  0.63it/s, v_num=0w59, train/loss_step=0.0341, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 384/459 [10:13<01:59,  0.63it/s, v_num=0w59, train/loss_step=0.030, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 385/459 [10:15<01:58,  0.63it/s, v_num=0w59, train/loss_step=0.030, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 385/459 [10:15<01:58,  0.63it/s, v_num=0w59, train/loss_step=0.0293, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 386/459 [10:16<01:56,  0.63it/s, v_num=0w59, train/loss_step=0.0293, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 386/459 [10:16<01:56,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 387/459 [10:18<01:55,  0.63it/s, v_num=0w59, train/loss_step=0.0348, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 387/459 [10:18<01:55,  0.63it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 388/459 [10:20<01:53,  0.63it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 388/459 [10:20<01:53,  0.63it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 389/459 [10:21<01:51,  0.63it/s, v_num=0w59, train/loss_step=0.0363, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 389/459 [10:21<01:51,  0.63it/s, v_num=0w59, train/loss_step=0.0332, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 390/459 [10:23<01:50,  0.63it/s, v_num=0w59, train/loss_step=0.0332, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 390/459 [10:23<01:50,  0.63it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 391/459 [10:24<01:48,  0.63it/s, v_num=0w59, train/loss_step=0.0345, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 391/459 [10:24<01:48,  0.63it/s, v_num=0w59, train/loss_step=0.0319, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 392/459 [10:26<01:47,  0.63it/s, v_num=0w59, train/loss_step=0.0319, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 392/459 [10:26<01:47,  0.63it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 393/459 [10:28<01:45,  0.63it/s, v_num=0w59, train/loss_step=0.0308, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 393/459 [10:28<01:45,  0.63it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 394/459 [10:29<01:43,  0.63it/s, v_num=0w59, train/loss_step=0.0355, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 394/459 [10:29<01:43,  0.63it/s, v_num=0w59, train/loss_step=0.0314, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 395/459 [10:31<01:42,  0.63it/s, v_num=0w59, train/loss_step=0.0314, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 395/459 [10:31<01:42,  0.63it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 396/459 [10:32<01:40,  0.63it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 396/459 [10:32<01:40,  0.63it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 397/459 [10:34<01:39,  0.63it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 397/459 [10:34<01:39,  0.63it/s, v_num=0w59, train/loss_step=0.0315, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 398/459 [10:36<01:37,  0.63it/s, v_num=0w59, train/loss_step=0.0315, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 398/459 [10:36<01:37,  0.63it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 399/459 [10:37<01:35,  0.63it/s, v_num=0w59, train/loss_step=0.0346, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 399/459 [10:37<01:35,  0.63it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 400/459 [10:39<01:34,  0.63it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 400/459 [10:39<01:34,  0.63it/s, v_num=0w59, train/loss_step=0.0299, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 401/459 [10:40<01:32,  0.63it/s, v_num=0w59, train/loss_step=0.0299, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 401/459 [10:40<01:32,  0.63it/s, v_num=0w59, train/loss_step=0.0318, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 402/459 [10:42<01:31,  0.63it/s, v_num=0w59, train/loss_step=0.0318, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 402/459 [10:42<01:31,  0.63it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 403/459 [10:44<01:29,  0.63it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 403/459 [10:44<01:29,  0.63it/s, v_num=0w59, train/loss_step=0.0301, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 404/459 [10:45<01:27,  0.63it/s, v_num=0w59, train/loss_step=0.0301, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 404/459 [10:45<01:27,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 405/459 [10:47<01:26,  0.63it/s, v_num=0w59, train/loss_step=0.0333, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 405/459 [10:47<01:26,  0.63it/s, v_num=0w59, train/loss_step=0.0326, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 406/459 [10:49<01:24,  0.63it/s, v_num=0w59, train/loss_step=0.0326, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 406/459 [10:49<01:24,  0.63it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 407/459 [10:51<01:23,  0.63it/s, v_num=0w59, train/loss_step=0.0343, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 407/459 [10:51<01:23,  0.63it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 408/459 [10:53<01:21,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 408/459 [10:53<01:21,  0.62it/s, v_num=0w59, train/loss_step=0.0312, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 409/459 [10:55<01:20,  0.62it/s, v_num=0w59, train/loss_step=0.0312, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 409/459 [10:55<01:20,  0.62it/s, v_num=0w59, train/loss_step=0.0327, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 410/459 [10:56<01:18,  0.62it/s, v_num=0w59, train/loss_step=0.0327, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 410/459 [10:56<01:18,  0.62it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 411/459 [10:58<01:16,  0.62it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 411/459 [10:58<01:16,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 412/459 [11:00<01:15,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 412/459 [11:00<01:15,  0.62it/s, v_num=0w59, train/loss_step=0.0309, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 413/459 [11:02<01:13,  0.62it/s, v_num=0w59, train/loss_step=0.0309, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 413/459 [11:02<01:13,  0.62it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 414/459 [11:04<01:12,  0.62it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 414/459 [11:04<01:12,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 415/459 [11:05<01:10,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 415/459 [11:05<01:10,  0.62it/s, v_num=0w59, train/loss_step=0.0375, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 416/459 [11:07<01:09,  0.62it/s, v_num=0w59, train/loss_step=0.0375, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 416/459 [11:07<01:09,  0.62it/s, v_num=0w59, train/loss_step=0.0301, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 417/459 [11:09<01:07,  0.62it/s, v_num=0w59, train/loss_step=0.0301, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 417/459 [11:09<01:07,  0.62it/s, v_num=0w59, train/loss_step=0.0297, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 418/459 [11:10<01:05,  0.62it/s, v_num=0w59, train/loss_step=0.0297, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 418/459 [11:10<01:05,  0.62it/s, v_num=0w59, train/loss_step=0.0274, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 419/459 [11:12<01:04,  0.62it/s, v_num=0w59, train/loss_step=0.0274, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 419/459 [11:12<01:04,  0.62it/s, v_num=0w59, train/loss_step=0.0344, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 420/459 [11:13<01:02,  0.62it/s, v_num=0w59, train/loss_step=0.0344, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 420/459 [11:13<01:02,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 421/459 [11:15<01:00,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 421/459 [11:15<01:00,  0.62it/s, v_num=0w59, train/loss_step=0.0314, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 422/459 [11:17<00:59,  0.62it/s, v_num=0w59, train/loss_step=0.0314, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 422/459 [11:17<00:59,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 423/459 [11:18<00:57,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 423/459 [11:18<00:57,  0.62it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 424/459 [11:20<00:56,  0.62it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 424/459 [11:20<00:56,  0.62it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 425/459 [11:21<00:54,  0.62it/s, v_num=0w59, train/loss_step=0.0317, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 425/459 [11:21<00:54,  0.62it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 426/459 [11:23<00:52,  0.62it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 426/459 [11:23<00:52,  0.62it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 427/459 [11:24<00:51,  0.62it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 427/459 [11:24<00:51,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 428/459 [11:26<00:49,  0.62it/s, v_num=0w59, train/loss_step=0.0364, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 428/459 [11:26<00:49,  0.62it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 429/459 [11:28<00:48,  0.62it/s, v_num=0w59, train/loss_step=0.0338, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 429/459 [11:28<00:48,  0.62it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 430/459 [11:29<00:46,  0.62it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 430/459 [11:29<00:46,  0.62it/s, v_num=0w59, train/loss_step=0.032, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 431/459 [11:31<00:44,  0.62it/s, v_num=0w59, train/loss_step=0.032, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 431/459 [11:31<00:44,  0.62it/s, v_num=0w59, train/loss_step=0.0337, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 432/459 [11:32<00:43,  0.62it/s, v_num=0w59, train/loss_step=0.0337, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 432/459 [11:32<00:43,  0.62it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 433/459 [11:34<00:41,  0.62it/s, v_num=0w59, train/loss_step=0.0361, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 433/459 [11:34<00:41,  0.62it/s, v_num=0w59, train/loss_step=0.0332, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 434/459 [11:35<00:40,  0.62it/s, v_num=0w59, train/loss_step=0.0332, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 434/459 [11:35<00:40,  0.62it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 435/459 [11:37<00:38,  0.62it/s, v_num=0w59, train/loss_step=0.0373, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 435/459 [11:37<00:38,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 436/459 [11:39<00:36,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 436/459 [11:39<00:36,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 437/459 [11:40<00:35,  0.62it/s, v_num=0w59, train/loss_step=0.0351, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 437/459 [11:40<00:35,  0.62it/s, v_num=0w59, train/loss_step=0.0262, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 438/459 [11:42<00:33,  0.62it/s, v_num=0w59, train/loss_step=0.0262, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 438/459 [11:42<00:33,  0.62it/s, v_num=0w59, train/loss_step=0.029, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 439/459 [11:43<00:32,  0.62it/s, v_num=0w59, train/loss_step=0.029, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 439/459 [11:43<00:32,  0.62it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 440/459 [11:45<00:30,  0.62it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 440/459 [11:45<00:30,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 441/459 [11:46<00:28,  0.62it/s, v_num=0w59, train/loss_step=0.0354, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 441/459 [11:46<00:28,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 442/459 [11:48<00:27,  0.62it/s, v_num=0w59, train/loss_step=0.0328, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 442/459 [11:48<00:27,  0.62it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 443/459 [11:50<00:25,  0.62it/s, v_num=0w59, train/loss_step=0.0311, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 443/459 [11:50<00:25,  0.62it/s, v_num=0w59, train/loss_step=0.0388, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 444/459 [11:51<00:24,  0.62it/s, v_num=0w59, train/loss_step=0.0388, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 444/459 [11:51<00:24,  0.62it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 445/459 [11:53<00:22,  0.62it/s, v_num=0w59, train/loss_step=0.0306, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 445/459 [11:53<00:22,  0.62it/s, v_num=0w59, train/loss_step=0.028, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 446/459 [11:54<00:20,  0.62it/s, v_num=0w59, train/loss_step=0.028, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 446/459 [11:54<00:20,  0.62it/s, v_num=0w59, train/loss_step=0.0316, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 447/459 [11:56<00:19,  0.62it/s, v_num=0w59, train/loss_step=0.0316, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 447/459 [11:56<00:19,  0.62it/s, v_num=0w59, train/loss_step=0.0299, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 448/459 [11:57<00:17,  0.62it/s, v_num=0w59, train/loss_step=0.0299, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 448/459 [11:57<00:17,  0.62it/s, v_num=0w59, train/loss_step=0.0309, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 449/459 [11:59<00:16,  0.62it/s, v_num=0w59, train/loss_step=0.0309, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 449/459 [11:59<00:16,  0.62it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 450/459 [12:01<00:14,  0.62it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 450/459 [12:01<00:14,  0.62it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 451/459 [12:02<00:12,  0.62it/s, v_num=0w59, train/loss_step=0.0325, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 451/459 [12:02<00:12,  0.62it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 452/459 [12:05<00:11,  0.62it/s, v_num=0w59, train/loss_step=0.0321, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 452/459 [12:05<00:11,  0.62it/s, v_num=0w59, train/loss_step=0.0294, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 453/459 [12:06<00:09,  0.62it/s, v_num=0w59, train/loss_step=0.0294, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 453/459 [12:06<00:09,  0.62it/s, v_num=0w59, train/loss_step=0.0289, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 454/459 [12:08<00:08,  0.62it/s, v_num=0w59, train/loss_step=0.0289, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 454/459 [12:08<00:08,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 455/459 [12:10<00:06,  0.62it/s, v_num=0w59, train/loss_step=0.033, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 455/459 [12:10<00:06,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 456/459 [12:12<00:04,  0.62it/s, v_num=0w59, train/loss_step=0.0322, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 456/459 [12:12<00:04,  0.62it/s, v_num=0w59, train/loss_step=0.0304, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 457/459 [12:13<00:03,  0.62it/s, v_num=0w59, train/loss_step=0.0304, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 457/459 [12:13<00:03,  0.62it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 458/459 [12:15<00:01,  0.62it/s, v_num=0w59, train/loss_step=0.0342, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 458/459 [12:15<00:01,  0.62it/s, v_num=0w59, train/loss_step=0.036, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203] Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [12:16<00:00,  0.62it/s, v_num=0w59, train/loss_step=0.036, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [12:16<00:00,  0.62it/s, v_num=0w59, train/loss_step=0.0323, val/loss=1.070, val/mAP=0.080, val/mAP_best=0.080, train/loss_epoch=0.203]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/52 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/52 [00:00<?, ?it/s][A
Validation DataLoader 0:   2%|â–         | 1/52 [00:00<00:43,  1.17it/s][A
Validation DataLoader 0:   4%|â–         | 2/52 [00:01<00:44,  1.14it/s][A
Validation DataLoader 0:   6%|â–Œ         | 3/52 [00:02<00:42,  1.15it/s][A
Validation DataLoader 0:   8%|â–Š         | 4/52 [00:03<00:41,  1.15it/s][A
Validation DataLoader 0:  10%|â–‰         | 5/52 [00:04<00:40,  1.15it/s][A
Validation DataLoader 0:  12%|â–ˆâ–        | 6/52 [00:05<00:39,  1.16it/s][A
Validation DataLoader 0:  13%|â–ˆâ–        | 7/52 [00:06<00:38,  1.16it/s][A
Validation DataLoader 0:  15%|â–ˆâ–Œ        | 8/52 [00:06<00:37,  1.16it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 9/52 [00:07<00:37,  1.16it/s][A
Validation DataLoader 0:  19%|â–ˆâ–‰        | 10/52 [00:08<00:36,  1.16it/s][A
Validation DataLoader 0:  21%|â–ˆâ–ˆ        | 11/52 [00:09<00:35,  1.16it/s][A
Validation DataLoader 0:  23%|â–ˆâ–ˆâ–       | 12/52 [00:10<00:34,  1.16it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 13/52 [00:11<00:33,  1.16it/s][A
Validation DataLoader 0:  27%|â–ˆâ–ˆâ–‹       | 14/52 [00:12<00:32,  1.16it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–‰       | 15/52 [00:12<00:31,  1.16it/s][A
Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆ       | 16/52 [00:13<00:30,  1.16it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 17/52 [00:14<00:30,  1.16it/s][A
Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 18/52 [00:15<00:29,  1.16it/s][A
Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/52 [00:16<00:28,  1.16it/s][A
Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/52 [00:17<00:27,  1.16it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/52 [00:18<00:26,  1.17it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/52 [00:18<00:25,  1.17it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/52 [00:19<00:24,  1.17it/s][A
Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/52 [00:20<00:24,  1.17it/s][A
Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 25/52 [00:21<00:23,  1.17it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/52 [00:22<00:22,  1.17it/s][A
Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/52 [00:23<00:21,  1.17it/s][A
Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/52 [00:24<00:20,  1.17it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 29/52 [00:24<00:19,  1.17it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 30/52 [00:25<00:18,  1.17it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 31/52 [00:26<00:18,  1.17it/s][A
Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/52 [00:27<00:17,  1.17it/s][A
Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/52 [00:28<00:16,  1.17it/s][A
Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 34/52 [00:29<00:15,  1.17it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 35/52 [00:30<00:14,  1.17it/s][A
Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 36/52 [00:30<00:13,  1.17it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 37/52 [00:31<00:12,  1.17it/s][A
Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/52 [00:32<00:12,  1.17it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 39/52 [00:33<00:11,  1.17it/s][A
Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 40/52 [00:34<00:10,  1.17it/s][A
Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 41/52 [00:35<00:09,  1.17it/s][A
Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 42/52 [00:35<00:08,  1.17it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/52 [00:36<00:07,  1.17it/s][A
Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/52 [00:37<00:06,  1.17it/s][A
Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 45/52 [00:38<00:05,  1.17it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 46/52 [00:39<00:05,  1.17it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 47/52 [00:40<00:04,  1.17it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/52 [00:41<00:03,  1.17it/s][A
Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/52 [00:41<00:02,  1.17it/s][A
Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 50/52 [00:42<00:01,  1.17it/s][A
Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 51/52 [00:43<00:00,  1.17it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:44<00:00,  1.17it/s][A